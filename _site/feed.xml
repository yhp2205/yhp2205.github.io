<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-31T17:12:23+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">YOUNGHYUN’s Blog</title><subtitle>노력하는 흔적 남기기</subtitle><author><name>Younghyun Park</name></author><entry><title type="html">[모두의파이썬X알고리즘] Unit 2. 간단한 프로그램 만들기</title><link href="http://localhost:4000/python/APY-1/" rel="alternate" type="text/html" title="[모두의파이썬X알고리즘] Unit 2. 간단한 프로그램 만들기" /><published>2023-07-31T00:00:00+09:00</published><updated>2023-07-31T00:00:00+09:00</updated><id>http://localhost:4000/python/APY-1</id><content type="html" xml:base="http://localhost:4000/python/APY-1/"><![CDATA[<p>거북이 그래픽으로 도형을 그리는 간단한 프로그램을 만들어보겠습니다.
우선 맥에서 terminal을 열고 거북이 그래픽을 띄워줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">turtle</span> <span class="k">as</span> <span class="n">t</span>
<span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="s">"turtle"</span><span class="p">)</span>
</code></pre></div></div>

<p>다음 코드를 실행하면 아래 이미지와 같은 거북이 그래픽 창이 생깁니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="Python" /><category term="Python" /><category term="모두의파이썬" /><summary type="html"><![CDATA[거북이 그래픽으로 도형을 그리는 프로그램]]></summary></entry><entry><title type="html">[EDA 리뷰] Google AI4Code</title><link href="http://localhost:4000/ml/googAIeda/" rel="alternate" type="text/html" title="[EDA 리뷰] Google AI4Code" /><published>2022-06-30T00:00:00+09:00</published><updated>2022-06-30T00:00:00+09:00</updated><id>http://localhost:4000/ml/googAIeda</id><content type="html" xml:base="http://localhost:4000/ml/googAIeda/"><![CDATA[<p>이 글은 Kaggle의 Google AI4Code 대회의 <a href="https://www.kaggle.com/code/andreaspalmgren/ai4code-comprehensive-eda">AI4Code - Comprehensive EDA</a>를 리뷰하기 위해 작성되었습니다.</p>

<h3 id="data-전처리-과정">data 전처리 과정</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_notebook</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span>
            <span class="n">path</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="p">{</span><span class="s">'cell_type'</span><span class="p">:</span> <span class="s">'category'</span><span class="p">,</span> <span class="s">'source'</span><span class="p">:</span> <span class="s">'str'</span><span class="p">})</span>
        <span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">path</span><span class="p">.</span><span class="n">stem</span><span class="p">)</span>
        <span class="p">.</span><span class="n">rename_axis</span><span class="p">(</span><span class="s">'cell_id'</span><span class="p">)</span>
    <span class="p">)</span>

<span class="c1"># Subset of training due to its large size
</span><span class="n">NUM_TRAIN</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">paths_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">((</span><span class="n">data_dir</span> <span class="o">/</span> <span class="s">'train'</span><span class="p">).</span><span class="n">glob</span><span class="p">(</span><span class="s">'*.json'</span><span class="p">))[:</span><span class="n">NUM_TRAIN</span><span class="p">]</span>
<span class="c1">#paths_train = list((data_dir / 'train').glob('*.json'))
</span>
<span class="n">notebooks_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">read_notebook</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">paths_train</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s">'Train NBs'</span><span class="p">)]</span>

<span class="c1"># Get notebooks
</span><span class="n">df_notebooks</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">notebooks_train</span><span class="p">).</span><span class="n">set_index</span><span class="p">(</span><span class="s">'id'</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">swaplevel</span><span class="p">().</span><span class="n">sort_index</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="s">'id'</span><span class="p">,</span> 
                                                                                               <span class="n">sort_remaining</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span> 

<span class="c1"># Get correct order of cells in notebooks                                                                                          
</span><span class="n">df_orders</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_dir</span><span class="o">/</span><span class="s">'train_orders.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'id'</span><span class="p">)</span>
<span class="n">df_orders</span> <span class="o">=</span> <span class="n">df_orders</span><span class="p">.</span><span class="n">squeeze</span><span class="p">().</span><span class="nb">str</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">).</span><span class="n">explode</span><span class="p">().</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">df_orders</span><span class="p">[</span><span class="s">"rank"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df_orders</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">"id"</span><span class="p">).</span><span class="n">count</span><span class="p">()[</span><span class="s">"cell_order"</span><span class="p">]]).</span><span class="n">explode</span><span class="p">().</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df_notebooks</span><span class="p">.</span><span class="n">reset_index</span><span class="p">().</span><span class="n">merge</span><span class="p">(</span><span class="n">df_orders</span><span class="p">.</span><span class="n">reset_index</span><span class="p">().</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s">'cell_order'</span><span class="p">:</span><span class="s">'cell_id'</span><span class="p">}),</span> 
                       <span class="n">how</span><span class="o">=</span><span class="s">'inner'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s">'id'</span><span class="p">,</span><span class="s">'cell_id'</span><span class="p">])</span>

<span class="c1"># Get ancestors for notebooks
</span><span class="n">df_ancestors</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">/</span> <span class="s">'train_ancestors.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'id'</span><span class="p">)</span>

<span class="c1"># Final combined dataframe
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_ancestors</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"id"</span><span class="p">).</span><span class="n">sort_values</span><span class="p">([</span><span class="s">"id"</span><span class="p">,</span> <span class="s">"rank"</span><span class="p">]).</span><span class="n">set_index</span><span class="p">([</span><span class="s">"id"</span><span class="p">,</span> <span class="s">"cell_id"</span><span class="p">])</span>


<span class="c1"># Dataframe for count information - Used in EDA
</span><span class="n">mkd</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"cell_type"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"markdown"</span><span class="p">].</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s">"id"</span><span class="p">]).</span><span class="n">count</span><span class="p">().</span><span class="n">source</span>
<span class="n">code</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"cell_type"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"code"</span><span class="p">].</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s">"id"</span><span class="p">]).</span><span class="n">count</span><span class="p">().</span><span class="n">source</span>
<span class="n">df_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mkd</span><span class="p">,</span> <span class="n">code</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_counts</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'markdown_count'</span><span class="p">,</span> <span class="s">'code_count'</span><span class="p">]</span>
<span class="n">df_counts</span><span class="p">[</span><span class="s">"tot"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_counts</span><span class="p">.</span><span class="n">markdown_count</span><span class="o">+</span><span class="n">df_counts</span><span class="p">.</span><span class="n">code_count</span>
<span class="n">df_counts</span><span class="p">[</span><span class="s">"ratio"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_counts</span><span class="p">.</span><span class="n">code_count</span> <span class="o">/</span> <span class="n">df_counts</span><span class="p">.</span><span class="n">tot</span>
</code></pre></div></div>
<p>이 포스트에서는 전체적인 데이터의 형태를 파악하기 위해 위와 같은 코드를 사용하여 변수를 추가했습니다. 변수에 대한 설명은 다음과 같습니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-1.png?raw=true" alt="1-1" /></p>

<p>cell이 mk인지 code인지에 대한 값을 cell_type이라는 변수에 지정하였습니다.</p>

<h3 id="data-analysis">DATA Analysis</h3>
<p>코드와 마크다운간의 관계를 알아내는 것이 중요한 포인트이기 때문에 cell_type의 비율을 먼저 출력합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig1</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">wedges</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">pie</span><span class="p">([</span><span class="n">df_counts</span><span class="p">.</span><span class="n">code_count</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">df_counts</span><span class="p">.</span><span class="n">markdown_count</span><span class="p">.</span><span class="nb">sum</span><span class="p">()],</span> <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                  <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">"Code"</span><span class="p">,</span> <span class="s">"Markdown"</span><span class="p">],</span> 
                                  <span class="n">autopct</span><span class="o">=</span><span class="s">'%1.1f%%'</span><span class="p">,</span> <span class="n">textprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">"w"</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s">"#008b8b"</span><span class="p">,</span> <span class="s">"#8b0000"</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">autotexts</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s">"bold"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">wedges</span><span class="p">,</span> <span class="p">[</span><span class="s">"Code"</span><span class="p">,</span> <span class="s">"Markdown"</span><span class="p">],</span>
          <span class="n">loc</span><span class="o">=</span><span class="s">"center left"</span><span class="p">,</span>
          <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>

<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Proportion of Code vs Markdown"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-2.png?raw=true" alt="1-2" /><br />
mk에 비해 코드의 비율이 높습니다.</p>

<p>다음으로는 노트북의 셀 수의 길이를 cell의 갯수에 따른 빈도로 히스토그램을 그려 나타내었습니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-3.png?raw=true" alt="1-3" /><br />
일반적으로 50개 이하의 셀을 지닌 것을 확인할 수 있습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-4.png?raw=true" alt="1-4" /><br />
코드와 마크다운 셀의 분포를 다시 비교한 것입니다. 이 분포를 보면 코드와 마크다운의 빈도가 비례하는 관계를 지니고 있다고 추측할 수 있습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-5.png?raw=true" alt="1-5" /><br />
가장 짧은 노트북과 긴 노트북에 존재하는 cell들의 갯수를 출력하였습니다. 가장 짧은 노트북에도 최소한 2개의 셀이 존재한다고 생각할 수 있겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-6.png?raw=true" alt="1-6" /><br />
2개의 셀을 가진 노트북에서 첫번째 셀과 두번째 셀의 형식을 3차원 막대로 출력한 그림과 첫 번째로 어떤 셀이 주로 오는지에 대한 파이차트를 출력한 결과입니다. 해석해보면, 2개의 cell을 가진 노트북들은 대부분 code보다 mk로 먼저 시작하는 것으로 볼 수 있습니다. 그렇다면 일반적으로 mk가 code 셀보다 앞에 오는지에 대한 의문이 있을 수 있는데, 2번째 파이차트를 확인해보면 대부분의 노트북이 마크다운 셀로 시작하는 것을 알 수 있습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-7.png?raw=true" alt="1-7" /></p>

<p>2개의 셀을 지닌 노트북은 3차원 plot으로 확인할 수 있었지만, 더 많은 셀을 가진 것들을 3차원으로 그리는 것은 어렵기 때문에 노트북의 길이에 따라 code 셀과 markdown 셀의 분포를 나타내어 보았습니다. 이를 보면 첫 번째 셀에서는 마크다운이 더 많지만 그를 제외한 모든 노트북에서는 code가 더 많은 부분을 차지합니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-8.png?raw=true" alt="1-8" /><br />
앞선 결과에 이어 code 셀이 각 노트북에서 어느정도의 비중을 차지하는지에 대한 히스토그램을 그려보았습니다. 많은 노트북에서 code 셀이 절반 이상의 비율을 차지하고 있습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-9.png?raw=true" alt="1-9" /><br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/AIEDA/1-10.png?raw=true" alt="1-10" /><br />
code 셀과 markdown 셀의 길이를 나타내었습니다. 일반적으로 코드셀보다 마크다운 셀이 길이가 더 짧은 것으로 알 수 있습니다.</p>

<p>이러한 코드를 통해 마크다운과 코드 셀간의 관계를 조금이나마 파악해보았습니다. 마크다운 셀과 코드 사이의 유의미한 관계를 알아내기 위해서 다른 공부들이 더 필요할 것 같습니다. 더 알아본 후에 다음 포스트로 작성해보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="google AI4Code" /><summary type="html"><![CDATA[google AI4Code EDA 리뷰]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 8. 이미지를 위한 인공 신경망</title><link href="http://localhost:4000/dl/HG8-4/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 8. 이미지를 위한 인공 신경망" /><published>2022-06-07T00:00:00+09:00</published><updated>2022-06-07T00:00:00+09:00</updated><id>http://localhost:4000/dl/HG8-4</id><content type="html" xml:base="http://localhost:4000/dl/HG8-4/"><![CDATA[<p>2절에서는 패션 MNIST 데이터를 사용하여 합성곱 신경망을 만들고 그래프를 그려보았습니다. 그리고 3절에서는 함수형 API를 사용하여 합성곱 신경망의 시각화에 대해 알아봤습니다. 이 글은 함수형 API에 대해 다뤄보고자 작성하였습니다.</p>

<h1 id="sequential-api로-만든-모델">Sequential API로 만든 모델</h1>

<p>2절에서는 신경망 모델을 만들 때에 다음과 같이 Sequential 클래스를 사용했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> 
                              <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> 
                              <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</code></pre></div></div>

<p>Sequential 객체를 만들고 두 개의 합성곱-풀링 층을 만든 후 Flatten 클래스와 hidden layer, dropout층 그리고 출력층을 차례로 구성한 모델입니다.<br />
model.summary()로 모델의 형태를 출력해보면 다음과 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-1.png?raw=true" alt="HG8-4-1" /></p>

<p>이를 함수형 API를 사용하여 만들어보겠습니다.</p>

<h1 id="함수형-api로-만든-모델">함수형 API로 만든 모델</h1>
<p>앞서 만든 모델과 모든 조건을 동일하게 설정하여 모델을 구성합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'softmax'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-2.png?raw=true" alt="HG8-4-2" /></p>

<p>두 모델이 동일하게 만들어진 것을 확인할 수 있습니다.</p>

<p>두 모델의 구성을 출력해보면 다음과 같습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-4.png?raw=true" alt="HG8-4-4" />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-3.png?raw=true" alt="HG8-4-3" /></p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-5.png?raw=true" alt="HG8-4-5" width="300" height="300" />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-6.png?raw=true" alt="HG8-4-6" width="300" height="300" /><br />
<br />
이렇게 Sequential class로 만든 모델을 함수형 API를 사용하여 동일하게 만들어보았습니다. 여기까지 생각했을 때 함수형 API의 용도에 의문이 생길 수 있습니다. 이런 단순한 모델의 경우에는 Sequential API만을 사용하여 무리없이 구현할 수 있기 때문입니다. 그러나 모델이 복잡해지고 입력층과 출력층이 여러개일 경우, Sequential API만을 사용하여 만들 수 없습니다.<br />
<br />
함수형 API의 장점은 모든 레이어를 개별적이고 독립적으로 정의할 수 있기 때문에 다중 입력 및 출력이 가능하다는 것입니다. 다시 말해 모든 레이어에서 원하는 만큼의 다른 레이어를 추가할 수 있습니다.</p>

<p><img src="https://editor.analyticsvidhya.com/uploads/48655toy%20network.png" alt="HG8-4-7" /><br />
다음과 같이 3개의 입력, 각 4개의 뉴런을 가진 hidden layer 2개와 하나의 output이 있는 신경망일 경우에는 Sequential API로도 모델을 생성할 수 있습니다.</p>

<p><img src="https://editor.analyticsvidhya.com/uploads/41982branched%20network.png" alt="HG8-4-8" /><br />
그러나 다음과 같이 2번째 hidden layer 다음에 하나의 레이어가 더 추가해서 원래의 출력값인 y1과 추가된 레이어를 통과한 출력값인 y2를 동시에 얻고싶을 때 함수형 API를 사용할 수 있습니다.</p>

<p>위 그림을 토대로 모델을 만들어보면 다음과 같이 표현할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">##입력 레이어
</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span><span class="n">name</span><span class="o">=</span><span class="s">'input_layer'</span><span class="p">)</span> 

<span class="c1">##2개의 히든 레이어
</span><span class="n">Layer_1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Layer_1'</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span> 
<span class="n">Layer_2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Layer_2'</span><span class="p">)(</span><span class="n">Layer_1</span><span class="p">)</span> 

<span class="c1">##출력 레이어 y1
</span><span class="n">y1_output</span><span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"linear"</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'y1_output'</span><span class="p">)(</span><span class="n">Layer_2</span><span class="p">)</span> 

<span class="c1">##정의 Branched layer 
</span><span class="n">Branched_layer</span><span class="o">=</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Branched_layer'</span><span class="p">)(</span><span class="n">Layer_2</span><span class="p">)</span> 

<span class="c1">##두 번째 출력 레이어 y2
</span><span class="n">y2_output</span><span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"linear"</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'y2_output'</span><span class="p">)(</span><span class="n">Branched_layer</span><span class="p">)</span> 

<span class="c1">##입력 및 출력 레이어 지정
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span><span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">y1_output</span><span class="p">,</span><span class="n">y2_output</span><span class="p">])</span>
</code></pre></div></div>
<p align="center">
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG8/HG8-4-9.png?raw=true" /> 
</p>
<p><br />
그림과 동일한 구조의 모델을 만들었습니다. 두 번째 hidden layer 다음에 하나의 레이어가 더 추가되었고, 두번째 hidden layer의 출력은 y1을 예측하는데에 사용되며 동시에 Branched layer에 전달됩니다. 또한 두 번째 출력을 예측하기 위해 더 많은 레이어를 추가할 수 있습니다. 이러한 방법을 사용하여 동시에 여러 출력층을 만들 수 있습니다.</p>

<p>결론적으로 Sequential API를 사용하면 출력층 y1 및 y2를 예측하기 위해 2개의 다른 신경망을 구축해야 하지만, 함수형 API를 사용하면 단일 네트워크에서 두 개의 출력을 만들 수 있습니다.</p>

<p><br />
또한 이보다 모델이 복잡해졌을 때에도, 함수형 API를 사용하면 훨씬 더 효율적으로 모델을 구성할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="DL" /><category term="Blog" /><category term="Deep learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[함수형 API]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 5. 트리 알고리즘(2)</title><link href="http://localhost:4000/ml/HG5-2/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 5. 트리 알고리즘(2)" /><published>2022-05-28T00:00:00+09:00</published><updated>2022-05-28T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG5-2</id><content type="html" xml:base="http://localhost:4000/ml/HG5-2/"><![CDATA[<h1 id="교차-검증과-그리드-서치">교차 검증과 그리드 서치</h1>

<p>테스트 세트 성능을 올바르게 판단하기 위해서는 모델을 전부 만든 뒤에 마지막에 한 번만 사용하는 것이 좋습니다. 그렇다면 테스트 세트를 사용하지 않고 어떻게 하이퍼 파라미터 튜닝을 할 수 있을지 알아보도록 하겠습니다.</p>

<h2 id="검증-세트">검증 세트</h2>
<p>테스트 세트를 사용하지 않고 모델이 과대적합인지 과소적합인지 알아보기 위해서는 훈련 세트를 또 나누는 방법을 취할 수 있습니다.
이 데이터를 검증세트라고 합니다.
훈련 세트에서 모델을 훈련한 후 검증세트로 평가합니다. 이런 식으로 매개변수들을 바꿔가며 가장 좋은 모델을 고르면 됩니다.</p>

<p>그럼 데이터를 불러와서 train 세트와 test 세트로 나눈 후에 검증 세트로도 나눠보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/wine_csv_data'</span><span class="p">)</span>
</code></pre></div></div>
<p>와인 데이터를 data에는 alcohol, sugar, pH 를 지정하고, target에는 class 를 넘파이 배열로 지정해줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s">'class'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>
<p>train_test_split를 사용하여 train data와 test data로 나눕니다. test_size를 0.2로 지정하여 20%만 test 데이터로 가져옵니다. test size를 지정하지 않을 경우 train_test_split는 디폴트로 0.25를 테스트 세트로 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>그리고 train data를 sub data와 val data로 한번 더 나누어 줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sub_input</span><span class="p">,</span> <span class="n">val_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">,</span> <span class="n">val_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>훈련 세트와 검증 세트의 크기를 확인해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">sub_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(4157, 3) (1040, 3)</p>
</blockquote>

<p>데이터를 각각 훈련데이터, 검증데이터, 테스트 데이터로 나누어줬습니다. 
다음으로 훈련데이터와 검증데이터를 사용하여 모델을 만들고 평가합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sub_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">sub_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">val_input</span><span class="p">,</span> <span class="n">val_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9971133028626413<br />
0.864423076923077</p>
</blockquote>

<p>모델이 훈련 세트에 과대적합 되어있으니 매개변수를 바꾸어 더 좋은 모델을 찾아보겠습니다.</p>

<h2 id="교차-검증">교차 검증</h2>
<p>많은 데이터를 훈련세트에 사용할 수록 모델의 정확도가 좋아집니다. 그렇다고 검증세트를 적게 사용하면 검증 점수가 불안정하게 나올 수 있습니다. 이를 해결하기 위해 교차검증을 이용합니다.<br />
검증세트를 떼어놓는 과정을 여러 번 반복하고 이 점수들을 평균내어 검증 점수를 얻어냅니다.
이런 방법을 사용하면 데이터의 8,90% 정도를 훈련에 사용할 수 있습니다.
사이킷런에 cross_validate() 함수를 사용하여 교차검증을 해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘fit_time’: array([0.00482202, 0.00468516, 0.00489712, 0.00469923, 0.0045011 ]), ‘score_time’: array([0.0005331 , 0.00038004, 0.00038218, 0.00042176, 0.00033784]), ‘test_score’: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}</p>
</blockquote>

<p>이 함수는 fit_time, score_time, test_score 키를 갖는 딕셔너리를 반환합니다. 
cross_validate 함수는 기본적으로 5-폴드 교차검증을 수행하기 때문에 각 키마다 5개의 숫자가 담겨있습니다. 
교차 검증의 점수들을 평균내어 최종 점수를 얻습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.855300214703487</p>
</blockquote>

<p>다만 cross_validate 함수는 훈련 세트를 섞어서 폴드를 나누지 않기 때문에 만약 훈련세트를 섞어서 나누고 싶다면 splitter을 지정해줘야합니다. 
cross_validate 함수는 기본적으로 회귀모델일 경우 KFold 분할기를 사용하고 분류모델일 경우 StratifiedKFold를 사용합니다. 
다음 코드와 앞서 수행한 교차 검증은 동일합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">StratifiedKFold</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.855300214703487</p>
</blockquote>

<p>10-교차검증을 해보고 싶다면 다음과 같이 stratifiedKFold에 splits를 10으로 지정하고 splitter에 할당하여 사용하면 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">splitter</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">splitter</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8574181117533719</p>
</blockquote>

<h2 id="하이퍼파라미터-튜닝">하이퍼파라미터 튜닝</h2>

<p>결정트리 모델에서는 여러 매개변수를 동시에 바꿔가며 최적의 값을 찾아야하기 때문에 그리드 서치를 사용합니다. 
GridSearchCV 클래스는 하이퍼파라미터 탐색과 교차 검증을 한번에 수행해줍니다. 
한 예시로 기본 매개변수를 사용한 결정트리 모델에서 min_impurity_decrease 매개변수의 최적값을 찾아보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">,</span> <span class="mf">0.0003</span><span class="p">,</span> <span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">]}</span>
</code></pre></div></div>
<p>여기서는 5개의 값을 시도합니다. 
결정트리 클래스의 객체를 생성하고 바로 전달합니다.<br />
gs 객체에 fit 메서드를 호출합니다. 메서드를 호출하면 그리드 서치 객체는 min_impurity_decrease 값을 바꿔가며 5번 실행합니다. 
많은 모델을 훈련하기 때문에 n_jobs 매개변수를 사용하여 미리 사용할 CPU 코어수를 지정해줄 수 있습니다. 
이 매개변수의 기본 값은 1로, -1로 지정하면 시스템에 있는 모든 코어를 사용합니다. 
사이킷런의 그리드 서치는 훈련이 끝나면 best_estimator_속성에 가장 좋은 모델을 저장합니다. 
그리고 그리드 서치로 찾은 최적의 매개변수는 best_params_ 속성에 저장되어있습니다. 
확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
             param_grid={‘min_impurity_decrease’: [0.0001, 0.0002, 0.0003,
                                                   0.0004, 0.0005]})</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">gs</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9615162593804117</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘min_impurity_decrease’: 0.0001}</p>
</blockquote>

<p>0.0001이 가장 좋은 값으로 선택되었습니다. 
각 매개변수에서 수행한 교차 검증의 평균 점수는 mean_test_score에 저장되어있습니다. 
그리고 어떤 값이 큰지 확인하기 위해 넘파이의 argmax() 함수를사용합니다. 
그다음 그 인덱스를 이용하여 params키에 저장된 매개변수를 출력합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'params'</span><span class="p">][</span><span class="n">best_index</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>{‘min_impurity_decrease’: 0.0001}</p>
</blockquote>

<p>이제 더 복잡한 매개변수 조합들에서도 사용해보겠습니다. params에 다음과 같이 여러 매개변수의 범위를 설정하고 그리드 서치를 실행해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">),</span>
          <span class="s">'max_depth'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
          <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
             param_grid={‘max_depth’: range(5, 20),
                         ‘min_impurity_decrease’: array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,
       0.0009]),
                         ‘min_samples_split’: range(2, 100, 10)})</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘max_depth’: 14, ‘min_impurity_decrease’: 0.0004, ‘min_samples_split’: 12}</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8683865773302731</p>
</blockquote>

<h2 id="랜덤서치">랜덤서치</h2>

<p>매개변수의 값의 범위를 미리 정하기 어려울 때, 그리드 서치 수행시간이 너무 오래걸릴 때 랜덤서치를 사용할 수 있습니다. 
랜덤 서치에는 매개변수를 샘플링할 수 있는 확률분포를 전달합니다. 
확률 분포 클래스를 임포트하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">randint</span>
</code></pre></div></div>
<p>임포트한 두 클래스 모두 주어진 범위 내에서 고르게 값을 뽑습니다. 
차이점은 randint 값은 정숫값을, uniform은 실수값을 뽑습니다. 
0과 10 사이의 범위를 갖는 randint 객체를 만들어 샘플링을 해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rgen</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">rgen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>array([3, 6, 1, 2, 3, 1, 7, 6, 5, 1])</p>
</blockquote>

<p>고르게 샘플링이 되는지 확인하기 어려우니, 1000개를 샘플링해서 각 갯수를 세어봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">rgen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([ 79,  89, 110, 112, 106, 104,  95,  99, 104, 102]))</p>
</blockquote>

<p>꽤 고르게 샘플링이 된 것을 확인할 수 있습니다.
마찬가지로 uniform도 사용해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ugen</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ugen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>array([0.38360187, 0.85618662, 0.41423051, 0.18153867, 0.36998781,
       0.93449236, 0.36294108, 0.2842492 , 0.26135179, 0.12996384])</p>
</blockquote>

<p>난수 발생기와 유사한 형태로 출력되었습니다.</p>

<p>이제 탐색할 매개변수의 딕셔너리를 만들어줍니다. 
탐색할 매개변수의 범위를 다음과 같이 지정합니다. 
min_samples_leaf 매개변수는 리프 노드가 되기 위한 최소 샘플의 개수입니다. 이보다 자식노드의 샘플수가 적을경우 분할하지 않는다는 의미입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">),</span>
          <span class="s">'max_depth'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
          <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
          <span class="s">'min_samples_leaf'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
          <span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> 
                        <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                   n_iter=100, n_jobs=-1,
                   param_distributions={‘max_depth’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f08156b80&gt;,
                                        ‘min_impurity_decrease’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f510ecfd0&gt;,
                                        ‘min_samples_leaf’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f081568b0&gt;,
                                        ‘min_samples_split’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f202cc2b0&gt;},
                   random_state=42)</p>
</blockquote>

<p>params에서 정의된 범위에서 총 100번을 샘플링하여 교차검증을 수행하여 최적의 값을 찾습니다.
샘플링 횟수는 n_iter 매개변수에 지정해주었습니다.
이제 최적의 매개변수의 값을 출력하고 최고의 교차 검증 점수도 확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘max_depth’: 39, ‘min_impurity_decrease’: 0.00034102546602601173, ‘min_samples_leaf’: 7, ‘min_samples_split’: 13}</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8695428296438884</p>
</blockquote>

<p>best_estimator_에 저장되어있는 최적의 모델을 사용하여 테스트의 성능을 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">gs</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.86</p>
</blockquote>

<p>오늘은 수동으로 매개변수를 바꾸는 대신 그리드 서치나 랜덤 서치를 이용하여 최적의 매개변수를 찾는 방법을 알아보았습니다. 다음에는 결정 트리를 확장한 앙상블 모델에 대해 알아보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[5-2 교차 검증과 그리드 서치]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 5. 트리 알고리즘</title><link href="http://localhost:4000/ml/HG5-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 5. 트리 알고리즘" /><published>2022-05-27T00:00:00+09:00</published><updated>2022-05-27T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG5-1</id><content type="html" xml:base="http://localhost:4000/ml/HG5-1/"><![CDATA[<p>결정 트리 알고리즘을 사용하여 새로운 분류 문제를 다루어보도록 하겠습니다.</p>

<p>우선적으로 와인 데이터의 여러 변수를 사용하여 와인 종류를 구별할 수 있도록 로지스틱 회귀를 사용합니다.</p>

<h2 id="로지스틱-회귀로-와인-분류하기">로지스틱 회귀로 와인 분류하기</h2>

<p>판다스를 이용해 데이터를 불러옵니다. head() 메서드를 사용해 데이터를 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/wine-date'</span><span class="p">)</span>
<span class="n">wine</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-1.png?raw=true" alt="HG5-1-1" width="400" height="400" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-2.png?raw=true" alt="HG5-1-2" width="400" height="400" /><br />
<br />
와인 데이터 프레임의 각 데이터 타입과 누락된 값이 있는지 확인하기 위해 .info() 메서드를 사용합니다. 결과적으로 총 6497열 중에 각 column 값이 6497인 것을 보아 누락된 데이터는 없다는 것을 확인했습니다.<br />
다음으로 describe() 메서드를 사용하여 각 column에 대한 간단한 통계값을 출력합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-3.png?raw=true" alt="HG5-1-3" width="500" height="500" /><br />
이 값에서 확인할 수 잇는 점은 각 column에 대해 스케일이 다르다는 것입니다. 따라서 앞서 진행했던 것처럼 표준화를 해주어야 합니다. 표준화에 앞서 배열을 넘파이 형식으로 바꾼 후에 훈련 데이터와 테스트 데이터로 나누는 작업을 진행합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s">'class'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(5197, 3) (1300, 3)</p>
</blockquote>

<p>훈련 세트와 테스트 세트의 갯수를 확인해봤습니다. 이제 StandardScaler을 사용해서 표준화합니다. 그리고 그 데이터를 사용하여 로지스틱 회귀 모델을 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>

<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.7808350971714451<br />
0.7776923076923077</p>
</blockquote>

<p>모델의 점수가 높지 않은 것을 확인할 수 있습니다. 로지스틱 회귀가 학습한 계수와 절편을 출력해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]</p>
</blockquote>

<p>모델을 설명하기 위해 로지스틱 회귀가 학습한 계수들을 출력해보았습니다. 그러나 한눈에 알아보기 어렵다는 단점이 있고, 관계를 쉽게 설명할 수 없기 때문에 비교적 설명이 쉬운 결정트리 모델을 학습해보겠습니다.</p>

<h2 id="결정트리">결정트리</h2>

<p>결정트리 모델을 사이킷런의 DecisionTreeClassifier 클래스에서 제공합니다. 모델을 훈련한 후 점수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.996921300750433
0.8592307692307692</p>
</blockquote>

<p>정확도가 아까보다 높게 출력되었습니다. 다만 점수 차이를 보니 과대적합이 된 것 같습니다. 모델을 그림으로 표현해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-4.png?raw=true" alt="HG5-1-4" /><br />
트리 그림이 너무 복잡하기 때문에 깊이를 제한해서 다시 그려보겠습니다. 트리의 깊이는 max_depth() 매개변수를 사용하면 됩니다. filled는 노드의 색을 칠할 수 있고 feature_names는 특성의 이름을 전달할 수 있습니다. 다음과 같은 매개변수들을 사용해서 다시 그림을 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-5.png?raw=true" alt="HG5-1-5" /></p>

<h3 id="가지치기">가지치기</h3>

<p>결정트리에서는 노드 수가 너무 많아지면 훈련 데이터에 과대적합될 수 있기 때문에 노드의 수를 조절하는 가지치기가 필요합니다. 가지치기를 할 수 있는 가장 간단한 방법은 max_depth를 조절하는 것입니다. 루트 노드 아래로 3개의 노드만 만들어 모델을 훈련해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8454877814123533<br />
0.8415384615384616</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-6.png?raw=true" alt="HG5-1-6" /></p>

<p>훈련 세트의 점수는 낮아지고 테스트 세트 점수는 거의 비슷하기 때문에 그림으로 그려보았습니다. 
깊이 3에 있는 마지막 노드들이 최종 노드인 리프 노드입니다. 
세 번째에 위치한 노드만 음성 클래스가 더 많기 때문에 이 노드에 도착한 것들만 레드 와인으로 예측합니다. 
루트 노드부터 이 노드까지 도달하기 위해서는 당도가 -0.239보다 작고 -0.802보다도 작아야 합니다. 알코올 도수는 0.454보다도 작아야 이 노드에 도달할 수 있습니다.<br />
<br />
결정트리는 불순도에 따라 샘플을 나누는데, 불순도는 클래스별 비율을 가지고 계산하기 때문에 표준화 작업이 필요가 없습니다. 
그렇다면 원래 데이터를 가지고 결정트리 모델을 다시 훈련해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8454877814123533<br />
0.8415384615384616</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-7.png?raw=true" alt="HG5-1-7" /><br />
점수는 동일하지만, 특성값이 음수로 나오지 않기 때문에 설명하기 쉬워졌습니다. 
마지막으로 결정 트리에서 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산합니다. 
feature_importances_ 속성으로 알아볼 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[0.12345626 0.86862934 0.0079144 ]</p>
</blockquote>

<p>두번째 값인 당도가 0.87 정도로 특성 중요도가 가장 높은 것을 볼 수 있습니다. 
특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더해서 계산합니다. 
특성 중요도를 활용하면 결정트리 모델을 특성 선택에 활용할 수 있습니다. 
성능이 매우 좋지는 않지만 설명하기는 좋은 모델이라고 생각할 수 있습니다.</p>

<p>다음 절에서는 결정 트리의 다양한 매개변수들, 하이퍼파라미터를 자동으로 찾을 수 있는 방법에 대해 알아보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[5-1 결정 트리]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 4. 다양한 분류 알고리즘(2)</title><link href="http://localhost:4000/ml/HG4-2/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 4. 다양한 분류 알고리즘(2)" /><published>2022-05-27T00:00:00+09:00</published><updated>2022-05-27T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG4-2</id><content type="html" xml:base="http://localhost:4000/ml/HG4-2/"><![CDATA[<h2 id="확률적-경사-하강법">확률적 경사 하강법</h2>
<p>확률적 경사 하강법이란 Stochastic Gradient Descent로써, 무작위로 배치 크기가 1인 데이터를 추출하여 기울기를 계산하고 경사 하강 알고리즘을 적용하는 방법을 말합니다. 랜덤하게 하나의 샘플을 택하여 하강 알고리즘을 적용하기 때문에 전체 샘플을 사용하지 않습니다. 이러한 형태로 전체 샘플을 모두 사용할 때 까지 알고리즘을 여러번 적용한다고 생각하면 됩니다.<br />
<br /></p>

<p>우선 pandas를 사용하여 데이터를 불러오겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#dataframe 생성
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">fish</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/fish_csv'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">fish</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-2-1.png?raw=true" alt="HG4-2-1" /><br />
<br />
우선 데이터를 불러온 후에 Species 열을 제외한 열들을 입력 데이터로, Species 열을 타깃 데이터롤 설정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_input</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[[</span><span class="s">'Weight'</span><span class="p">,</span> <span class="s">'Length'</span><span class="p">,</span> <span class="s">'Diagonal'</span><span class="p">,</span> <span class="s">'Height'</span><span class="p">,</span> <span class="s">'Width'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">fish_target</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1">#train set와 test set으로 나누기
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">fish_input</span><span class="p">,</span> <span class="n">fish_target</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>fish 데이터를 sklearn을 이용하여 train set와 test set로 나눠줍니다. 
그리고 StandardScaler을 사용하여 train set와 test set를 표준화 전처리를 해주도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>
<p>이제 확률적 경사 하강법을 사용하기 위해 sklearn의 SGDClassifier 클래스를 이용합니다. SGDClassifier 클래스에는 loss 변수에 손실 함수의 종류를 지정해주고, max_iter 변수에는 수행할 에포크의 횟수를 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.773109243697479<br />
0.775</p>
</blockquote>

<p>점수가 낮게 나왔기 때문에 점수를 높이기 위해 이어서 훈련하여 점진적인 학습을 해보도록 합니다. 이어서 훈련할 때에는 partial_fit() 메서드를 사용하는데, 사용할 때 마다 1 epoch 씩 이어 훈련할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8151260504201681<br />
0.85</p>
</blockquote>

<p>확률적 경사 하강법을 사용한 모델은 에포크의 횟수에 따라 과대적합이나 과소적합 될 수 있기 때문에 에포크 횟수에 따른 훈련세트 점수와 테스트세트 점수를 그래프로 나타내 적절한 에포크의 값을 찾아줘야 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">train_target</span><span class="p">)</span>

<span class="c1">#300번의 epoch동안 훈련 반복 진행
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">):</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
    <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
    <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>

<span class="c1">#그래프 그리기
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-2-2.png?raw=true" alt="HG4-2-2" /></p>

<p>결과인 그래프를 보면 초반에는 과소적합되어 훈련세트와 테스트세트 점수가 낮고, 약 100번째 에포크 이후에는 훈련세트와 테스트 세트의 점수가 벌어지고 있는 것을 볼 수 있습니다. 100번째 에포크가 적절한 값인 것을 확인할 수 있습니다. 반복횟수를 100으로 설정하고 모델을 다시 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span> 
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.957983193277311<br />
0.925</p>
</blockquote>

<p>최종 점수가 잘 나오는 것을 확인할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[4-2 확률적 경사 하강법]]></summary></entry><entry><title type="html">[Github] 깃허브 메인 페이지에 잔디심기 띄우기</title><link href="http://localhost:4000/github/GitAction/" rel="alternate" type="text/html" title="[Github] 깃허브 메인 페이지에 잔디심기 띄우기" /><published>2022-05-14T00:00:00+09:00</published><updated>2022-05-14T00:00:00+09:00</updated><id>http://localhost:4000/github/GitAction</id><content type="html" xml:base="http://localhost:4000/github/GitAction/"><![CDATA[<p>오늘은 Github Actions를 사용하여 잔디심기 대시보드를 다음과 같이 만들어보겠습니다.<br />
참고한 포스트는 게시글 하단에 작성해 두었습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A1.png?raw=true" alt="A1" /></p>

<h2 id="1-잔디심기-대시보드-만들-레포지토리-선택해서-권한-설정">1. 잔디심기 대시보드 만들 레포지토리 선택해서 권한 설정</h2>
<p>잔디심기 대시보드를 표시할 레포지토리를 정합니다. 저는 깃허브 프로필 페이지에 띄울것이기 때문에 고유 아이디로 된 yhp2205 레포지토리로 결정했습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A2.png?raw=true" alt="A2" /><br />
<br />
레포지토리를 정했으면 그 레포지토리의 권한을 설정하기 위해 settings를 눌러 들어갑니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A3.png?raw=true" alt="A3" /><br />
<br />
settings의 좌측 카테고리에서 <code class="language-plaintext highlighter-rouge">Actions - General</code>로 들어가서</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A4.png?raw=true" alt="A4" /><br />
<br />
페이지 하단에 위치한 Workflow permissions를 다음과 같이 설정해줍니다.</p>

<h2 id="2-metrics-기능-fork-해서-가져오기">2. metrics 기능 fork 해서 가져오기</h2>
<p><a href="https://github.com/lowlighter/metrics">다음 사이트</a>에 접속하여 metrics 기능을 fork 해서 가져오겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A5.png?raw=true" alt="A5" /><br />
<br />
이런 페이지가 뜨면 우측 상단의 fork 버튼을 누르고</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A6.png?raw=true" alt="A6" /><br />
<br />
Create fork를 눌러 본인의 깃허브 계정으로 가져옵니다.</p>

<h2 id="3-personal-token-생성하기">3. personal token 생성하기</h2>
<p>metrics를 가져왔으니 계정의 token을 생성해보겠습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A7.png?raw=true" alt="A7" /><br />
<br />
다음과 같이 계정의 settings로 들어갑니다. (레포지토리의 settings가 아닌 깃허브 계정의 settings로 들어갑니다)</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A8.png?raw=true" alt="A8" /><br />
<br />
우측 카테고리 제일 하단의 Developer settings를 누릅니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A9.png?raw=true" alt="A9" /><br />
<br />
다음과 같이 <code class="language-plaintext highlighter-rouge">personal access token - Generate new token</code>을 눌러줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A10.png?raw=true" alt="A10" /><br />
<br /></p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A11.png?raw=true" alt="A11" /><br />
<br />
note에 token의 이름을 지정해주고, 유효기간을 설정합니다. 원하는 대로 설정하면 됩니다. 그러나 토큰의 유효기간을 짧게 설정할 경우, 그 주기마다 토큰을 다시 만들어 업데이트 해줘야하기 때문에 기한 없이 설정하는 것을 권장합니다. 그리고 아래 repo, read::packages, read:org, gist, read:user를 선택한 후에 토큰을 만들어 줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A12.png?raw=true" alt="A12" /><br />
<br />
토큰을 성공적으로 생성했을 때 다음과 같은 페이지가 뜨고 빨간 박스로 표시한 아이콘을 눌러 토큰을 복사합니다. 잃어버리면 안되기 때문에 다른 메모장에 적어두셔도 좋습니다!</p>

<h2 id="4-actions-secrets에-token-등록하기">4. Actions secrets에 token 등록하기</h2>
<p>토큰을 만들어 잘 복사해두었다면 다음으로 잔디심기를 올릴 레포지토리에 토큰을 등록해보겠습니다.<br />
<br /></p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A13.png?raw=true" alt="A13" /><br />
<br />
잔디심기 대시보드를 만들 레포지토리에 들어갑니다. 1에서 권한을 설정해 주었던 레포지토리의 <code class="language-plaintext highlighter-rouge">settings - secrets - actions</code>를 차례로 눌러 들어가줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A14.png?raw=true" alt="A14" /><br />
<br />
<code class="language-plaintext highlighter-rouge">New repository secret</code>을 눌러 토큰을 등록하겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A15.png?raw=true" alt="A15" /><br />
<br />
다음과 같은 창이 뜨면 토큰의 이름을 작성하고(저는 YOUNGHYUNS_METRICS_TOKEN으로 등록하였습니다), Value 값에 복사해둔 token값을 넣고 Add secret 해줍니다.</p>

<h2 id="5-repo-내에-yml-파일-만들기">5. repo 내에 yml 파일 만들기</h2>

<p>깃허브 토큰 등록한 repo 내에 yml 파일을 만들겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A16.png?raw=true" alt="A16" /><br />
<br />
<code class="language-plaintext highlighter-rouge">Actions - set up a workflow yourself</code>를 누릅니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a basic workflow to help you get started with Actions

name: CI

# Controls when the workflow will run
on:
  # 하루에 한 번 씩 빌드 수행
  schedule:
    - cron: '0 1 * * *'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: [빌드를 수행할 때 표시할 작업명]
        uses: [metrics를 fork한 저장소명]/metrics@latest
        with:
          token: $
          filename: [빌드한 후 생성할 파일명].svg
          base: ""
          plugin_isocalendar: yes
          plugin_isocalendar_duration: full-year
</code></pre></div></div>
<p>다음 코드를 복사해서 붙여넣습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A17.png?raw=true" alt="A17" /><br />
<br />
다음과 같이 <code class="language-plaintext highlighter-rouge">activity_metrics_build.yml</code>로 이름을 지정해주고 앞선 코드에서 표시된 부분을 본인의 repo에 맞게 작성합니다. 하늘색 박스로 표시된 부분에는 전에 등록해준 토큰의 이름을 작성하면 됩니다. 저는 <code class="language-plaintext highlighter-rouge">YOUNGHYUNS_METRICS_TOKEN</code> 으로 지정했기 때문에 그대로 작성하였습니다.<br />
<br />
작성이 완료되었다면 우측 상단에 위치한 Start commit 버튼을 눌러 Commit new file을 해줍니다.</p>

<h2 id="6-workflow-실행하여-svg-파일-생성하기">6. Workflow 실행하여 svg 파일 생성하기</h2>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A18.png?raw=true" alt="A18" /><br />
<br />
commit을 완료하여 yml 파일을 생성했다면 다시 Actions 탭으로 들어갑니다. 그럼 다음과 같이 CI라는 이름의 workflow가 생성된 것을 확인할 수 있습니다. CI를 선택한 후에, 빨간색으로 표시되어 있는 Run Workflow를 눌러 workflow를 실행합니다.<br />
<br />
CI가 실행되는 데에는 약 5분정도의 시간이 걸립니다. 주황색 불이 들어와서 실행중임을 알리고 있다면 빌드가 완료될 때까지 기다립니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A19.png?raw=true" alt="A19" />
<br />
시간이 지난 후, 초록 표시가 들어오고 빌드가 완료된 것을 알리면, 레포지토리 내에 다음과 같이 svg 파일이 생성된 것을 볼 수 있습니다.</p>

<h2 id="7-잔디심기-대시보드-게시하기">7. 잔디심기 대시보드 게시하기</h2>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A20.png?raw=true" alt="A20" /><br />
<br />
마지막으로 동일 레포지토리의 Readme.md 파일에 다음과 같이 작성해줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A21.png?raw=true" alt="A21" /><br />
<br />
정상적으로 실행되었을 때, 다음과 같이 깃허브 메인 화면에서 잔디심기 보드를 확인할 수 있습니다.</p>

<blockquote>
  <p>참고한 포스트 <a href="https://ynkim0.github.io/posts/Github-Actions%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%94%EB%94%94%EC%8B%AC%EA%B8%B0-%EB%8C%80%EC%89%AC%EB%B3%B4%EB%93%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0/">Github Actions를 이용한 잔디심기 대쉬보드 만들기</a></p>
</blockquote>]]></content><author><name>Younghyun Park</name></author><category term="Github" /><category term="Github" /><category term="Github actions" /><summary type="html"><![CDATA[Github Actions를 사용한 잔디심기 대시보드 만들기]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 4. 다양한 분류 알고리즘</title><link href="http://localhost:4000/ml/HG4-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 4. 다양한 분류 알고리즘" /><published>2022-05-04T00:00:00+09:00</published><updated>2022-05-04T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG4-1</id><content type="html" xml:base="http://localhost:4000/ml/HG4-1/"><![CDATA[<h2 id="로지스틱-회귀">로지스틱 회귀</h2>

<p>랜덤하게 담긴 생선의 확률을 알아보려고 합니다.<br />
우선 앞서 배웠던 k-최근접 이웃 분류기를 사용하여 구한 이웃 클래스를 토대로 타깃 생선의 확률을 계산해보겠습니다.<br />
먼저 데이터를 준비합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">fish</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/fish_csv'</span><span class="p">)</span>
<span class="n">fish</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-1.png?raw=true" alt="HG4-1-1" /><br />
<br />
그리고 target 데이터가 될 Species 열에 어떤 종류가 있는지 판다스의 unique 함수를 사용해서 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Roach’ ‘Whitefish’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Smelt’]</p>
</blockquote>

<p>Species 에 들어있는 종들을 확인했으니 fish 데이터에서 Species 열을 제외하고 나머지 5개의 열을 입력 데이터로 선택합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_input</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[[</span><span class="s">'Weight'</span><span class="p">,</span> <span class="s">'Length'</span><span class="p">,</span> <span class="s">'Diagonal'</span><span class="p">,</span> <span class="s">'Height'</span><span class="p">,</span> <span class="s">'Width'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">fish_input</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[[242.      25.4     30.      11.52     4.02  ]<br />
 [290.      26.3     31.2     12.48     4.3056]<br />
 [340.      26.5     31.1     12.3778   4.6961]<br />
 [363.      29.      33.5     12.73     4.4555]<br />
 [430.      29.      34.      12.444    5.134 ]]</p>
</blockquote>

<p>입력 데이터가 잘 생성된 것을 확인했습니다.<br />
 동일한 방식으로 target 데이터도 만듭니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_target</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>
<p>이제 train set와 test set로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">fish_input</span><span class="p">,</span> <span class="n">fish_target</span><span class="p">)</span>
</code></pre></div></div>
<p>train 세트와 test를 준비했으니 데이터를 표준화 전처리를 해줍니다.<br />
사이킷런의 StandardScaler을 이용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#훈련 테스트 세트 표준화 전처리
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>
<p>사이킷런의 KNeighborsClassifier 클래스 객체를 만들고 훈련세트와 테스트세트의 각 점수를 확인해보겠습니다.<br />
최근접 이웃의 개수인 k는 3으로 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#최근접 이웃 분류기의 확률 예측
</span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">kn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">kn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8739495798319328<br />
0.75</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]</p>
</blockquote>

<p>알파벳 순서로 매겨진 것을 확인했습니다.<br />
이제 predict 메서드를 사용하여 test set에 있는 처음 5개의 샘플의 타깃값을 예측합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]</p>
</blockquote>

<p>predict 예측 값이 어떻게 나왔는지 predict_proba() 메서드를 사용하면 클래스별 확률 값을 확인할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">kn</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>  <span class="c1">#소숫점 넷째자리까지 표기(다섯번째에서 반올림)
</span></code></pre></div></div>
<blockquote>
  <p>[[0.     0.6667 0.3333 0.     0.     0.     0.    ]<br />
 [0.     0.     1.     0.     0.     0.     0.    ]<br />
 [0.     0.     0.3333 0.     0.6667 0.     0.    ]<br />
 [0.     0.     1.     0.     0.     0.     0.    ]<br />
 [0.     0.     0.3333 0.     0.3333 0.3333 0.    ]]</p>
</blockquote>

<p>첫번째 열이 Bream, 두번째 열이 Parkki, 세번째 열이 Perch, … 에 대한 확률을 나타내고 있습니다.<br />
 계산한 비율이 올바른지 확인하기 위해 첫 번째 샘플의 최근접 이웃들을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distances</span><span class="p">,</span> <span class="n">indexes</span> <span class="o">=</span> <span class="n">kn</span><span class="p">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_target</span><span class="p">[</span><span class="n">indexes</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[[‘Parkki’ ‘Parkki’ ‘Perch’]]</p>
</blockquote>

<p>첫 번째 샘플의 이웃은 Perch가 1개, Parkki가 2개임을 확인합니다.<br />
여기까지 k-최근접 이웃 분류기를 사용한 확률 예측을 해보았습니다.<br />
<br />
이제 본격적으로 로지스틱 회귀를 이용한 예측을 해보도록 하겠습니다.</p>

<p>로지스틱 회귀는 선형회귀와 동일하게 선형 방정식을 학습합니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-2.png?raw=true" alt="HG4-1-2" /><br />
<br />
여기서 각 변수 앞에 곱해진 값들은 가중치나 계수의 역할을 합니다.<br />
z는 어떤 값도 가능하지만, 확률이 되기 위해서는 0과 1 사이의 값을 가져야 합니다.<br />
z가 큰 음수일때 0이 되고 큰 양수일 때 1이 되도록 하려면 시그모이드 함수를 사용하면 됩니다.</p>

<p><img src="https://mblogthumb-phinf.pstatic.net/MjAxNzA5MjZfMzMg/MDAxNTA2NDIzMjMzNjE5.arSKSXckfkmGqMIEA0dAI1q_e080ntMcpyhXhBx2m4Ig.nofSEy_ifZ6BM7VvFOku71bQbM8d2ngYWwV_sO8oyNEg.PNG.junhyuk7272/%EC%9E%90%EC%97%B0%EC%A7%80%EC%88%98%ED%95%A8%EC%88%98.png?type=w2" alt="HG4-1-32" />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-3.png?raw=true" alt="HG4-1-3" /><br />
<br /></p>

<p>시그모이드 함수는 다음과 같은 형태를 띕니다.<br />
넘파이를 사용해서 그래프를 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">phi</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'z'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'phi'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-4.png?raw=true" alt="HG4-1-4" /><br />
<br /></p>

<p>이제 로지스틱 회귀를 사용하여 이진분류를 해보겠습니다.<br />
train 데이터에서 불리언 인덱싱을 사용하여 도미와 빙어만 따로 출력해줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#도미(Bream)와 빙어(Smelt)만 출력하기
</span><span class="n">bream_smelt_indexes</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_target</span> <span class="o">==</span> <span class="s">'Bream'</span><span class="p">)</span><span class="o">|</span><span class="p">(</span><span class="n">train_target</span> <span class="o">==</span> <span class="s">'Smelt'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">bream_smelt_indexes</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[False  True False False False False  True False False  True False  True
  True False False False  True  True False  True False False  True False
 False False  True False False False False False False False  True False
  True  True False  True False False  True False False  True False  True
 False False  True  True False False False  True  True False False False
  True  True False False False False False False False  True False  True
 False False  True False False False False  True False False False False
  True False  True False False False False False False False  True  True
 False False False False  True  True False False False False False False
 False False False False  True  True  True False False  True False]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_bream_smelt</span> <span class="o">=</span> <span class="n">train_scaled</span><span class="p">[</span><span class="n">bream_smelt_indexes</span><span class="p">]</span>
<span class="n">target_bream_smelt</span> <span class="o">=</span> <span class="n">train_target</span><span class="p">[</span><span class="n">bream_smelt_indexes</span><span class="p">]</span>
</code></pre></div></div>
<p>이 데이터를 이용하여 로지스틱 회귀 모델을 훈련하고 그 모델을 사용하여 train_bream_smelt에 있는 처음 5개의 샘플을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">,</span> <span class="n">target_bream_smelt</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Bream’]</p>
</blockquote>

<p>다섯 번째를 도미, 나머지를 빙어로 예측했습니다.<br />
5개 샘플의 확률을 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[[0.03391154 0.96608846]<br />
 [0.03603855 0.96396145]<br />
 [0.03582089 0.96417911]<br />
 [0.02900509 0.97099491]<br />
 [0.99410165 0.00589835]]</p>
</blockquote>

<p>두 개의 열 중에 어떤 것이 도미이고 빙어인지 헷갈린다면 classes_ 를 사용하여 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Smelt’]</p>
</blockquote>

<p>이제 로지스틱 회귀가 학습한 계수를 확인합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[-0.42646881 -0.60256452 -0.68252074 -0.99456193 -0.78263044]] [-2.28791769]</p>
</blockquote>

<p>이를 통해 로지스틱 회귀 모델이 학습한 방정식을 다음과 같이 도출할 수 있습니다. 
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-5.png?raw=true" alt="HG4-1-5" /><br />
<br />
이제 이 방정식을 이용하여 z 값을 계산해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decisions</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">decisions</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[ 3.34950001  3.28646197  3.29274597  3.51084984 -5.12716733]</p>
</blockquote>

<p>이 z값을 시그모이드 함수에 통과하면 확률을 얻을 수 있습니다.<br />
scipy에 시그모이드 함수를 제공하는 expit()를 사용해서 확률을 구합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="k">print</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="n">decisions</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>[0.96608846 0.96396145 0.96417911 0.97099491 0.00589835]</p>
</blockquote>

<p>이제 다중 분류를 수행해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9411764705882353<br />
0.85</p>
</blockquote>

<p>test 세트의 처음 5개 샘플에 대한 예측을 출력해봅니다.<br />
그리고 그 예측에 대한 확률을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set의 처음 5개 샘플에 대한 예측
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set의 처음 5개에 대한 예측 확률
</span><span class="n">proba</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span> <span class="c1">#소수점 세번째자리까지 표기
</span></code></pre></div></div>
<blockquote>
  <p>[[0.01  0.73  0.006 0.    0.223 0.    0.03 ]<br />
 [0.01  0.    0.898 0.002 0.    0.    0.089]<br />
 [0.001 0.018 0.28  0.002 0.661 0.    0.038]<br />
 [0.002 0.    0.953 0.001 0.    0.    0.044]<br />
 [0.    0.012 0.816 0.    0.119 0.051 0.002]]</p>
</blockquote>

<p>5개 샘플과 7개의 클래스가 있기 때문에 5개의 행과 7개의 열로 출력이 되었습니다.<br />
각 열이 의미하는 것이 어떤 생선에 대한 확률인지 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#classes_속성으로 클래스 정보 확인
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(7, 5) (7,)</p>
</blockquote>

<p>coef_ 배열이 7행에 5열의 형태를 띄고 있고 intercept_도 7개가 있는 것을 확인할 수 있습니다.<br />
7개의 클래스가 있기 때문에 클래스마다 z값을 계산하여 총 7개의 z값이 나온다는 말과 같습니다.</p>

<p>이와 같이 7개의 z값들을 확률로 바꾸기 위해 소프트맥스 함수를 사용합니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-6.png?raw=true" alt="HG4-1-6" /><br />
<br />
7개의 z값을 소프트맥스 함수를 이용하여 확률로 바꾸면 0과 1사이의 값을 가진 확률이 출력됩니다.<br />
그럼 7개의 z값을 구한 후에 소프트맥스 함수를 사용하여 확률로 바꾸어보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#z1부터 z7까지의 값 구하고 확률로 바꾸기
</span><span class="n">decision</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#소수점 둘째자리까지 표기
</span></code></pre></div></div>
<blockquote>
  <p>[[ -0.17   4.13  -0.67  -3.44   2.95  -3.75   0.96]<br />
 [  4.31  -5.68   8.79   2.78   0.69 -17.37   6.48]<br />
 [ -2.69   0.28   3.04  -2.01   3.9   -3.56   1.05]<br />
 [  3.78  -6.28   9.85   2.82   0.69 -17.63   6.78]<br />
 [ -8.63   1.35   5.59  -4.1    3.66   2.82  -0.7 ]]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))</span> <span class="c1">#소수점 셋째자리까지 표시
</span></code></pre></div></div>
<blockquote>
  <p>[[0.01  0.73  0.006 0.    0.223 0.    0.03 ]<br />
 [0.01  0.    0.898 0.002 0.    0.    0.089]<br />
 [0.001 0.018 0.28  0.002 0.661 0.    0.038]<br />
 [0.002 0.    0.953 0.001 0.    0.    0.044]<br />
 [0.    0.012 0.816 0.    0.119 0.051 0.002]]</p>
</blockquote>

<p>이렇게 로지스틱 회귀를 사용하여 7개의 클래스에 대한 확률을 예측하는 모델을 만들었습니다.<br />
다음에는 확률적 경사 하강법에 대해 배워보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[4-1 로지스틱 회귀]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(3)</title><link href="http://localhost:4000/ml/HG3-3/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(3)" /><published>2022-05-03T00:00:00+09:00</published><updated>2022-05-03T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG3-3</id><content type="html" xml:base="http://localhost:4000/ml/HG3-3/"><![CDATA[<p>앞서 훈련했던 모델에서 훈련 세트보다 테스트 세트가 더 점수가 높게 나왔습니다.<br />
이 문제를 해결하기 위해 농어의 길이 뿐만 아니라 농어의 높이와 두께 등의 여러 특성들을 추가로 사용해보겠습니다.<br />
또한 이전에 사용했던 방법인 각 항을 제곱하여 데이터에 추가하는 것과 각 특성을 서로 곱해 새로운 특성을 만드는 방식을 사용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># data load
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/perch_csv'</span><span class="p">)</span>
<span class="n">perch_full</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">perch_full</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[ 8.4   2.11  1.41]<br />
 [13.7   3.53  2.  ]<br />
 [15.    3.82  2.43]<br />
 [16.2   4.59  2.63]<br />
 [17.4   4.59  2.94]<br />
 [18.    5.22  3.32]<br />
…</p>
</blockquote>

<p>이번에는 데이터가 여러 특성이 있음을 고려하여 pandas를 이용해 직접 csv 데이터를 불러오는 방식을 사용했습니다.<br />
이제 이전과 동일한 방식으로 타깃 데이터를 준비하고, perch_full과 perch_weight를 훈련 세트와 테스트 세트로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span>
       <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span>
       <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span>
       <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span>
       <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1000.0</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">perch_full</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>사이킷런의 sklearn.preprocessing 패키지의 PolynomialFeatures 클래스를 사용해서 새로운 특성을 만들어 보겠습니다.<br />
PolynomialFeatures 클래스는 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가합니다.<br />
이때 기본적으로 1을 추가하여 각 특성에 곱함으로써 절편을 만드는데, 사이킷런 선형 모델은 자동으로 절편이 들어가있기 때문에 include_bias=False 인자를 추가함으로서 절편을 위한 항을 제거할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="c1"># train_input을 변환하여 train_poly 만들기
</span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">poly</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 9)</p>
</blockquote>

<p>PolynomialFeatures 클래스는 9개의 특성이 어떻게 반영되어있는지 확인하는 방법을 제공합니다.<br />
get_feature_names() 메서드를 사용하여 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poly</span><span class="p">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</code></pre></div></div>
<blockquote>
  <p>[‘x0’, ‘x1’, ‘x2’, ‘x0^2’, ‘x0 x1’, ‘x0 x2’, ‘x1^2’, ‘x1 x2’, ‘x2^2’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set 변환하기
</span><span class="n">test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

<span class="c1">#다중회귀 모델 훈련하기
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9903183436982124</p>
</blockquote>

<p>train set을 이용하여 점수를 내보았으니 test set도 점수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set 점수도 확인하기
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_poly</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9714559911594134</p>
</blockquote>

<p>테스트 세트 점수와 훈련세트 점수 모두 높게 나오면서 train 세트가 더 점수가 높게 나온 것을 확인할 수 있습니다.<br />
이제 특성을 더 많이 추가해보겠습니다.<br />
PolynomialFeatures 클래스의 degree 변수를 사용하면 고차항의 최대 차수를 지정할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">poly</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 55)</p>
</blockquote>

<p>만들어진 특성의 개수가 55개인 것을 확인할 수 있습니다.<br />
train_poly 배열의 열의 개수가 특성의 개수라고 생각하면 됩니다.<br />
이제 transform을 마쳤으니 이 데이터로 다시 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_poly</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9999999999991097<br />
-144.40579242684848</p>
</blockquote>

<p>결과를 보면 train set에 대한 점수는 매우 높게 나오고, test set에 대한 점수는 음수가 나왔습니다.<br />
이는 특성의 개수를 크게 늘렸기 때문에 train set에 대해 과대적합된 결과로 볼 수 있습니다.<br />
이러한 과대적합을 줄이기 위해서는 선형 회귀 모델의 계수를 규제하여 해결해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_poly</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_poly</span><span class="p">)</span>
</code></pre></div></div>

<p>선형 회귀 모델에 규제를 추가한 모델을 ridge와 lasso라고 부르는데, 릿지는 계수를 제곱한 값을 기준으로 규제를 적용하고, 라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.<br />
일반적으로는 릿지를 조금 더 선호하기 때문에 릿지 회귀를 먼저 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9896101671037343<br />
0.9790693977615397</p>
</blockquote>

<p>ridge 모델을 사용하여 훈련한 결과 많은 특성을 사용했음에도 불구하고 훈련세트, 테스트 세트 모두에서 좋은 점수가 나왔습니다.<br />
릿지와 라쏘 모델을 적용할 때 모델 객체 중 alpha 값을 사용함으로써 규제의 정도를 정할 수 있는데,
alpha값이 크면 규제의 강도가 세기 때문에 과소적합될 가능성이 크고,
작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지기 때문에 과대적합될 가능성이 큽니다.<br />
적절한 alpha 값을 찾기 위해서는 alpha 값에 대한 결정계수 값을 그려 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점을 최적의 alpha 값으로 생각하면 됩니다.<br />
그럼 alpha값을 바꿀 때 마다 score 값을 저장할 리스트를 먼저 만들어주겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#alpha 값 리스트 생성
</span><span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span> <span class="p">:</span>
  <span class="c1">#ridge model 생성
</span>  <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
  <span class="c1">#ridge model 훈련
</span>  <span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="c1">#훈련 점수와 테스트 점수 저장
</span>  <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
  <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<p>이제 alpha 값에 따른 그래프를 그려 값을 확인해보도록 하겠습니다.<br />
그냥 그리면 그래프 왼쪽이 너무 촘촘해지기 때문에 로그함수로 바꿔 표현합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'R^2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-3-1.png?raw=true" alt="HG3-3-1" /><br />
<br />
다음 그래프를 확인해보면, 적절한 alpha 값이 -1, 즉 0.1 이라는 것을 알 수 있습니다.<br />
alpha 값을 0.1로 지정하여 모델을 훈련하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9903815817570366<br />
0.9827976465386926</p>
</blockquote>

<p>train값이 test값보다 높게 나오면서 두 점수 모두 적절한 점수가 나온것을 확인할 수 있습니다.<br />
이제 lasso 회귀모델을 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span><span class="p">:</span>
  <span class="c1"># lasso model을 만들기
</span>  <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
  <span class="c1"># lasso model 훈련하기
</span>  <span class="n">lasso</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="c1"># train set와 test set score을 저장
</span>  <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
  <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>

<span class="c1">#x축을 로그스케일로 바꿔 그래프 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'R^2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-3-2.png?raw=true" alt="HG3-3-2" /><br />
<br />
위 그래프를 확인했을 때 올바른 alpha값은 1이라는것을 알 수 있습니다.<br />
이 값으로 lasso 모델을 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">lasso</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9888067471131867<br />
0.9824470598706695</p>
</blockquote>

<p>lasso 모델은 계수값을 0으로 만들 수 있습니다.<br />
라쏘모델의 계수중 0인것의 개수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">coef_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>40</p>
</blockquote>

<p>계수가 0인 항이 40개인 것을 확인할 수 있습니다.<br />
총 55개의 특성 중 15개만 사용한 것입니다.<br />
이런 특성이 있기 때문에 라쏘 모델을 유용한 특성을 골라내는 데에 사용하기도 합니다.<br />
릿지와 라쏘 회귀를 사용하여 최적의 alpha 값을 찾아보고 특성이 많은 데이터를 규제하여 모델의 성능을 확인해봤습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[3-3 특성 공학과 규제]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(1)</title><link href="http://localhost:4000/ml/HG3-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(1)" /><published>2022-05-02T00:00:00+09:00</published><updated>2022-05-02T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG3-1</id><content type="html" xml:base="http://localhost:4000/ml/HG3-1/"><![CDATA[<p>지도학습 알고리즘은 분류와 회귀로 나눌 수 있습니다.<br />
분류는 앞서 2장에서 했던 것처럼 클래스 중 하나로 분류하는 것이고, 회귀는 임의의 어떤 숫자를 예측하는 것입니다.</p>

<p>k-최근접 이웃 분류는 앞서 진행했고, 이번에는 k-최근접 이웃 회귀를 알아보겠습니다.<br />
k-최근접 이웃 회귀는 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하여 이웃 수치들의 평균을 구하는 방식입니다.<br />
<br /><br />
회귀분석에 쓰일 데이터를 불러온 후 산점도를 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#numpy import
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#Data load
</span><span class="n">perch_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.4</span><span class="p">,</span> <span class="mf">13.7</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">16.2</span><span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">18.0</span><span class="p">,</span> <span class="mf">18.7</span><span class="p">,</span> <span class="mf">19.0</span><span class="p">,</span> <span class="mf">19.6</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span>
       <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.3</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.7</span><span class="p">,</span>
       <span class="mf">23.0</span><span class="p">,</span> <span class="mf">23.5</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.6</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">,</span> <span class="mf">25.6</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">27.3</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span>
       <span class="mf">27.5</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">28.7</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">32.8</span><span class="p">,</span> <span class="mf">34.5</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">,</span> <span class="mf">36.5</span><span class="p">,</span> <span class="mf">36.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span>
       <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">42.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.5</span><span class="p">,</span>
       <span class="mf">44.0</span><span class="p">])</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span>
       <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span>
       <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span>
       <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span>
       <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1000.0</span><span class="p">])</span>

<span class="c1">#산점도 그리기
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-1-1.png?raw=true" alt="HG3-1-1" /><br />
<br />
다음과 같이 우상향하는 그래프가 출력되었습니다.<br />
농어 길이가 늘어날 수록 무게도 늘어나는 것은 확인했습니다.<br />
다음으로 데이터를 훈련 세트와 테스트 세트로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="p">)</span>
</code></pre></div></div>
<p>train_test_split 메서드는 총 데이터의 25%를 test 데이터로 떼어냅니다.<br />
random_state는 책과 동일한 결과가 나올 수 있게 한 것이기 때문에 생략해도 무방합니다.<br />
사이킷런을 사용해 모델을 훈련할 것이기 때문에 reshape을 사용하여 배열을 2차원으로 바꿔줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_input</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">test_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 1) (14, 1)</p>
</blockquote>

<p>reshape의 첫 번째 인자를 -1로 지정함으로써 원소의 개수로 채우라는 의미입니다. 배열의 전체 원소의 개수를 외우지 않아도 되기 때문에 편리합니다.<br />
이제 k-최근접 이웃 알고리즘을 훈련시켜보도록 하겠습니다.
사이킷런에서 KNeighborsRegressor을 사용하여 회귀 모델을 fit 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">knr</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">()</span>

<span class="c1">#k-최근접 이웃 회귀모델을 훈련합니다.
</span><span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.992809406101064</p>
</blockquote>

<p>거의 1에 가까운 숫자가 나왔습니다.<br />
2장에서 분류를 할 때는 정확도라고 불렀던 이 숫자를 회귀에서는 결정계수라고 부릅니다. 결정계수가 1에 가까워지면 예측이 타깃과 비슷하다는 의미가 됩니다.<br />
아제 타깃과 예측한 값의 차이를 구해보겠습니다. sklearn.metrics 패키지의 mean_absolute_error을 이용하여 둘 사이의 절댓값 오차를 평균을 낸 값을 출력합니다.<br />
그리고 훈련 세트를 사용하여 score값도 출력해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="c1">#테스트 세트에 대한 예측
</span><span class="n">test_prediction</span> <span class="o">=</span> <span class="n">knr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

<span class="c1">#테스트 세트의 평균 절댓값 오차 계산
</span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">test_prediction</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mae</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>19.157142857142862<br />
0.9698823289099254</p>
</blockquote>

<p>위 코드를 출력하면 평균적으로 오차가 19g정도 났다는 것을 확인할 수 있습니다.<br />
또한 훈련을 한 세트로 score을 냈는데도 불구하고 오히려 test set보다 결정계수가 더 낮게 나온 것을 확인할 수 있습니다.<br />
이처럼 훈련 세트보다 테스트 세트가 점수가 높거나 두 점수 모두 낮은 경우를 과소적합이라고 하고, 반대로 너무 차이나게 훈련 세트가 점수가 높을 경우를 과대적합이라고 합니다. 이번 훈련에서는 과소적합이 된 것입니다.  그렇다면 모델을 더 복잡하게 만듦으로써 이를 해결해보도록 하겠습니다.<br />
모델을 복잡하게 만들기 위해 k를 3으로 줄입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 이웃의 개수를 3으로 설정
</span><span class="n">knr</span><span class="p">.</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># 모델 훈련
</span><span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9804899950518966</p>
</blockquote>

<p>k값을 낮췄더니 결정계수의 값이 올라갔습니다. 이번엔 test 세트를 적용해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9746459963987609</p>
</blockquote>

<p>test set 또한 너무 낮지 않은 점수로 잘 나왔습니다.<br />
train set과 점수 차이도 많이 나지 않아 과대적합도 아닙니다. 이처럼 과소적합 시 모델을 더 복잡하게 (k값을 줄여서) 만들어야 하고, 과대적합 시 모델을 덜 복잡하게 (k값을 늘려서) 만듦으로써 해결할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[3-1 k-최근접 이웃 회귀]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/HGML.png" /><media:content medium="image" url="http://localhost:4000/assets/images/HGML.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>