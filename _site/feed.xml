<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-05-28T23:27:50+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">YOUNGHYUN’s Blog</title><subtitle>노력하는 흔적 남기기</subtitle><author><name>Younghyun Park</name></author><entry><title type="html">[혼공머신러닝] Ch 5. 트리 알고리즘(2)</title><link href="http://localhost:4000/ml/HG5-2/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 5. 트리 알고리즘(2)" /><published>2022-05-28T00:00:00+09:00</published><updated>2022-05-28T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG5-2</id><content type="html" xml:base="http://localhost:4000/ml/HG5-2/"><![CDATA[<h1 id="교차-검증과-그리드-서치">교차 검증과 그리드 서치</h1>

<p>테스트 세트 성능을 올바르게 판단하기 위해서는 모델을 전부 만든 뒤에 마지막에 한 번만 사용하는 것이 좋습니다. 그렇다면 테스트 세트를 사용하지 않고 어떻게 하이퍼 파라미터 튜닝을 할 수 있을지 알아보도록 하겠습니다.</p>

<h2 id="검증-세트">검증 세트</h2>
<p>테스트 세트를 사용하지 않고 모델이 과대적합인지 과소적합인지 알아보기 위해서는 훈련 세트를 또 나누는 방법을 취할 수 있습니다.
이 데이터를 검증세트라고 합니다.
훈련 세트에서 모델을 훈련한 후 검증세트로 평가합니다. 이런 식으로 매개변수들을 바꿔가며 가장 좋은 모델을 고르면 됩니다.</p>

<p>그럼 데이터를 불러와서 train 세트와 test 세트로 나눈 후에 검증 세트로도 나눠보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/wine_csv_data'</span><span class="p">)</span>
</code></pre></div></div>
<p>와인 데이터를 data에는 alcohol, sugar, pH 를 지정하고, target에는 class 를 넘파이 배열로 지정해줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s">'class'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>
<p>train_test_split를 사용하여 train data와 test data로 나눕니다. test_size를 0.2로 지정하여 20%만 test 데이터로 가져옵니다. test size를 지정하지 않을 경우 train_test_split는 디폴트로 0.25를 테스트 세트로 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>그리고 train data를 sub data와 val data로 한번 더 나누어 줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sub_input</span><span class="p">,</span> <span class="n">val_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">,</span> <span class="n">val_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>훈련 세트와 검증 세트의 크기를 확인해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">sub_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(4157, 3) (1040, 3)</p>
</blockquote>

<p>데이터를 각각 훈련데이터, 검증데이터, 테스트 데이터로 나누어줬습니다. 
다음으로 훈련데이터와 검증데이터를 사용하여 모델을 만들고 평가합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sub_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">sub_input</span><span class="p">,</span> <span class="n">sub_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">val_input</span><span class="p">,</span> <span class="n">val_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9971133028626413<br />
0.864423076923077</p>
</blockquote>

<p>모델이 훈련 세트에 과대적합 되어있으니 매개변수를 바꾸어 더 좋은 모델을 찾아보겠습니다.</p>

<h2 id="교차-검증">교차 검증</h2>
<p>많은 데이터를 훈련세트에 사용할 수록 모델의 정확도가 좋아집니다. 그렇다고 검증세트를 적게 사용하면 검증 점수가 불안정하게 나올 수 있습니다. 이를 해결하기 위해 교차검증을 이용합니다.<br />
검증세트를 떼어놓는 과정을 여러 번 반복하고 이 점수들을 평균내어 검증 점수를 얻어냅니다.
이런 방법을 사용하면 데이터의 8,90% 정도를 훈련에 사용할 수 있습니다.
사이킷런에 cross_validate() 함수를 사용하여 교차검증을 해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘fit_time’: array([0.00482202, 0.00468516, 0.00489712, 0.00469923, 0.0045011 ]), ‘score_time’: array([0.0005331 , 0.00038004, 0.00038218, 0.00042176, 0.00033784]), ‘test_score’: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}</p>
</blockquote>

<p>이 함수는 fit_time, score_time, test_score 키를 갖는 딕셔너리를 반환합니다. 
cross_validate 함수는 기본적으로 5-폴드 교차검증을 수행하기 때문에 각 키마다 5개의 숫자가 담겨있습니다. 
교차 검증의 점수들을 평균내어 최종 점수를 얻습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.855300214703487</p>
</blockquote>

<p>다만 cross_validate 함수는 훈련 세트를 섞어서 폴드를 나누지 않기 때문에 만약 훈련세트를 섞어서 나누고 싶다면 splitter을 지정해줘야합니다. 
cross_validate 함수는 기본적으로 회귀모델일 경우 KFold 분할기를 사용하고 분류모델일 경우 StratifiedKFold를 사용합니다. 
다음 코드와 앞서 수행한 교차 검증은 동일합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">StratifiedKFold</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.855300214703487</p>
</blockquote>

<p>10-교차검증을 해보고 싶다면 다음과 같이 stratifiedKFold에 splits를 10으로 지정하고 splitter에 할당하여 사용하면 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">splitter</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">splitter</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s">'test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8574181117533719</p>
</blockquote>

<h2 id="하이퍼파라미터-튜닝">하이퍼파라미터 튜닝</h2>

<p>결정트리 모델에서는 여러 매개변수를 동시에 바꿔가며 최적의 값을 찾아야하기 때문에 그리드 서치를 사용합니다. 
GridSearchCV 클래스는 하이퍼파라미터 탐색과 교차 검증을 한번에 수행해줍니다. 
한 예시로 기본 매개변수를 사용한 결정트리 모델에서 min_impurity_decrease 매개변수의 최적값을 찾아보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">,</span> <span class="mf">0.0003</span><span class="p">,</span> <span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">]}</span>
</code></pre></div></div>
<p>여기서는 5개의 값을 시도합니다. 
결정트리 클래스의 객체를 생성하고 바로 전달합니다.<br />
gs 객체에 fit 메서드를 호출합니다. 메서드를 호출하면 그리드 서치 객체는 min_impurity_decrease 값을 바꿔가며 5번 실행합니다. 
많은 모델을 훈련하기 때문에 n_jobs 매개변수를 사용하여 미리 사용할 CPU 코어수를 지정해줄 수 있습니다. 
이 매개변수의 기본 값은 1로, -1로 지정하면 시스템에 있는 모든 코어를 사용합니다. 
사이킷런의 그리드 서치는 훈련이 끝나면 best_estimator_속성에 가장 좋은 모델을 저장합니다. 
그리고 그리드 서치로 찾은 최적의 매개변수는 best_params_ 속성에 저장되어있습니다. 
확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
             param_grid={‘min_impurity_decrease’: [0.0001, 0.0002, 0.0003,
                                                   0.0004, 0.0005]})</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">gs</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9615162593804117</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘min_impurity_decrease’: 0.0001}</p>
</blockquote>

<p>0.0001이 가장 좋은 값으로 선택되었습니다. 
각 매개변수에서 수행한 교차 검증의 평균 점수는 mean_test_score에 저장되어있습니다. 
그리고 어떤 값이 큰지 확인하기 위해 넘파이의 argmax() 함수를사용합니다. 
그다음 그 인덱스를 이용하여 params키에 저장된 매개변수를 출력합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'params'</span><span class="p">][</span><span class="n">best_index</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>{‘min_impurity_decrease’: 0.0001}</p>
</blockquote>

<p>이제 더 복잡한 매개변수 조합들에서도 사용해보겠습니다. params에 다음과 같이 여러 매개변수의 범위를 설정하고 그리드 서치를 실행해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">),</span>
          <span class="s">'max_depth'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
          <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
             param_grid={‘max_depth’: range(5, 20),
                         ‘min_impurity_decrease’: array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,
       0.0009]),
                         ‘min_samples_split’: range(2, 100, 10)})</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘max_depth’: 14, ‘min_impurity_decrease’: 0.0004, ‘min_samples_split’: 12}</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8683865773302731</p>
</blockquote>

<h2 id="랜덤서치">랜덤서치</h2>

<p>매개변수의 값의 범위를 미리 정하기 어려울 때, 그리드 서치 수행시간이 너무 오래걸릴 때 랜덤서치를 사용할 수 있습니다. 
랜덤 서치에는 매개변수를 샘플링할 수 있는 확률분포를 전달합니다. 
확률 분포 클래스를 임포트하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">randint</span>
</code></pre></div></div>
<p>임포트한 두 클래스 모두 주어진 범위 내에서 고르게 값을 뽑습니다. 
차이점은 randint 값은 정숫값을, uniform은 실수값을 뽑습니다. 
0과 10 사이의 범위를 갖는 randint 객체를 만들어 샘플링을 해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rgen</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">rgen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>array([3, 6, 1, 2, 3, 1, 7, 6, 5, 1])</p>
</blockquote>

<p>고르게 샘플링이 되는지 확인하기 어려우니, 1000개를 샘플링해서 각 갯수를 세어봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">rgen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([ 79,  89, 110, 112, 106, 104,  95,  99, 104, 102]))</p>
</blockquote>

<p>꽤 고르게 샘플링이 된 것을 확인할 수 있습니다.
마찬가지로 uniform도 사용해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ugen</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ugen</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>array([0.38360187, 0.85618662, 0.41423051, 0.18153867, 0.36998781,
       0.93449236, 0.36294108, 0.2842492 , 0.26135179, 0.12996384])</p>
</blockquote>

<p>난수 발생기와 유사한 형태로 출력되었습니다.</p>

<p>이제 탐색할 매개변수의 딕셔너리를 만들어줍니다. 
탐색할 매개변수의 범위를 다음과 같이 지정합니다. 
min_samples_leaf 매개변수는 리프 노드가 되기 위한 최소 샘플의 개수입니다. 이보다 자식노드의 샘플수가 적을경우 분할하지 않는다는 의미입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">),</span>
          <span class="s">'max_depth'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
          <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
          <span class="s">'min_samples_leaf'</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
          <span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> 
                        <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                   n_iter=100, n_jobs=-1,
                   param_distributions={‘max_depth’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f08156b80&gt;,
                                        ‘min_impurity_decrease’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f510ecfd0&gt;,
                                        ‘min_samples_leaf’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f081568b0&gt;,
                                        ‘min_samples_split’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f202cc2b0&gt;},
                   random_state=42)</p>
</blockquote>

<p>params에서 정의된 범위에서 총 100번을 샘플링하여 교차검증을 수행하여 최적의 값을 찾습니다.
샘플링 횟수는 n_iter 매개변수에 지정해주었습니다.
이제 최적의 매개변수의 값을 출력하고 최고의 교차 검증 점수도 확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>{‘max_depth’: 39, ‘min_impurity_decrease’: 0.00034102546602601173, ‘min_samples_leaf’: 7, ‘min_samples_split’: 13}</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">gs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8695428296438884</p>
</blockquote>

<p>best_estimator_에 저장되어있는 최적의 모델을 사용하여 테스트의 성능을 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">gs</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.86</p>
</blockquote>

<p>오늘은 수동으로 매개변수를 바꾸는 대신 그리드 서치나 랜덤 서치를 이용하여 최적의 매개변수를 찾는 방법을 알아보았습니다. 다음에는 결정 트리를 확장한 앙상블 모델에 대해 알아보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[5-2 교차 검증과 그리드 서치]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 5. 트리 알고리즘</title><link href="http://localhost:4000/ml/HG5-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 5. 트리 알고리즘" /><published>2022-05-27T00:00:00+09:00</published><updated>2022-05-27T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG5-1</id><content type="html" xml:base="http://localhost:4000/ml/HG5-1/"><![CDATA[<p>결정 트리 알고리즘을 사용하여 새로운 분류 문제를 다루어보도록 하겠습니다.</p>

<p>우선적으로 와인 데이터의 여러 변수를 사용하여 와인 종류를 구별할 수 있도록 로지스틱 회귀를 사용합니다.</p>

<h2 id="로지스틱-회귀로-와인-분류하기">로지스틱 회귀로 와인 분류하기</h2>

<p>판다스를 이용해 데이터를 불러옵니다. head() 메서드를 사용해 데이터를 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/wine-date'</span><span class="p">)</span>
<span class="n">wine</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-1.png?raw=true" alt="HG5-1-1" width="400" height="400" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-2.png?raw=true" alt="HG5-1-2" width="400" height="400" /><br />
<br />
와인 데이터 프레임의 각 데이터 타입과 누락된 값이 있는지 확인하기 위해 .info() 메서드를 사용합니다. 결과적으로 총 6497열 중에 각 column 값이 6497인 것을 보아 누락된 데이터는 없다는 것을 확인했습니다.<br />
다음으로 describe() 메서드를 사용하여 각 column에 대한 간단한 통계값을 출력합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-3.png?raw=true" alt="HG5-1-3" width="500" height="500" /><br />
이 값에서 확인할 수 잇는 점은 각 column에 대해 스케일이 다르다는 것입니다. 따라서 앞서 진행했던 것처럼 표준화를 해주어야 합니다. 표준화에 앞서 배열을 넘파이 형식으로 바꾼 후에 훈련 데이터와 테스트 데이터로 나누는 작업을 진행합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s">'class'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(5197, 3) (1300, 3)</p>
</blockquote>

<p>훈련 세트와 테스트 세트의 갯수를 확인해봤습니다. 이제 StandardScaler을 사용해서 표준화합니다. 그리고 그 데이터를 사용하여 로지스틱 회귀 모델을 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>

<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.7808350971714451<br />
0.7776923076923077</p>
</blockquote>

<p>모델의 점수가 높지 않은 것을 확인할 수 있습니다. 로지스틱 회귀가 학습한 계수와 절편을 출력해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]</p>
</blockquote>

<p>모델을 설명하기 위해 로지스틱 회귀가 학습한 계수들을 출력해보았습니다. 그러나 한눈에 알아보기 어렵다는 단점이 있고, 관계를 쉽게 설명할 수 없기 때문에 비교적 설명이 쉬운 결정트리 모델을 학습해보겠습니다.</p>

<h2 id="결정트리">결정트리</h2>

<p>결정트리 모델을 사이킷런의 DecisionTreeClassifier 클래스에서 제공합니다. 모델을 훈련한 후 점수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.996921300750433
0.8592307692307692</p>
</blockquote>

<p>정확도가 아까보다 높게 출력되었습니다. 다만 점수 차이를 보니 과대적합이 된 것 같습니다. 모델을 그림으로 표현해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-4.png?raw=true" alt="HG5-1-4" /><br />
트리 그림이 너무 복잡하기 때문에 깊이를 제한해서 다시 그려보겠습니다. 트리의 깊이는 max_depth() 매개변수를 사용하면 됩니다. filled는 노드의 색을 칠할 수 있고 feature_names는 특성의 이름을 전달할 수 있습니다. 다음과 같은 매개변수들을 사용해서 다시 그림을 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-5.png?raw=true" alt="HG5-1-5" /></p>

<h3 id="가지치기">가지치기</h3>

<p>결정트리에서는 노드 수가 너무 많아지면 훈련 데이터에 과대적합될 수 있기 때문에 노드의 수를 조절하는 가지치기가 필요합니다. 가지치기를 할 수 있는 가장 간단한 방법은 max_depth를 조절하는 것입니다. 루트 노드 아래로 3개의 노드만 만들어 모델을 훈련해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8454877814123533<br />
0.8415384615384616</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-6.png?raw=true" alt="HG5-1-6" /></p>

<p>훈련 세트의 점수는 낮아지고 테스트 세트 점수는 거의 비슷하기 때문에 그림으로 그려보았습니다. 
깊이 3에 있는 마지막 노드들이 최종 노드인 리프 노드입니다. 
세 번째에 위치한 노드만 음성 클래스가 더 많기 때문에 이 노드에 도착한 것들만 레드 와인으로 예측합니다. 
루트 노드부터 이 노드까지 도달하기 위해서는 당도가 -0.239보다 작고 -0.802보다도 작아야 합니다. 알코올 도수는 0.454보다도 작아야 이 노드에 도달할 수 있습니다.<br />
<br />
결정트리는 불순도에 따라 샘플을 나누는데, 불순도는 클래스별 비율을 가지고 계산하기 때문에 표준화 작업이 필요가 없습니다. 
그렇다면 원래 데이터를 가지고 결정트리 모델을 다시 훈련해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8454877814123533<br />
0.8415384615384616</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">,</span> <span class="s">'sugar'</span><span class="p">,</span> <span class="s">'pH'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG5/HG5-1-7.png?raw=true" alt="HG5-1-7" /><br />
점수는 동일하지만, 특성값이 음수로 나오지 않기 때문에 설명하기 쉬워졌습니다. 
마지막으로 결정 트리에서 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산합니다. 
feature_importances_ 속성으로 알아볼 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">dt</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[0.12345626 0.86862934 0.0079144 ]</p>
</blockquote>

<p>두번째 값인 당도가 0.87 정도로 특성 중요도가 가장 높은 것을 볼 수 있습니다. 
특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더해서 계산합니다. 
특성 중요도를 활용하면 결정트리 모델을 특성 선택에 활용할 수 있습니다. 
성능이 매우 좋지는 않지만 설명하기는 좋은 모델이라고 생각할 수 있습니다.</p>

<p>다음 절에서는 결정 트리의 다양한 매개변수들, 하이퍼파라미터를 자동으로 찾을 수 있는 방법에 대해 알아보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[5-1 결정 트리]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 4. 다양한 분류 알고리즘(2)</title><link href="http://localhost:4000/ml/HG4-2/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 4. 다양한 분류 알고리즘(2)" /><published>2022-05-27T00:00:00+09:00</published><updated>2022-05-27T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG4-2</id><content type="html" xml:base="http://localhost:4000/ml/HG4-2/"><![CDATA[<h2 id="확률적-경사-하강법">확률적 경사 하강법</h2>
<p>확률적 경사 하강법이란 Stochastic Gradient Descent로써, 무작위로 배치 크기가 1인 데이터를 추출하여 기울기를 계산하고 경사 하강 알고리즘을 적용하는 방법을 말합니다. 랜덤하게 하나의 샘플을 택하여 하강 알고리즘을 적용하기 때문에 전체 샘플을 사용하지 않습니다. 이러한 형태로 전체 샘플을 모두 사용할 때 까지 알고리즘을 여러번 적용한다고 생각하면 됩니다.<br />
<br /></p>

<p>우선 pandas를 사용하여 데이터를 불러오겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#dataframe 생성
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">fish</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/fish_csv'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">fish</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-2-1.png?raw=true" alt="HG4-2-1" /><br />
<br />
우선 데이터를 불러온 후에 Species 열을 제외한 열들을 입력 데이터로, Species 열을 타깃 데이터롤 설정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_input</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[[</span><span class="s">'Weight'</span><span class="p">,</span> <span class="s">'Length'</span><span class="p">,</span> <span class="s">'Diagonal'</span><span class="p">,</span> <span class="s">'Height'</span><span class="p">,</span> <span class="s">'Width'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">fish_target</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1">#train set와 test set으로 나누기
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">fish_input</span><span class="p">,</span> <span class="n">fish_target</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>
<p>fish 데이터를 sklearn을 이용하여 train set와 test set로 나눠줍니다. 
그리고 StandardScaler을 사용하여 train set와 test set를 표준화 전처리를 해주도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>
<p>이제 확률적 경사 하강법을 사용하기 위해 sklearn의 SGDClassifier 클래스를 이용합니다. SGDClassifier 클래스에는 loss 변수에 손실 함수의 종류를 지정해주고, max_iter 변수에는 수행할 에포크의 횟수를 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.773109243697479<br />
0.775</p>
</blockquote>

<p>점수가 낮게 나왔기 때문에 점수를 높이기 위해 이어서 훈련하여 점진적인 학습을 해보도록 합니다. 이어서 훈련할 때에는 partial_fit() 메서드를 사용하는데, 사용할 때 마다 1 epoch 씩 이어 훈련할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8151260504201681<br />
0.85</p>
</blockquote>

<p>확률적 경사 하강법을 사용한 모델은 에포크의 횟수에 따라 과대적합이나 과소적합 될 수 있기 때문에 에포크 횟수에 따른 훈련세트 점수와 테스트세트 점수를 그래프로 나타내 적절한 에포크의 값을 찾아줘야 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">train_target</span><span class="p">)</span>

<span class="c1">#300번의 epoch동안 훈련 반복 진행
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">):</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
    <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
    <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>

<span class="c1">#그래프 그리기
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-2-2.png?raw=true" alt="HG4-2-2" /></p>

<p>결과인 그래프를 보면 초반에는 과소적합되어 훈련세트와 테스트세트 점수가 낮고, 약 100번째 에포크 이후에는 훈련세트와 테스트 세트의 점수가 벌어지고 있는 것을 볼 수 있습니다. 100번째 에포크가 적절한 값인 것을 확인할 수 있습니다. 반복횟수를 100으로 설정하고 모델을 다시 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span> 
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.957983193277311<br />
0.925</p>
</blockquote>

<p>최종 점수가 잘 나오는 것을 확인할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[4-2 확률적 경사 하강법]]></summary></entry><entry><title type="html">[Github] 깃허브 메인 페이지에 잔디심기 띄우기</title><link href="http://localhost:4000/github/GitAction/" rel="alternate" type="text/html" title="[Github] 깃허브 메인 페이지에 잔디심기 띄우기" /><published>2022-05-14T00:00:00+09:00</published><updated>2022-05-14T00:00:00+09:00</updated><id>http://localhost:4000/github/GitAction</id><content type="html" xml:base="http://localhost:4000/github/GitAction/"><![CDATA[<p>오늘은 Github Actions를 사용하여 잔디심기 대시보드를 다음과 같이 만들어보겠습니다.<br />
참고한 포스트는 게시글 하단에 작성해 두었습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A1.png?raw=true" alt="A1" /></p>

<h2 id="1-잔디심기-대시보드-만들-레포지토리-선택해서-권한-설정">1. 잔디심기 대시보드 만들 레포지토리 선택해서 권한 설정</h2>
<p>잔디심기 대시보드를 표시할 레포지토리를 정합니다. 저는 깃허브 프로필 페이지에 띄울것이기 때문에 고유 아이디로 된 yhp2205 레포지토리로 결정했습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A2.png?raw=true" alt="A2" /><br />
<br />
레포지토리를 정했으면 그 레포지토리의 권한을 설정하기 위해 settings를 눌러 들어갑니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A3.png?raw=true" alt="A3" /><br />
<br />
settings의 좌측 카테고리에서 <code class="language-plaintext highlighter-rouge">Actions - General</code>로 들어가서</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A4.png?raw=true" alt="A4" /><br />
<br />
페이지 하단에 위치한 Workflow permissions를 다음과 같이 설정해줍니다.</p>

<h2 id="2-metrics-기능-fork-해서-가져오기">2. metrics 기능 fork 해서 가져오기</h2>
<p><a href="https://github.com/lowlighter/metrics">다음 사이트</a>에 접속하여 metrics 기능을 fork 해서 가져오겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A5.png?raw=true" alt="A5" /><br />
<br />
이런 페이지가 뜨면 우측 상단의 fork 버튼을 누르고</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A6.png?raw=true" alt="A6" /><br />
<br />
Create fork를 눌러 본인의 깃허브 계정으로 가져옵니다.</p>

<h2 id="3-personal-token-생성하기">3. personal token 생성하기</h2>
<p>metrics를 가져왔으니 계정의 token을 생성해보겠습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A7.png?raw=true" alt="A7" /><br />
<br />
다음과 같이 계정의 settings로 들어갑니다. (레포지토리의 settings가 아닌 깃허브 계정의 settings로 들어갑니다)</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A8.png?raw=true" alt="A8" /><br />
<br />
우측 카테고리 제일 하단의 Developer settings를 누릅니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A9.png?raw=true" alt="A9" /><br />
<br />
다음과 같이 <code class="language-plaintext highlighter-rouge">personal access token - Generate new token</code>을 눌러줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A10.png?raw=true" alt="A10" /><br />
<br /></p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A11.png?raw=true" alt="A11" /><br />
<br />
note에 token의 이름을 지정해주고, 유효기간을 설정합니다. 원하는 대로 설정하면 됩니다. 그러나 토큰의 유효기간을 짧게 설정할 경우, 그 주기마다 토큰을 다시 만들어 업데이트 해줘야하기 때문에 기한 없이 설정하는 것을 권장합니다. 그리고 아래 repo, read::packages, read:org, gist, read:user를 선택한 후에 토큰을 만들어 줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A12.png?raw=true" alt="A12" /><br />
<br />
토큰을 성공적으로 생성했을 때 다음과 같은 페이지가 뜨고 빨간 박스로 표시한 아이콘을 눌러 토큰을 복사합니다. 잃어버리면 안되기 때문에 다른 메모장에 적어두셔도 좋습니다!</p>

<h2 id="4-actions-secrets에-token-등록하기">4. Actions secrets에 token 등록하기</h2>
<p>토큰을 만들어 잘 복사해두었다면 다음으로 잔디심기를 올릴 레포지토리에 토큰을 등록해보겠습니다.<br />
<br /></p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A13.png?raw=true" alt="A13" /><br />
<br />
잔디심기 대시보드를 만들 레포지토리에 들어갑니다. 1에서 권한을 설정해 주었던 레포지토리의 <code class="language-plaintext highlighter-rouge">settings - secrets - actions</code>를 차례로 눌러 들어가줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A14.png?raw=true" alt="A14" /><br />
<br />
<code class="language-plaintext highlighter-rouge">New repository secret</code>을 눌러 토큰을 등록하겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A15.png?raw=true" alt="A15" /><br />
<br />
다음과 같은 창이 뜨면 토큰의 이름을 작성하고(저는 YOUNGHYUNS_METRICS_TOKEN으로 등록하였습니다), Value 값에 복사해둔 token값을 넣고 Add secret 해줍니다.</p>

<h2 id="5-repo-내에-yml-파일-만들기">5. repo 내에 yml 파일 만들기</h2>

<p>깃허브 토큰 등록한 repo 내에 yml 파일을 만들겠습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A16.png?raw=true" alt="A16" /><br />
<br />
<code class="language-plaintext highlighter-rouge">Actions - set up a workflow yourself</code>를 누릅니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a basic workflow to help you get started with Actions

name: CI

# Controls when the workflow will run
on:
  # 하루에 한 번 씩 빌드 수행
  schedule:
    - cron: '0 1 * * *'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: [빌드를 수행할 때 표시할 작업명]
        uses: [metrics를 fork한 저장소명]/metrics@latest
        with:
          token: $
          filename: [빌드한 후 생성할 파일명].svg
          base: ""
          plugin_isocalendar: yes
          plugin_isocalendar_duration: full-year
</code></pre></div></div>
<p>다음 코드를 복사해서 붙여넣습니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A17.png?raw=true" alt="A17" /><br />
<br />
다음과 같이 <code class="language-plaintext highlighter-rouge">activity_metrics_build.yml</code>로 이름을 지정해주고 앞선 코드에서 표시된 부분을 본인의 repo에 맞게 작성합니다. 하늘색 박스로 표시된 부분에는 전에 등록해준 토큰의 이름을 작성하면 됩니다. 저는 <code class="language-plaintext highlighter-rouge">YOUNGHYUNS_METRICS_TOKEN</code> 으로 지정했기 때문에 그대로 작성하였습니다.<br />
<br />
작성이 완료되었다면 우측 상단에 위치한 Start commit 버튼을 눌러 Commit new file을 해줍니다.</p>

<h2 id="6-workflow-실행하여-svg-파일-생성하기">6. Workflow 실행하여 svg 파일 생성하기</h2>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A18.png?raw=true" alt="A18" /><br />
<br />
commit을 완료하여 yml 파일을 생성했다면 다시 Actions 탭으로 들어갑니다. 그럼 다음과 같이 CI라는 이름의 workflow가 생성된 것을 확인할 수 있습니다. CI를 선택한 후에, 빨간색으로 표시되어 있는 Run Workflow를 눌러 workflow를 실행합니다.<br />
<br />
CI가 실행되는 데에는 약 5분정도의 시간이 걸립니다. 주황색 불이 들어와서 실행중임을 알리고 있다면 빌드가 완료될 때까지 기다립니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A19.png?raw=true" alt="A19" />
<br />
시간이 지난 후, 초록 표시가 들어오고 빌드가 완료된 것을 알리면, 레포지토리 내에 다음과 같이 svg 파일이 생성된 것을 볼 수 있습니다.</p>

<h2 id="7-잔디심기-대시보드-게시하기">7. 잔디심기 대시보드 게시하기</h2>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A20.png?raw=true" alt="A20" /><br />
<br />
마지막으로 동일 레포지토리의 Readme.md 파일에 다음과 같이 작성해줍니다.</p>

<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/Actions/A21.png?raw=true" alt="A21" /><br />
<br />
정상적으로 실행되었을 때, 다음과 같이 깃허브 메인 화면에서 잔디심기 보드를 확인할 수 있습니다.</p>

<blockquote>
  <p>참고한 포스트 <a href="https://ynkim0.github.io/posts/Github-Actions%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%94%EB%94%94%EC%8B%AC%EA%B8%B0-%EB%8C%80%EC%89%AC%EB%B3%B4%EB%93%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0/">Github Actions를 이용한 잔디심기 대쉬보드 만들기</a></p>
</blockquote>]]></content><author><name>Younghyun Park</name></author><category term="Github" /><category term="Github" /><category term="Github actions" /><summary type="html"><![CDATA[Github Actions를 사용한 잔디심기 대시보드 만들기]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 4. 다양한 분류 알고리즘</title><link href="http://localhost:4000/ml/HG4-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 4. 다양한 분류 알고리즘" /><published>2022-05-04T00:00:00+09:00</published><updated>2022-05-04T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG4-1</id><content type="html" xml:base="http://localhost:4000/ml/HG4-1/"><![CDATA[<h2 id="로지스틱-회귀">로지스틱 회귀</h2>

<p>랜덤하게 담긴 생선의 확률을 알아보려고 합니다.<br />
우선 앞서 배웠던 k-최근접 이웃 분류기를 사용하여 구한 이웃 클래스를 토대로 타깃 생선의 확률을 계산해보겠습니다.<br />
먼저 데이터를 준비합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">fish</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/fish_csv'</span><span class="p">)</span>
<span class="n">fish</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-1.png?raw=true" alt="HG4-1-1" /><br />
<br />
그리고 target 데이터가 될 Species 열에 어떤 종류가 있는지 판다스의 unique 함수를 사용해서 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Roach’ ‘Whitefish’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Smelt’]</p>
</blockquote>

<p>Species 에 들어있는 종들을 확인했으니 fish 데이터에서 Species 열을 제외하고 나머지 5개의 열을 입력 데이터로 선택합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_input</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[[</span><span class="s">'Weight'</span><span class="p">,</span> <span class="s">'Length'</span><span class="p">,</span> <span class="s">'Diagonal'</span><span class="p">,</span> <span class="s">'Height'</span><span class="p">,</span> <span class="s">'Width'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">fish_input</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[[242.      25.4     30.      11.52     4.02  ]<br />
 [290.      26.3     31.2     12.48     4.3056]<br />
 [340.      26.5     31.1     12.3778   4.6961]<br />
 [363.      29.      33.5     12.73     4.4555]<br />
 [430.      29.      34.      12.444    5.134 ]]</p>
</blockquote>

<p>입력 데이터가 잘 생성된 것을 확인했습니다.<br />
 동일한 방식으로 target 데이터도 만듭니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fish_target</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div>
<p>이제 train set와 test set로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">fish_input</span><span class="p">,</span> <span class="n">fish_target</span><span class="p">)</span>
</code></pre></div></div>
<p>train 세트와 test를 준비했으니 데이터를 표준화 전처리를 해줍니다.<br />
사이킷런의 StandardScaler을 이용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#훈련 테스트 세트 표준화 전처리
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>
<p>사이킷런의 KNeighborsClassifier 클래스 객체를 만들고 훈련세트와 테스트세트의 각 점수를 확인해보겠습니다.<br />
최근접 이웃의 개수인 k는 3으로 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#최근접 이웃 분류기의 확률 예측
</span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">kn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">kn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.8739495798319328<br />
0.75</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]</p>
</blockquote>

<p>알파벳 순서로 매겨진 것을 확인했습니다.<br />
이제 predict 메서드를 사용하여 test set에 있는 처음 5개의 샘플의 타깃값을 예측합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">kn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]</p>
</blockquote>

<p>predict 예측 값이 어떻게 나왔는지 predict_proba() 메서드를 사용하면 클래스별 확률 값을 확인할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">kn</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>  <span class="c1">#소숫점 넷째자리까지 표기(다섯번째에서 반올림)
</span></code></pre></div></div>
<blockquote>
  <p>[[0.     0.6667 0.3333 0.     0.     0.     0.    ]<br />
 [0.     0.     1.     0.     0.     0.     0.    ]<br />
 [0.     0.     0.3333 0.     0.6667 0.     0.    ]<br />
 [0.     0.     1.     0.     0.     0.     0.    ]<br />
 [0.     0.     0.3333 0.     0.3333 0.3333 0.    ]]</p>
</blockquote>

<p>첫번째 열이 Bream, 두번째 열이 Parkki, 세번째 열이 Perch, … 에 대한 확률을 나타내고 있습니다.<br />
 계산한 비율이 올바른지 확인하기 위해 첫 번째 샘플의 최근접 이웃들을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distances</span><span class="p">,</span> <span class="n">indexes</span> <span class="o">=</span> <span class="n">kn</span><span class="p">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_target</span><span class="p">[</span><span class="n">indexes</span><span class="p">])</span>
</code></pre></div></div>
<blockquote>
  <p>[[‘Parkki’ ‘Parkki’ ‘Perch’]]</p>
</blockquote>

<p>첫 번째 샘플의 이웃은 Perch가 1개, Parkki가 2개임을 확인합니다.<br />
여기까지 k-최근접 이웃 분류기를 사용한 확률 예측을 해보았습니다.<br />
<br />
이제 본격적으로 로지스틱 회귀를 이용한 예측을 해보도록 하겠습니다.</p>

<p>로지스틱 회귀는 선형회귀와 동일하게 선형 방정식을 학습합니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-2.png?raw=true" alt="HG4-1-2" /><br />
<br />
여기서 각 변수 앞에 곱해진 값들은 가중치나 계수의 역할을 합니다.<br />
z는 어떤 값도 가능하지만, 확률이 되기 위해서는 0과 1 사이의 값을 가져야 합니다.<br />
z가 큰 음수일때 0이 되고 큰 양수일 때 1이 되도록 하려면 시그모이드 함수를 사용하면 됩니다.</p>

<p><img src="https://mblogthumb-phinf.pstatic.net/MjAxNzA5MjZfMzMg/MDAxNTA2NDIzMjMzNjE5.arSKSXckfkmGqMIEA0dAI1q_e080ntMcpyhXhBx2m4Ig.nofSEy_ifZ6BM7VvFOku71bQbM8d2ngYWwV_sO8oyNEg.PNG.junhyuk7272/%EC%9E%90%EC%97%B0%EC%A7%80%EC%88%98%ED%95%A8%EC%88%98.png?type=w2" alt="HG4-1-32" />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-3.png?raw=true" alt="HG4-1-3" /><br />
<br /></p>

<p>시그모이드 함수는 다음과 같은 형태를 띕니다.<br />
넘파이를 사용해서 그래프를 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">phi</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'z'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'phi'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-4.png?raw=true" alt="HG4-1-4" /><br />
<br /></p>

<p>이제 로지스틱 회귀를 사용하여 이진분류를 해보겠습니다.<br />
train 데이터에서 불리언 인덱싱을 사용하여 도미와 빙어만 따로 출력해줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#도미(Bream)와 빙어(Smelt)만 출력하기
</span><span class="n">bream_smelt_indexes</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_target</span> <span class="o">==</span> <span class="s">'Bream'</span><span class="p">)</span><span class="o">|</span><span class="p">(</span><span class="n">train_target</span> <span class="o">==</span> <span class="s">'Smelt'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">bream_smelt_indexes</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[False  True False False False False  True False False  True False  True
  True False False False  True  True False  True False False  True False
 False False  True False False False False False False False  True False
  True  True False  True False False  True False False  True False  True
 False False  True  True False False False  True  True False False False
  True  True False False False False False False False  True False  True
 False False  True False False False False  True False False False False
  True False  True False False False False False False False  True  True
 False False False False  True  True False False False False False False
 False False False False  True  True  True False False  True False]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_bream_smelt</span> <span class="o">=</span> <span class="n">train_scaled</span><span class="p">[</span><span class="n">bream_smelt_indexes</span><span class="p">]</span>
<span class="n">target_bream_smelt</span> <span class="o">=</span> <span class="n">train_target</span><span class="p">[</span><span class="n">bream_smelt_indexes</span><span class="p">]</span>
</code></pre></div></div>
<p>이 데이터를 이용하여 로지스틱 회귀 모델을 훈련하고 그 모델을 사용하여 train_bream_smelt에 있는 처음 5개의 샘플을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">,</span> <span class="n">target_bream_smelt</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Bream’]</p>
</blockquote>

<p>다섯 번째를 도미, 나머지를 빙어로 예측했습니다.<br />
5개 샘플의 확률을 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[[0.03391154 0.96608846]<br />
 [0.03603855 0.96396145]<br />
 [0.03582089 0.96417911]<br />
 [0.02900509 0.97099491]<br />
 [0.99410165 0.00589835]]</p>
</blockquote>

<p>두 개의 열 중에 어떤 것이 도미이고 빙어인지 헷갈린다면 classes_ 를 사용하여 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Smelt’]</p>
</blockquote>

<p>이제 로지스틱 회귀가 학습한 계수를 확인합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[-0.42646881 -0.60256452 -0.68252074 -0.99456193 -0.78263044]] [-2.28791769]</p>
</blockquote>

<p>이를 통해 로지스틱 회귀 모델이 학습한 방정식을 다음과 같이 도출할 수 있습니다. 
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-5.png?raw=true" alt="HG4-1-5" /><br />
<br />
이제 이 방정식을 이용하여 z 값을 계산해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decisions</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">train_bream_smelt</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">decisions</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[ 3.34950001  3.28646197  3.29274597  3.51084984 -5.12716733]</p>
</blockquote>

<p>이 z값을 시그모이드 함수에 통과하면 확률을 얻을 수 있습니다.<br />
scipy에 시그모이드 함수를 제공하는 expit()를 사용해서 확률을 구합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="k">print</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="n">decisions</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>[0.96608846 0.96396145 0.96417911 0.97099491 0.00589835]</p>
</blockquote>

<p>이제 다중 분류를 수행해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9411764705882353<br />
0.85</p>
</blockquote>

<p>test 세트의 처음 5개 샘플에 대한 예측을 출력해봅니다.<br />
그리고 그 예측에 대한 확률을 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set의 처음 5개 샘플에 대한 예측
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set의 처음 5개에 대한 예측 확률
</span><span class="n">proba</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span> <span class="c1">#소수점 세번째자리까지 표기
</span></code></pre></div></div>
<blockquote>
  <p>[[0.01  0.73  0.006 0.    0.223 0.    0.03 ]<br />
 [0.01  0.    0.898 0.002 0.    0.    0.089]<br />
 [0.001 0.018 0.28  0.002 0.661 0.    0.038]<br />
 [0.002 0.    0.953 0.001 0.    0.    0.044]<br />
 [0.    0.012 0.816 0.    0.119 0.051 0.002]]</p>
</blockquote>

<p>5개 샘플과 7개의 클래스가 있기 때문에 5개의 행과 7개의 열로 출력이 되었습니다.<br />
각 열이 의미하는 것이 어떤 생선에 대한 확률인지 확인합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#classes_속성으로 클래스 정보 확인
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(7, 5) (7,)</p>
</blockquote>

<p>coef_ 배열이 7행에 5열의 형태를 띄고 있고 intercept_도 7개가 있는 것을 확인할 수 있습니다.<br />
7개의 클래스가 있기 때문에 클래스마다 z값을 계산하여 총 7개의 z값이 나온다는 말과 같습니다.</p>

<p>이와 같이 7개의 z값들을 확률로 바꾸기 위해 소프트맥스 함수를 사용합니다.<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG4/HG4-1-6.png?raw=true" alt="HG4-1-6" /><br />
<br />
7개의 z값을 소프트맥스 함수를 이용하여 확률로 바꾸면 0과 1사이의 값을 가진 확률이 출력됩니다.<br />
그럼 7개의 z값을 구한 후에 소프트맥스 함수를 사용하여 확률로 바꾸어보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#z1부터 z7까지의 값 구하고 확률로 바꾸기
</span><span class="n">decision</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#소수점 둘째자리까지 표기
</span></code></pre></div></div>
<blockquote>
  <p>[[ -0.17   4.13  -0.67  -3.44   2.95  -3.75   0.96]<br />
 [  4.31  -5.68   8.79   2.78   0.69 -17.37   6.48]<br />
 [ -2.69   0.28   3.04  -2.01   3.9   -3.56   1.05]<br />
 [  3.78  -6.28   9.85   2.82   0.69 -17.63   6.78]<br />
 [ -8.63   1.35   5.59  -4.1    3.66   2.82  -0.7 ]]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">decimals</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))</span> <span class="c1">#소수점 셋째자리까지 표시
</span></code></pre></div></div>
<blockquote>
  <p>[[0.01  0.73  0.006 0.    0.223 0.    0.03 ]<br />
 [0.01  0.    0.898 0.002 0.    0.    0.089]<br />
 [0.001 0.018 0.28  0.002 0.661 0.    0.038]<br />
 [0.002 0.    0.953 0.001 0.    0.    0.044]<br />
 [0.    0.012 0.816 0.    0.119 0.051 0.002]]</p>
</blockquote>

<p>이렇게 로지스틱 회귀를 사용하여 7개의 클래스에 대한 확률을 예측하는 모델을 만들었습니다.<br />
다음에는 확률적 경사 하강법에 대해 배워보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[4-1 로지스틱 회귀]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(3)</title><link href="http://localhost:4000/ml/HG3-3/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(3)" /><published>2022-05-03T00:00:00+09:00</published><updated>2022-05-03T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG3-3</id><content type="html" xml:base="http://localhost:4000/ml/HG3-3/"><![CDATA[<p>앞서 훈련했던 모델에서 훈련 세트보다 테스트 세트가 더 점수가 높게 나왔습니다.<br />
이 문제를 해결하기 위해 농어의 길이 뿐만 아니라 농어의 높이와 두께 등의 여러 특성들을 추가로 사용해보겠습니다.<br />
또한 이전에 사용했던 방법인 각 항을 제곱하여 데이터에 추가하는 것과 각 특성을 서로 곱해 새로운 특성을 만드는 방식을 사용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># data load
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/perch_csv'</span><span class="p">)</span>
<span class="n">perch_full</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">perch_full</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[[ 8.4   2.11  1.41]<br />
 [13.7   3.53  2.  ]<br />
 [15.    3.82  2.43]<br />
 [16.2   4.59  2.63]<br />
 [17.4   4.59  2.94]<br />
 [18.    5.22  3.32]<br />
…</p>
</blockquote>

<p>이번에는 데이터가 여러 특성이 있음을 고려하여 pandas를 이용해 직접 csv 데이터를 불러오는 방식을 사용했습니다.<br />
이제 이전과 동일한 방식으로 타깃 데이터를 준비하고, perch_full과 perch_weight를 훈련 세트와 테스트 세트로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span>
       <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span>
       <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span>
       <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span>
       <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1000.0</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">perch_full</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>사이킷런의 sklearn.preprocessing 패키지의 PolynomialFeatures 클래스를 사용해서 새로운 특성을 만들어 보겠습니다.<br />
PolynomialFeatures 클래스는 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가합니다.<br />
이때 기본적으로 1을 추가하여 각 특성에 곱함으로써 절편을 만드는데, 사이킷런 선형 모델은 자동으로 절편이 들어가있기 때문에 include_bias=False 인자를 추가함으로서 절편을 위한 항을 제거할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="c1"># train_input을 변환하여 train_poly 만들기
</span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">poly</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 9)</p>
</blockquote>

<p>PolynomialFeatures 클래스는 9개의 특성이 어떻게 반영되어있는지 확인하는 방법을 제공합니다.<br />
get_feature_names() 메서드를 사용하여 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poly</span><span class="p">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</code></pre></div></div>
<blockquote>
  <p>[‘x0’, ‘x1’, ‘x2’, ‘x0^2’, ‘x0 x1’, ‘x0 x2’, ‘x1^2’, ‘x1 x2’, ‘x2^2’]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set 변환하기
</span><span class="n">test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

<span class="c1">#다중회귀 모델 훈련하기
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9903183436982124</p>
</blockquote>

<p>train set을 이용하여 점수를 내보았으니 test set도 점수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#test set 점수도 확인하기
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_poly</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9714559911594134</p>
</blockquote>

<p>테스트 세트 점수와 훈련세트 점수 모두 높게 나오면서 train 세트가 더 점수가 높게 나온 것을 확인할 수 있습니다.<br />
이제 특성을 더 많이 추가해보겠습니다.<br />
PolynomialFeatures 클래스의 degree 변수를 사용하면 고차항의 최대 차수를 지정할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">poly</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 55)</p>
</blockquote>

<p>만들어진 특성의 개수가 55개인 것을 확인할 수 있습니다.<br />
train_poly 배열의 열의 개수가 특성의 개수라고 생각하면 됩니다.<br />
이제 transform을 마쳤으니 이 데이터로 다시 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_poly</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9999999999991097<br />
-144.40579242684848</p>
</blockquote>

<p>결과를 보면 train set에 대한 점수는 매우 높게 나오고, test set에 대한 점수는 음수가 나왔습니다.<br />
이는 특성의 개수를 크게 늘렸기 때문에 train set에 대해 과대적합된 결과로 볼 수 있습니다.<br />
이러한 과대적합을 줄이기 위해서는 선형 회귀 모델의 계수를 규제하여 해결해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_poly</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_poly</span><span class="p">)</span>
</code></pre></div></div>

<p>선형 회귀 모델에 규제를 추가한 모델을 ridge와 lasso라고 부르는데, 릿지는 계수를 제곱한 값을 기준으로 규제를 적용하고, 라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.<br />
일반적으로는 릿지를 조금 더 선호하기 때문에 릿지 회귀를 먼저 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9896101671037343<br />
0.9790693977615397</p>
</blockquote>

<p>ridge 모델을 사용하여 훈련한 결과 많은 특성을 사용했음에도 불구하고 훈련세트, 테스트 세트 모두에서 좋은 점수가 나왔습니다.<br />
릿지와 라쏘 모델을 적용할 때 모델 객체 중 alpha 값을 사용함으로써 규제의 정도를 정할 수 있는데,
alpha값이 크면 규제의 강도가 세기 때문에 과소적합될 가능성이 크고,
작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지기 때문에 과대적합될 가능성이 큽니다.<br />
적절한 alpha 값을 찾기 위해서는 alpha 값에 대한 결정계수 값을 그려 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점을 최적의 alpha 값으로 생각하면 됩니다.<br />
그럼 alpha값을 바꿀 때 마다 score 값을 저장할 리스트를 먼저 만들어주겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#alpha 값 리스트 생성
</span><span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span> <span class="p">:</span>
  <span class="c1">#ridge model 생성
</span>  <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
  <span class="c1">#ridge model 훈련
</span>  <span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="c1">#훈련 점수와 테스트 점수 저장
</span>  <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
  <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<p>이제 alpha 값에 따른 그래프를 그려 값을 확인해보도록 하겠습니다.<br />
그냥 그리면 그래프 왼쪽이 너무 촘촘해지기 때문에 로그함수로 바꿔 표현합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'R^2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-3-1.png?raw=true" alt="HG3-3-1" /><br />
<br />
다음 그래프를 확인해보면, 적절한 alpha 값이 -1, 즉 0.1 이라는 것을 알 수 있습니다.<br />
alpha 값을 0.1로 지정하여 모델을 훈련하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">ridge</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">ridge</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9903815817570366<br />
0.9827976465386926</p>
</blockquote>

<p>train값이 test값보다 높게 나오면서 두 점수 모두 적절한 점수가 나온것을 확인할 수 있습니다.<br />
이제 lasso 회귀모델을 훈련해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span><span class="p">:</span>
  <span class="c1"># lasso model을 만들기
</span>  <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
  <span class="c1"># lasso model 훈련하기
</span>  <span class="n">lasso</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="c1"># train set와 test set score을 저장
</span>  <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
  <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>

<span class="c1">#x축을 로그스케일로 바꿔 그래프 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">),</span> <span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'R^2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-3-2.png?raw=true" alt="HG3-3-2" /><br />
<br />
위 그래프를 확인했을 때 올바른 alpha값은 1이라는것을 알 수 있습니다.<br />
이 값으로 lasso 모델을 훈련합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">lasso</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9888067471131867<br />
0.9824470598706695</p>
</blockquote>

<p>lasso 모델은 계수값을 0으로 만들 수 있습니다.<br />
라쏘모델의 계수중 0인것의 개수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lasso</span><span class="p">.</span><span class="n">coef_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>40</p>
</blockquote>

<p>계수가 0인 항이 40개인 것을 확인할 수 있습니다.<br />
총 55개의 특성 중 15개만 사용한 것입니다.<br />
이런 특성이 있기 때문에 라쏘 모델을 유용한 특성을 골라내는 데에 사용하기도 합니다.<br />
릿지와 라쏘 회귀를 사용하여 최적의 alpha 값을 찾아보고 특성이 많은 데이터를 규제하여 모델의 성능을 확인해봤습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[3-3 특성 공학과 규제]]></summary></entry><entry><title type="html">[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(2)</title><link href="http://localhost:4000/ml/HG3-2/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(2)" /><published>2022-05-02T00:00:00+09:00</published><updated>2022-05-02T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG3-2</id><content type="html" xml:base="http://localhost:4000/ml/HG3-2/"><![CDATA[<p>앞서 만든 k-최근접 이웃 회귀 모델로 길이 50cm 농어의 무게를 예측해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Data load
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">perch_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.4</span><span class="p">,</span> <span class="mf">13.7</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">16.2</span><span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">18.0</span><span class="p">,</span> <span class="mf">18.7</span><span class="p">,</span> <span class="mf">19.0</span><span class="p">,</span> <span class="mf">19.6</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span>
       <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.3</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.7</span><span class="p">,</span>
       <span class="mf">23.0</span><span class="p">,</span> <span class="mf">23.5</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.6</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">,</span> <span class="mf">25.6</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">27.3</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span>
       <span class="mf">27.5</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">28.7</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">32.8</span><span class="p">,</span> <span class="mf">34.5</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">,</span> <span class="mf">36.5</span><span class="p">,</span> <span class="mf">36.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span>
       <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">42.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.5</span><span class="p">,</span>
       <span class="mf">44.0</span><span class="p">])</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span>
       <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span>
       <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span>
       <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span>
       <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1000.0</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="p">)</span>

<span class="n">train_input</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">test_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">knr</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1">#k-최근접 이웃 회귀모델을 훈련합니다.
</span><span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="c1">#모델을 사용하여 길이가 50cm인 농어의 무게를 예측합니다.
</span><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">50</span><span class="p">]]))</span>
</code></pre></div></div>
<blockquote>
  <p>[1033.33333333]</p>
</blockquote>

<p>최근접 이웃 회귀 모델은 이 농어의 무게를 약 1033g 정도로 예측했지만, 실제 값은 더 많이 나간다고 합니다.
이를 해결하기 위해 우선 산점도에 표시해 자료를 확인해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1">#타깃 농어 이웃 구하기
</span><span class="n">distances</span><span class="p">,</span> <span class="n">indexes</span> <span class="o">=</span> <span class="n">knr</span><span class="p">.</span><span class="n">kneighbors</span><span class="p">([[</span><span class="mi">50</span><span class="p">]])</span>

<span class="c1">#훈련 세트 산점도 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="c1">#훈련 세트 중 이웃 샘플만 다시 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="n">indexes</span><span class="p">],</span> <span class="n">train_target</span><span class="p">[</span><span class="n">indexes</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'D'</span><span class="p">)</span>

<span class="c1">#타깃 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1033</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-2-1.png?raw=true" alt="HG3-2-1" /><br />
<br />
산점도를 보면 어떤 문제인지 알 수 있습니다.<br />
샘플에 비해 이웃 샘플들의 평균한 무게가 너무 적었던 것입니다.<br />
이웃 샘플의 평균을 구하면 수치상으로도 알수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_target</span><span class="p">[</span><span class="n">indexes</span><span class="p">]))</span>
</code></pre></div></div>
<blockquote>
  <p>1033.3333333333333</p>
</blockquote>

<p>예측한 수치와 일치합니다. 길이가 100cm인 농어도 똑같은 값으로 예측합니다. 이를 해결하기 위해 다른 모델을 적용하고자 합니다.<br />
선형회귀 또한 사이킷런의 LinearRegression 클래스를 이용하여 훈련할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># 모델 훈련
</span><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="c1"># 50cm 농어 예측
</span><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">50</span><span class="p">]]))</span>
</code></pre></div></div>
<blockquote>
  <p>[1241.83860323]</p>
</blockquote>

<p>최근접 이웃 회귀모델이 예측한 값보다 훨씬 높은 값을 예측했습니다.<br />
훈련세트의 산점도와 함께 농어의 길이에 따른 무게를 직선으로 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 훈련세트 산점도
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="c1"># 15에서 50까지의 1차 방정식 그래프 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="o">*</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="o">+</span><span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="mi">50</span><span class="o">*</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="o">+</span><span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">])</span>

<span class="c1"># 타깃 데이터 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mf">1241.8</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-2-2.png?raw=true" alt="HG3-2-2" /><br />
이제 결정계수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.939846333997604<br />
0.8247503123313558</p>
</blockquote>

<p>점수를 본 결과 전체적으로 높지 않습니다. 또한 그래프에서도 산점도는 곡선의 형태를 띄기 때문에 직선은 데이터를 잘 대변하지 못하는 것 같습니다.<br />
최적의 곡선을 찾기 위해 길이를 제곱한 항을 훈련 세트에 추가해보겠습니다.<br />
농어의 길이를 제곱하여 원래 데이터 앞에 붙입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_poly</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">train_input</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">train_input</span><span class="p">))</span>
<span class="n">test_poly</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">test_input</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">test_input</span><span class="p">))</span>

<span class="c1">#새 데이터셋 크기 확인하기
</span><span class="k">print</span><span class="p">(</span><span class="n">train_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_poly</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 2) (14, 2)</p>
</blockquote>

<p>이제 train_poly() 함수를 사용하여 선형 회귀 모델을 다시 훈련합니다.<br />
여기서 주의할 점은 2차 방정식 그래프를 찾기 위해 데이터에는 제곱항을 추가했지만 타깃 데이터는 그대로 사용한다는 것입니다.<br />
이 훈련세트로 모델을 훈련한 후 50cm 농어의 무게를 예측해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">50</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">]]))</span>
</code></pre></div></div>
<blockquote>
  <p>[1573.98423528]</p>
</blockquote>

<p>앞서 훈련한 값보다 높은 값을 출력했습니다.<br />
이 모델이 훈련한 계수와 절편을 확인해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[1.01433211 -21.55792498] 116.0502107827827</p>
</blockquote>

<p>이 모델은</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>무게 = 1.01 * 길이^2 - 21.6 * 길이 + 116.05
</code></pre></div></div>

<p>다음과 같은 그래프를 학습한 것으로 생각할 수 있습니다.<br />
이러한 다항식을 사용한 선형 회귀를 다항회귀라고 부릅니다.<br />
이제 산점도를 다시 그려보겠습니다.<br />
다항식을 표현할 곡선은 직선을 짧게 이어 그려 표현할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#구간별 직선을 그리기 위하여 15부터 49까지 정수 배열 만들기
</span><span class="n">point</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1">#훈련세트 산점도
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>

<span class="c1">#15부터 49까지 2차 방정식 그래프 그리기
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="mf">1.01</span><span class="o">*</span><span class="n">point</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">21.6</span><span class="o">*</span><span class="n">point</span> <span class="o">+</span> <span class="mf">116.05</span><span class="p">)</span>

<span class="c1">#50cm 농어 데이터
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1574</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-2-3.png?raw=true" alt="HG3-2-3" /><br />
<br />
앞서 만들었던 선형 회귀 모형보다 더 나은 그래프가 그려진 것이 보입니다.<br />
이제 결정계수를 출력해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_poly</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_poly</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9706807451768623<br />
0.9775935108325122</p>
</blockquote>

<p>두 점수 모두 좋은 점수가 나왔습니다.<br />
다만 train 보다 test 점수가 더 높은 것을 보니 과소적합이 조금 남아있는 것 같습니다.<br />
더 복잡한 모델이 필요한 것 같으니 다음 시간에 알아보도록 하겠습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[3-2 선형 회귀]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/HGML.png" /><media:content medium="image" url="http://localhost:4000/assets/images/HGML.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(1)</title><link href="http://localhost:4000/ml/HG3-1/" rel="alternate" type="text/html" title="[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(1)" /><published>2022-05-02T00:00:00+09:00</published><updated>2022-05-02T00:00:00+09:00</updated><id>http://localhost:4000/ml/HG3-1</id><content type="html" xml:base="http://localhost:4000/ml/HG3-1/"><![CDATA[<p>지도학습 알고리즘은 분류와 회귀로 나눌 수 있습니다.<br />
분류는 앞서 2장에서 했던 것처럼 클래스 중 하나로 분류하는 것이고, 회귀는 임의의 어떤 숫자를 예측하는 것입니다.</p>

<p>k-최근접 이웃 분류는 앞서 진행했고, 이번에는 k-최근접 이웃 회귀를 알아보겠습니다.<br />
k-최근접 이웃 회귀는 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하여 이웃 수치들의 평균을 구하는 방식입니다.<br />
<br /><br />
회귀분석에 쓰일 데이터를 불러온 후 산점도를 그려보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#numpy import
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#Data load
</span><span class="n">perch_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.4</span><span class="p">,</span> <span class="mf">13.7</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">16.2</span><span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">18.0</span><span class="p">,</span> <span class="mf">18.7</span><span class="p">,</span> <span class="mf">19.0</span><span class="p">,</span> <span class="mf">19.6</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span>
       <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.3</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.7</span><span class="p">,</span>
       <span class="mf">23.0</span><span class="p">,</span> <span class="mf">23.5</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.6</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">,</span> <span class="mf">25.6</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">27.3</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span>
       <span class="mf">27.5</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">28.7</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">32.8</span><span class="p">,</span> <span class="mf">34.5</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">,</span> <span class="mf">36.5</span><span class="p">,</span> <span class="mf">36.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span>
       <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">42.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.5</span><span class="p">,</span>
       <span class="mf">44.0</span><span class="p">])</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span>
       <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span>
       <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span>
       <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span>
       <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1000.0</span><span class="p">])</span>

<span class="c1">#산점도 그리기
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/HG03-1/HG3-1-1.png?raw=true" alt="HG3-1-1" /><br />
<br />
다음과 같이 우상향하는 그래프가 출력되었습니다.<br />
농어 길이가 늘어날 수록 무게도 늘어나는 것은 확인했습니다.<br />
다음으로 데이터를 훈련 세트와 테스트 세트로 나눕니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="p">)</span>
</code></pre></div></div>
<p>train_test_split 메서드는 총 데이터의 25%를 test 데이터로 떼어냅니다.<br />
random_state는 책과 동일한 결과가 나올 수 있게 한 것이기 때문에 생략해도 무방합니다.<br />
사이킷런을 사용해 모델을 훈련할 것이기 때문에 reshape을 사용하여 배열을 2차원으로 바꿔줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_input</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">test_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>(42, 1) (14, 1)</p>
</blockquote>

<p>reshape의 첫 번째 인자를 -1로 지정함으로써 원소의 개수로 채우라는 의미입니다. 배열의 전체 원소의 개수를 외우지 않아도 되기 때문에 편리합니다.<br />
이제 k-최근접 이웃 알고리즘을 훈련시켜보도록 하겠습니다.
사이킷런에서 KNeighborsRegressor을 사용하여 회귀 모델을 fit 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">knr</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">()</span>

<span class="c1">#k-최근접 이웃 회귀모델을 훈련합니다.
</span><span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.992809406101064</p>
</blockquote>

<p>거의 1에 가까운 숫자가 나왔습니다.<br />
2장에서 분류를 할 때는 정확도라고 불렀던 이 숫자를 회귀에서는 결정계수라고 부릅니다. 결정계수가 1에 가까워지면 예측이 타깃과 비슷하다는 의미가 됩니다.<br />
아제 타깃과 예측한 값의 차이를 구해보겠습니다. sklearn.metrics 패키지의 mean_absolute_error을 이용하여 둘 사이의 절댓값 오차를 평균을 낸 값을 출력합니다.<br />
그리고 훈련 세트를 사용하여 score값도 출력해보도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="c1">#테스트 세트에 대한 예측
</span><span class="n">test_prediction</span> <span class="o">=</span> <span class="n">knr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

<span class="c1">#테스트 세트의 평균 절댓값 오차 계산
</span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">test_prediction</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mae</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>19.157142857142862<br />
0.9698823289099254</p>
</blockquote>

<p>위 코드를 출력하면 평균적으로 오차가 19g정도 났다는 것을 확인할 수 있습니다.<br />
또한 훈련을 한 세트로 score을 냈는데도 불구하고 오히려 test set보다 결정계수가 더 낮게 나온 것을 확인할 수 있습니다.<br />
이처럼 훈련 세트보다 테스트 세트가 점수가 높거나 두 점수 모두 낮은 경우를 과소적합이라고 하고, 반대로 너무 차이나게 훈련 세트가 점수가 높을 경우를 과대적합이라고 합니다. 이번 훈련에서는 과소적합이 된 것입니다.  그렇다면 모델을 더 복잡하게 만듦으로써 이를 해결해보도록 하겠습니다.<br />
모델을 복잡하게 만들기 위해 k를 3으로 줄입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 이웃의 개수를 3으로 설정
</span><span class="n">knr</span><span class="p">.</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># 모델 훈련
</span><span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9804899950518966</p>
</blockquote>

<p>k값을 낮췄더니 결정계수의 값이 올라갔습니다. 이번엔 test 세트를 적용해봅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p>0.9746459963987609</p>
</blockquote>

<p>test set 또한 너무 낮지 않은 점수로 잘 나왔습니다.<br />
train set과 점수 차이도 많이 나지 않아 과대적합도 아닙니다. 이처럼 과소적합 시 모델을 더 복잡하게 (k값을 줄여서) 만들어야 하고, 과대적합 시 모델을 덜 복잡하게 (k값을 늘려서) 만듦으로써 해결할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="ML" /><category term="Blog" /><category term="Machine learning" /><category term="Data mining" /><category term="혼공머신러닝" /><summary type="html"><![CDATA[3-1 k-최근접 이웃 회귀]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/HGML.png" /><media:content medium="image" url="http://localhost:4000/assets/images/HGML.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">[Github Blog] 깃헙 블로그에 giscus 적용하기</title><link href="http://localhost:4000/blog/coments/" rel="alternate" type="text/html" title="[Github Blog] 깃헙 블로그에 giscus 적용하기" /><published>2022-04-28T00:00:00+09:00</published><updated>2022-04-28T00:00:00+09:00</updated><id>http://localhost:4000/blog/coments</id><content type="html" xml:base="http://localhost:4000/blog/coments/"><![CDATA[<p>github blog에 giscus를 사용하여 댓글창을 만들어보겠습니다.<br />
보통 깃헙 블로그 댓글창으로는 disqus를 많이 사용하지만, 광고가 많고 무겁다는 평이 많아 비교적 가벼운 giscus를 적용해보려고 합니다.</p>

<h3 id="1-giscus-설치하기">1. giscus 설치하기</h3>
<p>다음 <a href="https://giscus.app/ko">giscus</a> 사이트에 접속합니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc1.png?raw=true" alt="gc1" /><br />
<br />
다음 giscus를 눌러 giscus 댓글창을 사용할 레포지토리에 install 합니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc2.png?raw=true" alt="gc2" /><br />
<br />
저는 이미 설치했기 때문에 configure이라고 뜨지만, 파란 install 버튼을 눌러줍니다.<br />
install 버튼을 누르면, <code class="language-plaintext highlighter-rouge">all repositories</code>와 <code class="language-plaintext highlighter-rouge">only select repositories</code> 둘 중 선택하는 옵션이 있는데, <code class="language-plaintext highlighter-rouge">only select repositories &gt; 댓글창 적용할 repositories</code>선택하여 설치합니다.</p>

<h3 id="2-repository-discussion-기능-활성화">2. repository discussion 기능 활성화</h3>
<p>이제 giscus를 다운받은 repo의 discussion 기능을 활성화 시켜줘야 합니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc3.png?raw=true" alt="gc3" /><br />
<br />
giscus를 다운받은 repo의 settings로 들어갑니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc4.png?raw=true" alt="gc4" /><br />
<br />
스크롤을 내려 <code class="language-plaintext highlighter-rouge">Features/Discussions</code> 를 활성화 시켜줍니다.</p>

<h3 id="3-giscus-코드-copy하기">3. giscus 코드 copy하기</h3>
<p><br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc5.png?raw=true" alt="gc5" /><br />
<br />
이제 다시 giscus 사이트로 돌아가 언어를 지정하고, giscus를 설치하고 discussions 기능을 활성화해준 repo의 이름을 적어줍니다.<br />
지금까지 정상적으로 진행되었다면 repo 입력란 주위에 통과했다는 메세지가 뜹니다.<br />
그리고 페이지와 discussion 간의 연결을 선택해줍니다.<br />
보통 title과 url보다 바뀔 가능성이 적은 경로로 연결하기 때문에 저도 그렇게 지정했습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc6.png?raw=true" alt="gc6" /><br />
<br />
이제 discussion 카테고리를 지정해주고, 본인이 원하는 기능을 선택한 후에 쓰고있는 테마를 선택합니다.<br />
그럼 이제 giscus 활성화 아래 코드가 나타나는데, 코드를 copy하고 <code class="language-plaintext highlighter-rouge">_layouts/posts.html</code> 파일 마지막 부분에 붙여넣습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc7.png?raw=true" alt="gc7" /></p>

<h3 id="4-config-파일에서-적용하기">4. config 파일에서 적용하기</h3>
<p>제가 사용하는 minimal-mistakes 테마가 giscus를 지원하기 때문에 블로그의 _config.yml 파일에 comments를 지정하는 곳에 “giscus”를 입력하고 아래 giscus창에 앞선 코드를 참고하여 붙여넣어줍니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gc8.png?raw=true" alt="gc8" /><br />
<br />
보통 이 정도의 과정을 거치면 댓글창이 적용이 되지만, 확인해보니 아직 적용이 되지 않은 것을 볼 수 있습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gis1.png?raw=true" alt="gis1" /><br />
<br />
이럴 때는 다음과 같이 각 포스트의 title 과 tag 등을 지정하는 곳에 <code class="language-plaintext highlighter-rouge">comments : "giscus"</code>를 지정해주고 push 해줍니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gis2.png?raw=true" alt="gis2" /><br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/giscus/gis3.png?raw=true" alt="gis3" /><br />
<br />
그리고 다시 확인해보면 위와 같이 comments 창이 제대로 뜨는 것을 확인할 수 있습니다.</p>]]></content><author><name>Younghyun Park</name></author><category term="Blog" /><category term="Blog" /><category term="giscus" /><category term="Github blog" /><category term="blog custom" /><summary type="html"><![CDATA[깃허브 블로그에 giscus를 이용하여 댓글 사용하기]]></summary></entry><entry><title type="html">[Github Blog] google analytics 적용하기</title><link href="http://localhost:4000/blog/GA-post/" rel="alternate" type="text/html" title="[Github Blog] google analytics 적용하기" /><published>2022-04-27T00:00:00+09:00</published><updated>2022-04-27T00:00:00+09:00</updated><id>http://localhost:4000/blog/GA%20post</id><content type="html" xml:base="http://localhost:4000/blog/GA-post/"><![CDATA[<p>github blog에 google analytics를 적용해보겠습니다.<br />
저는 github blog에 jekyll minimal-mistakes 테마를 적용하고 있습니다.</p>

<h3 id="1-google-analytics-계정-생성하기">1. google analytics 계정 생성하기</h3>
<p>우선 <a href="https://analytics.google.com/analytics/web/provision/?hl=ko&amp;pli=1#/provision">google analytics</a> 페이지에 접속해서 계정을 생성합니다.<br />
  <br />
  <img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA1.png?raw=true" alt="GA1" /><br />
  <br />
  google analytics 페이지에서 측정 시작을 눌러줍니다.<br />
  <br />
  <img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA2.png?raw=true" alt="GA2" /><br />
  <br />
  계정 이름을 입력하고 다음으로 넘어갑니다.<br />
  <br />
  <img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA3.png?raw=true" alt="GA3" /><br />
  <br />
  속성 이름을 입력하고, 보고 시간대와 통화를 지정해줍니다.<br />
  저는 속성 이름에 Github blog를, 시간대와 통화를 모두 대한민국으로 지정했습니다.<br />
  <br />
  <img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA4.png?raw=true" alt="GA4" /> <br />
  <br />
  비즈니스 정보를 입력합니다. 업종은 지정하지 않아도 됩니다.<br />
  비즈니스 규모와 사용 계획은 본인의 사이트와 맞게 입력해줍니다.<br />
  <br />
  <img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA5.png?raw=true" alt="GA5" /><br />
  <br />
  마지막으로 만들기를 누르고, 약관에 동의하면 계정 만들기가 완료 되었습니다.</p>

<h3 id="2-데이터-스트림-설정하기">2. 데이터 스트림 설정하기</h3>
<p>이제 생성한 GA 계정을 이용하여 데이터 스트림을 설정하겠습니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA6.png?raw=true" alt="GA6" /><br />
<br />
다음 화면에서 우리는 github blog에 만들 것이기 때문에 웹을 선택해줍니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA7.png?raw=true" alt="GA7" />
<br />
데이터 스트림 설정 창이 뜨면, <code class="language-plaintext highlighter-rouge">http://</code> 를 제외한 깃헙 url을 입력한 후, 스트림 이름을 지정해줍니다.<br />
저는 github blog로 지정하였습니다.<br />
그리고 만들기를 눌러 생성합니다. 
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA8.png?raw=true" alt="GA8" /><br />
<br />
스트림을 만든 후, 웹 스트림 세부정보에서 측정 ID를 복사합니다.<br />
이렇게 복사한 측정 ID는 github blog 로컬에 위치해있는 <code class="language-plaintext highlighter-rouge">_config.yml</code> 파일 속 google tracking id 적는 란에 다음과 같이 추가해줍니다.<br />
<br />
<img src="https://github.com/yhp2205/yhp2205.github.io/blob/main/assets/images/GA/GA9.png?raw=true" alt="GA9" /></p>

<blockquote>
  <p>참고한 포스트
<a href="https://ynkim0.github.io/posts/Jekyll-Chripy-Google-Analytics-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0/">Jekyll Chripy Google Analytics 연동하기</a></p>
</blockquote>]]></content><author><name>Younghyun Park</name></author><category term="Blog" /><category term="Blog" /><category term="google analytics" /><category term="Github blog" /><category term="blog custom" /><summary type="html"><![CDATA[깃허브 블로그에 GA를 설정하여 사용자 통계 확인하기]]></summary></entry></feed>