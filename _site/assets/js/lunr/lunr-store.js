var store = [{
        "title": "github.io 블로그 첫 포스팅입니다",
        "excerpt":"새로 블로그를 운영해보도록 하겠습니다.  ","categories": ["Blog"],
        "tags": ["Blog"],
        "url": "/blog/First-post/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 2. 데이터 다루기(1)",
        "excerpt":"훈련 세트와 테스트 세트  머신러닝 알고리즘     지도학습 : 훈련하기 위한 데이터, 정답이 필요   비지도학습   지도학습에서 데이터를 입력, 정답(결과)을 타깃이라고 하고 입력과 타깃을 합쳐 훈련 데이터라고 하고, 입력 데이터에서 타깃을 결정짓는 것을 특성이라고 합니다.   데이터를 나누어 일부는 머신러닝 알고리즘을 만드는 데에 사용하고, 나머지는 만들어진 알고리즘의 성능을 평가하는 데에 쓰인다는 것은 알고 있기 때문에 바로 데이터를 나눠보겠습니다.   우선 데이터를 입력합니다.   #fish의 길이와 무게에 대한 데이터 fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,                  31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,                  35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,                  10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,                  500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,                  700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,                  7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]  fish_length와 fist_weight 이 각각의 데이터를 fish_data라는 이름의 하나의 데이터로 묶고,  target 변수로 도미 35마리, 빙어 14마리로 각각 지정해줍니다.  fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1]*35 + [0]*14  #target 변수를 앞선 35개엔 1로, 나머지 14개에는 0으로 부여  모델 객체를 만들기 위하여 사이킷런의 KNeighborsClassifier을 임포트해줍니다.  from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier()  전체 데이터에서 처음 35개의 데이터를 훈련 세트로, 나머지 14개의 데이터를 테스트 세트로 사용합니다.   슬라이싱하여 선택하도록 하겠습니다.  슬라이싱을 사용할 때 마지막 인덱스의 번호는 포함되지 않는다는 것을 유의합니다.  # 훈련 세트로 입력값 중 0부터 34번째 인덱스까지 사용할 때 train_input = fish_data[:35] train_target = fish_target[:35]  # 테스트 세트로 입력값 중 35번째부터 마지막 인덱스까지 사용할 때 test_input = fish_data[35:] test_target = fish_target[35:]  훈련 데이터와 테스트용 데이터를 준비했으니 훈련 세트로 fit() 메서드를 호출하여 모델을 훈련하고, 테스트 세트로 score() 매서드를 호출하여 평가해봅니다.  kn = kn.fit(train_input, train_target) kn.score(test_input, test_target)  정확도가 0이 나오는 것을 확인할 수 있는데, 이는 훈련세트와 테스트세트가 적절하지 못하게 나뉘었기 때문입니다.  fish_data에는 순서대로 35개의 도미와 14개의 빙어가 들어있기 때문에, 훈련세트에 빙어가 없었다는 것입니다.  이러한 샘플링 편향을 방지하기 위해 데이터를 섞거나 고루 샘플을 뽑는 법을 알아보겠습니다.   배열을 다루기 위해서는 파이썬의 대표적인 배열 라이브러리인 numpy를 알아야 합니다. 넘파이를 사용하여 데이터를 다시 나눌 것이기 때문에 우선 import를 해줍니다.  import numpy as np  그리고 넘파이를 이용하여 파이썬 리스트 형태였던 fish_data, fish_target을 넘파이 배열로 바꿔줍니다.  input_arr = np.array(fish_data) target_arr = np.array(fish_target) print(input_arr)  원하는 형태로 잘 저장된 것을 볼 수 있습니다. 이제 무작위로 샘플을 고릅니다. 우선 인덱스를 input_arr와 target_arr에 부여하고 랜덤하게 섞어 출력해보면, 골고루 섞인 것을 확인할 수 있습니다.  np.random.seed(42) index = np.arange(49) np.random.shuffle(index) print(index)  이제 넘파이 배열을 인덱스로 전달하여 랜덤한 35개의 샘플을 train 세트로 만들고,  나머지 14개의 샘플을 test 세트로 만듭니다.  train_input = input_arr[index[:35]] train_target = target_arr[index[:35]]  test_input = input_arr[index[35:]] test_target = target_arr[index[35:]]  이제 훈련 데이터를 잘 섞어서 만들었으니 산점도를 그려 잘 섞여있는지 다시 한 번 확인합니다.  import matplotlib.pyplot as plt plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(test_input[:,0], test_input[:,1]) plt.xlabel('length') plt.ylabel('weight') plt.show()  이제 모델을 다시 만들어보겠습니다.  kn = kn.fit(train_input, train_target) kn.score(test_input, test_target)  아까와 달리 정확도가 1.0이 나온 것을 알 수 있습니다.  이제 predict() 메서드를 사용하여 예측 결과와 실제 타깃을 확인합니다.  kn.predict(test_input)  test_target  predict 메서드를 사용한 결과와 실제 test_target 결과가 일치합니다.   처음에 진행한 모델링에서는 도미와 빙어가 적절히 섞이지 않은 데이터를 가지고 훈련을 했기 때문에 정확도가 매우 떨어졌지만, 후에 데이터를 적절히 잘 섞어 골라 모델을 만들고 나니 정확도가 올라가는 것을 경험할 수 있었습니다.  이와 같이 데이터를 추출하여 지도학습을 할 때에 train dataset과 test dataset을 적절히 잘 나누어 모델링을 해야 좋은 결과를 얻을 수 있습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG2-1/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[Github Blog] favicon 적용하기",
        "excerpt":"Github blog에 favicon 적용하기   favicon은 브라우저의 탭에 위치해있는 이미지를 의미합니다.     favicon을 특별히 지정하지 않으면, 다음과 같이 지구본 모양을 띄게 됩니다.  favicon을 바꾸는 방법에 대해 알아보도록 하겠습니다.    1. 적절한 이미지 준비하기   저는 다음 이미지로 해보도록 하겠습니다.         2. 이미지 파일 사이트에 업로드하기   realfavicongenerator 사이트에 접속하여    Select your Favicon image를 누르고 파일을 업로드 합니다.           다음 페이지에서는 원하는 이미지가 favicon으로 적용됐을 때 어떻게 보이는지 알려줍니다.          3. 가공된 이미지 파일 다운받기          페이지 하단에 위치한 Generate your Favicons and HTML code를 누르고 다음 페이지의 Favicon package를 눌러 zip 파일을 다운로드 합니다.           4. asset 폴더 내에 logo 폴더 만들어서 붙여넣기           다운로드한 파일 속에 있는 이미지들을 전부 복사하여    github.io 속 asset 폴더에 새로 logo.ico 폴더를 생성하고 붙여넣기 해줍니다.           5. _includes 폴더 속 head.html 파일 수정하기           사이트 마지막에 있었던 코드들을 _includes -&gt; head -&gt; custom.html 파일에 복사 붙여넣기 합니다.    이 때 주의할 점이 {{site.baseurl}}/assets/logo.ico     다음 코드를 반드시 href=” 뒤에 붙여주어야 합니다.        수정이 끝난 후 push를 해주었더니 다음과 같이 적용된 것을 확인할 수 있습니다.           ","categories": ["Blog"],
        "tags": ["Blog","favicon","Github blog","blog custom"],
        "url": "/blog/Blogfavi/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 2. 데이터 다루기(2)",
        "excerpt":"앞선 머신러닝 알고리즘에서 길이 25센치에 무게 150g의 도미를 빙어로 예측한다는 이상한 결과가 나왔습니다. 이를 보완하기 위해 우선 앞선 데이터를 다시 불러오겠습니다.   #fish의 길이와 무게에 대한 데이터 fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,                  31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,                  35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,                  10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,                  500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,                  700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,                  7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]  numpy의 column_stack() 함수는 전달받은 리스트를 일렬로 세운 차례대로 연결해주는 함수입니다.  이 함수를 사용하여 fish_length와 fish_weight를 합쳐보겠습니다.  import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) print(fish_data[:5])  앞에서부터 5개의 데이터를 확인함으로써 두 리스트가 잘 연결된 것을 확인했습니다. 동일한 방법으로 타깃 데이터도 만들어보겠습니다. 넘파이에서는 np.ones() 와 np.zeros() 함수를 이용하여 배열을 만들 수 있습니다. 또한 np.concatenate()함수를 이용하여 두 배열을 연결할 수 있습니다.  fish_target = np.concatenate((np.ones(35), np.zeros(14))) print(fish_target)  fish_target을 만들었으니 훈련세트와 테스트세트로 나눠보겠습니다. 앞선 방식과 다르게 사이킷런의 model_selection 모듈 아래 있는 train_test_split() 함수를 이용합니다.  from sklearn.model_selection import train_test_split  train_test_split() 함수에는 자체적으로 랜덤 시드를 지정할 수 있는 random_state 매개변수가 존재하니, 사용해서 나눠보겠습니다.  train_input, test_input, train_target, test_target = train_test_split(     fish_data, fish_target, random_state = 42 )  train_test_split() 함수는 기본적으로 25%를 테스트 세트로 분류합니다. 잘 나누어졌는지 shape 속성으로 확인해보겠습니다.  print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape)  이제 도미와 방어가 적절히 섞였는지 test_target을 출력하여 확인합니다.  print(test_target)  도미와 방어가 섞이기는 했지만 두 생선의 비율에 비해 샘플링 편향이 일어났습니다. 따라서 train_test_split() 함수에서 stratify 매개변수에 타깃 데이터를 입력하면 클래스 비율에 맞춰 데이터를 나눌 수 있습니다.  train_input, test_input, train_target, test_target = train_test_split(     fish_data, fish_target, stratify = fish_target, random_state = 42 ) print(test_target)  조금 더 전체 비율과 비슷한 비율로 나누었습니다. 이 데이터를 가지고 앞서 진행했던 모델 훈련과 평가를 해보도록 하겠습니다.  from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target)  이제 이 모델에 문제의 도미인 25cm에 150g의 물고기를 입력하여 예측값을 확인합니다. 1은 도미, 0은 빙어입니다.  print(kn.predict([[25, 150]]))  도미가 빙어로 출력된 것을 볼 수 있습니다. 이를 산점도로 그려 확인해봅니다.  import matplotlib.pyplot as plt plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker='^') #marker을 이용하여 다른 모양으로 지정해줍니다 plt.xlabel('length') plt.ylabel('weight') plt.show()    샘플의 위치를 보면 도미에 가깝습니다. 이를 빙어로 판단한 원인은 KNeighborsClassifier 클래스에 있습니다. KNeighborsClassifier 클래스는 주어진 샘플에서 가장 가까운 이웃 5개 중 다수인 클래스를 예측으로 사용합니다. indexes 배열을 사용하여 이웃 샘플을 구분해 산점도를 그려보겠습니다.  distances, indexes = kn.kneighbors([[25, 150]]) plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = '^') plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker='D') # marker = 'D' : 산점도 마름모로 그리기 plt.xlabel('length') plt.ylabel('weight') plt.show()    산점도로 확인해보니 5개중 하나만 도미고, 4개의 데이터가 빙어입니다.  print(train_input[indexes]) print(train_target[indexes])  직접 데이터를 확인해 본 결과, 더 명확히 1개만 도미고 나머지 이웃은 빙어임을 알 수 있습니다. 이번엔 kneighbors() 매서드에서 이웃 샘플까지의 거리를 알 수 있는 distances 배열을 출력해보면,  산점도에서는 도미보다 빙어가 훨씬 많이 떨어져있는데, distances 배열에서는 그렇지 않은 것을 알 수 있습니다.  거리와 산점도가 다르게 나온 이유는 x축과 y축의 scale이 다르기 때문입니다.  length는 10부터 40이 주 범위인 반면에 weight는 0부터 1000까지의 넓은 범위를 갖고 있습니다.  x축의 scale을 조정하여 범위를 동일하게 맞추고 산점도를 다시 그려봅니다.  plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = '^') plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = 'D') plt.xlim((0, 1000)) plt.xlabel('length') plt.ylabel('weight') plt.show()    x축의 scale을 조정하여 범위를 동일하게 맞추면, 다음과 같이 산점도가 나오게 됩니다. 이러한 데이터라면 length는 weight에 비해 크게 영향을 미치지 못하게 됩니다. 따라서 두 특성의 범위가 다른 것을 고려하여, 특성값을 일정한 기준으로 맞춰주어야 하고 이런 작업을 데이터를 전처리한다고 말합니다.  이럴 때 가장 흔하게 사용되는 방법은 표준점수로, 각 특정 값이 0에서 표준편차의 몇 배만큼 떨어져있는지를 나타냅니다. 이를 이용하면 실제 특성 값의 크기와 상관 없이 동일한 조건으로 비교할 수 있습니다. 각 특성별로 계산해야하기 때문에 mean과 std를 axis = 0으로 지정하고 계산해보겠습니다.  mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) print(mean, std)  이제 train_input 데이터에서 평균을 빼고 표준편차로 나누어 표준 점수를 구합니다.  train_scaled = (train_input - mean)/std  plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(25, 150, marker = '^') plt.xlabel('length') plt.ylabel('weight') plt.show()    다시 산점도를 그려봤을 때 이런 결과가 나온 것은 데이터는 전부 표준점수화 시켜두고 25, 150의 샘플은 그대로 입력했기 때문입니다.  샘플 데이터인 25, 150도 mean과 std를 사용하여 변환해줍니다.  new = ([25, 150] - mean)/std plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker = '^') plt.xlabel('length') plt.ylabel('weight') plt.show()    산점도도 잘 그려지고 범위도 표준점수화 된 것을 확인했으니 모델을 다시 훈련합니다.  kn.fit(train_scaled, train_target) test_scaled = (test_input - mean)/std #test dataset도 표준점수화하기 kn.score(test_scaled, test_target)  이제 앞서 잘못 분류했던 데이터를 predict 해보겠습니다.  print(kn.predict([new])) # 1 = 도미, 0 = 빙어  결과값은 1로, 도미로 예측하였습니다.  마지막으로 kneighbors() 함수를 사용하여 근접한 이웃을 구하는 산점도를 그려봅니다.  distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker = '^') plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = 'D') plt.xlabel('length') plt.ylabel('weight') plt.show()    이제 샘플에서 가까운 점들은 모두 도미로 옳게 출력이 된 것을 확인할 수 있습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG2-2/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[Github Blog] google에서 검색되도록 설정하기",
        "excerpt":"github blog 글이 포털에 검색되도록 설정하려면 각 포탈에 블로그를 등록해주어야 합니다.  google에서 글이 검색이 가능하도록 등록해보겠습니다.   Github Blog 글이 google에 검색되도록 설정하기   1. sitemap 설정  sitemap을 google에 등록함으로써 주기적인 크롤링을 통해 url을 연결할 수 있습니다. sitemap을 만들어보겠습니다.  블로그의 root 경로(github.io 폴더)에 /sitemap.xml 파일을 만들고 아래의 내용을 복사해서 붙여넣습니다.   --- layout: null --- &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;urlset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\" xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;   {% for post in site.posts %}     &lt;url&gt;       &lt;loc&gt;{{ site.url }}{{ post.url }}&lt;/loc&gt;       {% if post.lastmod == null %}         &lt;lastmod&gt;{{ post.date | date_to_xmlschema }}&lt;/lastmod&gt;       {% else %}         &lt;lastmod&gt;{{ post.lastmod | date_to_xmlschema }}&lt;/lastmod&gt;       {% endif %}        {% if post.sitemap.changefreq == null %}         &lt;changefreq&gt;weekly&lt;/changefreq&gt;       {% else %}         &lt;changefreq&gt;{{ post.sitemap.changefreq }}&lt;/changefreq&gt;       {% endif %}        {% if post.sitemap.priority == null %}           &lt;priority&gt;0.5&lt;/priority&gt;       {% else %}         &lt;priority&gt;{{ post.sitemap.priority }}&lt;/priority&gt;       {% endif %}      &lt;/url&gt;   {% endfor %} &lt;/urlset&gt;   다음 파일을 git commit으로 push 후에 \"blog_url\"/sitemap.xml 로 접속했을 때 아래와 같은 화면이 나와야 정상적으로 sitemap이 등록된 것입니다.     sitemap에서는 해당 글의 정보들이 설정이 되는데 이것은 각 포스트의 맨 윗 부분에 다음과 같이 sitemap 옵션을 추가해줌으로서 추가적으로 설정할 수 있습니다.  --- layout: post title:  \"제목\" date:   2022-04-23 12:00:00  lastmod : 2022-04-24 12:00:00 sitemap :   changefreq : daily   priority : 1.0 ---  포스트의 시작 부분에 다음과 같이 sitemap 코드를 적용해보겠습니다.      git push 한 후에 다시 \"blog_url\"/sitemap.xml로 접속하여 확인해보겠습니다.       위 사진이 sitemap 지정 전, 아래 사진이 지정 후 사진입니다.  sitemap을 설정해준 부분은 changefreq가 daily로 바뀌고, priority는 1.0으로 잘 바뀌었습니다.  반면 sitemap을 설정 안 한 포스트는 default값 그대로임을 확인할 수 있습니다.   2. robots.txt 생성  robots.txt 파일에 sitemap.xml 파일의 위치를 등록해두면 검색 엔진의 크롤러들이 홈페이지를 크롤링하는 데에 도움이 된다고 합니다.  따라서 root 디렉토리github.io/에 robots.txt 파일을 만들고 다음과 같이 입력합니다.  User-agent: * Allow: /  Sitemap: http://\"blog_url\".github.io/sitemap.xml   3. 검색 엔진에 등록하기  google search console에 등록을 해야 최종적으로 검색 엔진에 블로그 포스트가 뜨게 됩니다.  google search console site에 접속한 후 시작하기 버튼을 누릅니다.      위 화면에서 URL prefix에 블로그 url을 붙여넣습니다.  여기서 Domain, URL prefix 등록의 차이점은 도메인 속성은 DNS 레코드만을 사용해서 verify할 수 있으나 모든 하위 도메인의 데이터를 포함한 웹사이트의 정보를 모두 얻을 수 있습니다.  URL perfix 등록은 입력한 속성과 하위 도메인의 레벨만 포함되기 때문에, https://yhp2205.github.io/를 등록했다면 https://yhp2205.github.io/First-post/ 같은 데이터는 포함이 되지만,  http://yhp2205.github.io는 포함이 되지 않습니다. 대신 DNS와 달리 여러 경로로 사이트 소유 인증을 할 수 있습니다.  저는 URL prefix를 이용하여 등록해보겠습니다.  Other verification methods에서 HTML tag를 눌러 메타 태그를 copy합니다.      이제 github blog의 root에 존재하는 _includes/head/custom.html 파일 맨 상단에 copy한 메타 태그를 붙여넣어 github에 push 해주었습니다.  그리고 HTML tag를 copy한 페이지에서 verify를 눌러줍니다.      그럼 Ownership verified 라는 문구가 뜨며 등록이 완료됩니다. 그럼 이제 마지막으로 sitemap을 올리기 위해 GO TO PROPERTY를 누릅니다.         마지막으로 google search console 창에서 sitemaps 카테고리를 눌러 sitemap.xml을 submit 해주면 끝입니다.      참고한 포스트  쉽게 하는 웹사이트 구글서치콘솔 등록  github blog를 google에서 검색되도록 설정하기   ","categories": ["Blog"],
        "tags": ["Blog","SEO","Github blog","blog custom"],
        "url": "/blog/SEO-set/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[Github Blog] google analytics 적용하기",
        "excerpt":"github blog에 google analytics를 적용해보겠습니다.  저는 github blog에 jekyll minimal-mistakes 테마를 적용하고 있습니다.   1. google analytics 계정 생성하기  우선 google analytics 페이지에 접속해서 계정을 생성합니다.                google analytics 페이지에서 측정 시작을 눌러줍니다.                계정 이름을 입력하고 다음으로 넘어갑니다.                속성 이름을 입력하고, 보고 시간대와 통화를 지정해줍니다.    저는 속성 이름에 Github blog를, 시간대와 통화를 모두 대한민국으로 지정했습니다.                 비즈니스 정보를 입력합니다. 업종은 지정하지 않아도 됩니다.    비즈니스 규모와 사용 계획은 본인의 사이트와 맞게 입력해줍니다.                마지막으로 만들기를 누르고, 약관에 동의하면 계정 만들기가 완료 되었습니다.   2. 데이터 스트림 설정하기  이제 생성한 GA 계정을 이용하여 데이터 스트림을 설정하겠습니다.        다음 화면에서 우리는 github blog에 만들 것이기 때문에 웹을 선택해줍니다.       데이터 스트림 설정 창이 뜨면, http:// 를 제외한 깃헙 url을 입력한 후, 스트림 이름을 지정해줍니다.  저는 github blog로 지정하였습니다.  그리고 만들기를 눌러 생성합니다.        스트림을 만든 후, 웹 스트림 세부정보에서 측정 ID를 복사합니다.  이렇게 복사한 측정 ID는 github blog 로컬에 위치해있는 _config.yml 파일 속 google tracking id 적는 란에 다음과 같이 추가해줍니다.          참고한 포스트 Jekyll Chripy Google Analytics 연동하기   ","categories": ["Blog"],
        "tags": ["Blog","google analytics","Github blog","blog custom"],
        "url": "/blog/GA-post/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[Github Blog] 깃헙 블로그에 giscus 적용하기",
        "excerpt":"github blog에 giscus를 사용하여 댓글창을 만들어보겠습니다.  보통 깃헙 블로그 댓글창으로는 disqus를 많이 사용하지만, 광고가 많고 무겁다는 평이 많아 비교적 가벼운 giscus를 적용해보려고 합니다.   1. giscus 설치하기  다음 giscus 사이트에 접속합니다.        다음 giscus를 눌러 giscus 댓글창을 사용할 레포지토리에 install 합니다.        저는 이미 설치했기 때문에 configure이라고 뜨지만, 파란 install 버튼을 눌러줍니다.  install 버튼을 누르면, all repositories와 only select repositories 둘 중 선택하는 옵션이 있는데, only select repositories &gt; 댓글창 적용할 repositories선택하여 설치합니다.   2. repository discussion 기능 활성화  이제 giscus를 다운받은 repo의 discussion 기능을 활성화 시켜줘야 합니다.        giscus를 다운받은 repo의 settings로 들어갑니다.        스크롤을 내려 Features/Discussions 를 활성화 시켜줍니다.   3. giscus 코드 copy하기        이제 다시 giscus 사이트로 돌아가 언어를 지정하고, giscus를 설치하고 discussions 기능을 활성화해준 repo의 이름을 적어줍니다.  지금까지 정상적으로 진행되었다면 repo 입력란 주위에 통과했다는 메세지가 뜹니다.  그리고 페이지와 discussion 간의 연결을 선택해줍니다.  보통 title과 url보다 바뀔 가능성이 적은 경로로 연결하기 때문에 저도 그렇게 지정했습니다.        이제 discussion 카테고리를 지정해주고, 본인이 원하는 기능을 선택한 후에 쓰고있는 테마를 선택합니다.  그럼 이제 giscus 활성화 아래 코드가 나타나는데, 코드를 copy하고 _layouts/posts.html 파일 마지막 부분에 붙여넣습니다.       4. config 파일에서 적용하기  제가 사용하는 minimal-mistakes 테마가 giscus를 지원하기 때문에 블로그의 _config.yml 파일에 comments를 지정하는 곳에 “giscus”를 입력하고 아래 giscus창에 앞선 코드를 참고하여 붙여넣어줍니다.        보통 이 정도의 과정을 거치면 댓글창이 적용이 되지만, 확인해보니 아직 적용이 되지 않은 것을 볼 수 있습니다.        이럴 때는 다음과 같이 각 포스트의 title 과 tag 등을 지정하는 곳에 comments : \"giscus\"를 지정해주고 push 해줍니다.            그리고 다시 확인해보면 위와 같이 comments 창이 제대로 뜨는 것을 확인할 수 있습니다.  ","categories": ["Blog"],
        "tags": ["Blog","giscus","Github blog","blog custom"],
        "url": "/blog/coments/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(1)",
        "excerpt":"지도학습 알고리즘은 분류와 회귀로 나눌 수 있습니다.  분류는 앞서 2장에서 했던 것처럼 클래스 중 하나로 분류하는 것이고, 회귀는 임의의 어떤 숫자를 예측하는 것입니다.   k-최근접 이웃 분류는 앞서 진행했고, 이번에는 k-최근접 이웃 회귀를 알아보겠습니다.  k-최근접 이웃 회귀는 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하여 이웃 수치들의 평균을 구하는 방식입니다.     회귀분석에 쓰일 데이터를 불러온 후 산점도를 그려보겠습니다.   #numpy import import numpy as np  #Data load perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,        21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,        23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,        27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,        39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,        44.0]) perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,        115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,        150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,        218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,        556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,        850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,        1000.0])  #산점도 그리기 import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel('length') plt.ylabel('weight') plt.show()      다음과 같이 우상향하는 그래프가 출력되었습니다.  농어 길이가 늘어날 수록 무게도 늘어나는 것은 확인했습니다.  다음으로 데이터를 훈련 세트와 테스트 세트로 나눕니다.   from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(     perch_length, perch_weight, random_state = 42 )  train_test_split 메서드는 총 데이터의 25%를 test 데이터로 떼어냅니다.  random_state는 책과 동일한 결과가 나올 수 있게 한 것이기 때문에 생략해도 무방합니다.  사이킷런을 사용해 모델을 훈련할 것이기 때문에 reshape을 사용하여 배열을 2차원으로 바꿔줍니다.   train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) print(train_input.shape, test_input.shape)     (42, 1) (14, 1)    reshape의 첫 번째 인자를 -1로 지정함으로써 원소의 개수로 채우라는 의미입니다. 배열의 전체 원소의 개수를 외우지 않아도 되기 때문에 편리합니다.  이제 k-최근접 이웃 알고리즘을 훈련시켜보도록 하겠습니다. 사이킷런에서 KNeighborsRegressor을 사용하여 회귀 모델을 fit 합니다.   from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor()  #k-최근접 이웃 회귀모델을 훈련합니다. knr.fit(train_input, train_target) print(knr.score(test_input, test_target))     0.992809406101064    거의 1에 가까운 숫자가 나왔습니다.  2장에서 분류를 할 때는 정확도라고 불렀던 이 숫자를 회귀에서는 결정계수라고 부릅니다. 결정계수가 1에 가까워지면 예측이 타깃과 비슷하다는 의미가 됩니다.  아제 타깃과 예측한 값의 차이를 구해보겠습니다. sklearn.metrics 패키지의 mean_absolute_error을 이용하여 둘 사이의 절댓값 오차를 평균을 낸 값을 출력합니다.  그리고 훈련 세트를 사용하여 score값도 출력해보도록 하겠습니다.   from sklearn.metrics import mean_absolute_error  #테스트 세트에 대한 예측 test_prediction = knr.predict(test_input)  #테스트 세트의 평균 절댓값 오차 계산 mae = mean_absolute_error(test_target, test_prediction) print(mae)  print(knr.score(train_input, train_target))     19.157142857142862  0.9698823289099254    위 코드를 출력하면 평균적으로 오차가 19g정도 났다는 것을 확인할 수 있습니다.  또한 훈련을 한 세트로 score을 냈는데도 불구하고 오히려 test set보다 결정계수가 더 낮게 나온 것을 확인할 수 있습니다.  이처럼 훈련 세트보다 테스트 세트가 점수가 높거나 두 점수 모두 낮은 경우를 과소적합이라고 하고, 반대로 너무 차이나게 훈련 세트가 점수가 높을 경우를 과대적합이라고 합니다. 이번 훈련에서는 과소적합이 된 것입니다.  그렇다면 모델을 더 복잡하게 만듦으로써 이를 해결해보도록 하겠습니다.  모델을 복잡하게 만들기 위해 k를 3으로 줄입니다.   # 이웃의 개수를 3으로 설정 knr.n_neighbors = 3  # 모델 훈련 knr.fit(train_input, train_target) print(knr.score(train_input, train_target))     0.9804899950518966    k값을 낮췄더니 결정계수의 값이 올라갔습니다. 이번엔 test 세트를 적용해봅니다.   print(knr.score(test_input, test_target))     0.9746459963987609    test set 또한 너무 낮지 않은 점수로 잘 나왔습니다.  train set과 점수 차이도 많이 나지 않아 과대적합도 아닙니다. 이처럼 과소적합 시 모델을 더 복잡하게 (k값을 줄여서) 만들어야 하고, 과대적합 시 모델을 덜 복잡하게 (k값을 늘려서) 만듦으로써 해결할 수 있습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG3-1/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(2)",
        "excerpt":"앞서 만든 k-최근접 이웃 회귀 모델로 길이 50cm 농어의 무게를 예측해보도록 하겠습니다.   #Data load import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,        21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,        23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,        27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,        39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,        44.0]) perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,        115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,        150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,        218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,        556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,        850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,        1000.0])  from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(     perch_length, perch_weight, random_state = 42 )  train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1)  from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor(n_neighbors = 3)  #k-최근접 이웃 회귀모델을 훈련합니다. knr.fit(train_input, train_target)  #모델을 사용하여 길이가 50cm인 농어의 무게를 예측합니다. print(knr.predict([[50]]))     [1033.33333333]    최근접 이웃 회귀 모델은 이 농어의 무게를 약 1033g 정도로 예측했지만, 실제 값은 더 많이 나간다고 합니다. 이를 해결하기 위해 우선 산점도에 표시해 자료를 확인해보도록 하겠습니다.   import matplotlib.pyplot as plt  #타깃 농어 이웃 구하기 distances, indexes = knr.kneighbors([[50]])  #훈련 세트 산점도 그리기 plt.scatter(train_input, train_target)  #훈련 세트 중 이웃 샘플만 다시 그리기 plt.scatter(train_input[indexes], train_target[indexes], marker = 'D')  #타깃 그리기 plt.scatter(50, 1033, marker = '^')  plt.xlabel('length') plt.ylabel('weight') plt.show()      산점도를 보면 어떤 문제인지 알 수 있습니다.  샘플에 비해 이웃 샘플들의 평균한 무게가 너무 적었던 것입니다.  이웃 샘플의 평균을 구하면 수치상으로도 알수 있습니다.   print(np.mean(train_target[indexes]))     1033.3333333333333    예측한 수치와 일치합니다. 길이가 100cm인 농어도 똑같은 값으로 예측합니다. 이를 해결하기 위해 다른 모델을 적용하고자 합니다.  선형회귀 또한 사이킷런의 LinearRegression 클래스를 이용하여 훈련할 수 있습니다.   from sklearn.linear_model import LinearRegression lr = LinearRegression()  # 모델 훈련 lr.fit(train_input, train_target)  # 50cm 농어 예측 print(lr.predict([[50]]))     [1241.83860323]    최근접 이웃 회귀모델이 예측한 값보다 훨씬 높은 값을 예측했습니다.  훈련세트의 산점도와 함께 농어의 길이에 따른 무게를 직선으로 그려보겠습니다.   # 훈련세트 산점도 plt.scatter(train_input, train_target)  # 15에서 50까지의 1차 방정식 그래프 그리기 plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])  # 타깃 데이터 그리기 plt.scatter(50, 1241.8, marker = '^')  plt.xlabel('length') plt.ylabel('weight') plt.show()    이제 결정계수를 출력해보겠습니다.   print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target))     0.939846333997604  0.8247503123313558    점수를 본 결과 전체적으로 높지 않습니다. 또한 그래프에서도 산점도는 곡선의 형태를 띄기 때문에 직선은 데이터를 잘 대변하지 못하는 것 같습니다.  최적의 곡선을 찾기 위해 길이를 제곱한 항을 훈련 세트에 추가해보겠습니다.  농어의 길이를 제곱하여 원래 데이터 앞에 붙입니다.   train_poly = np.column_stack((train_input**2, train_input)) test_poly = np.column_stack((test_input**2, test_input))  #새 데이터셋 크기 확인하기 print(train_poly.shape, test_poly.shape)     (42, 2) (14, 2)    이제 train_poly() 함수를 사용하여 선형 회귀 모델을 다시 훈련합니다.  여기서 주의할 점은 2차 방정식 그래프를 찾기 위해 데이터에는 제곱항을 추가했지만 타깃 데이터는 그대로 사용한다는 것입니다.  이 훈련세트로 모델을 훈련한 후 50cm 농어의 무게를 예측해보겠습니다.   lr= LinearRegression() lr.fit(train_poly, train_target)  print(lr.predict([[50**2, 50]]))     [1573.98423528]    앞서 훈련한 값보다 높은 값을 출력했습니다.  이 모델이 훈련한 계수와 절편을 확인해봅니다.   print(lr.coef_, lr.intercept_)     [1.01433211 -21.55792498] 116.0502107827827    이 모델은  무게 = 1.01 * 길이^2 - 21.6 * 길이 + 116.05   다음과 같은 그래프를 학습한 것으로 생각할 수 있습니다.  이러한 다항식을 사용한 선형 회귀를 다항회귀라고 부릅니다.  이제 산점도를 다시 그려보겠습니다.  다항식을 표현할 곡선은 직선을 짧게 이어 그려 표현할 수 있습니다.   #구간별 직선을 그리기 위하여 15부터 49까지 정수 배열 만들기 point = np.arange(15, 50)  #훈련세트 산점도 plt.scatter(train_input, train_target)  #15부터 49까지 2차 방정식 그래프 그리기 plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)  #50cm 농어 데이터 plt.scatter(50, 1574, marker = '^')  plt.xlabel('length') plt.ylabel('weight') plt.show()      앞서 만들었던 선형 회귀 모형보다 더 나은 그래프가 그려진 것이 보입니다.  이제 결정계수를 출력해보겠습니다.   print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target))     0.9706807451768623  0.9775935108325122    두 점수 모두 좋은 점수가 나왔습니다.  다만 train 보다 test 점수가 더 높은 것을 보니 과소적합이 조금 남아있는 것 같습니다.  더 복잡한 모델이 필요한 것 같으니 다음 시간에 알아보도록 하겠습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG3-2/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 3. 회귀 알고리즘과 모델 규제(3)",
        "excerpt":"앞서 훈련했던 모델에서 훈련 세트보다 테스트 세트가 더 점수가 높게 나왔습니다.  이 문제를 해결하기 위해 농어의 길이 뿐만 아니라 농어의 높이와 두께 등의 여러 특성들을 추가로 사용해보겠습니다.  또한 이전에 사용했던 방법인 각 항을 제곱하여 데이터에 추가하는 것과 각 특성을 서로 곱해 새로운 특성을 만드는 방식을 사용합니다.   # data load import pandas as pd df = pd.read_csv('https://bit.ly/perch_csv') perch_full = df.to_numpy() print(perch_full)     [[ 8.4   2.11  1.41]   [13.7   3.53  2.  ]   [15.    3.82  2.43]   [16.2   4.59  2.63]   [17.4   4.59  2.94]   [18.    5.22  3.32]  …    이번에는 데이터가 여러 특성이 있음을 고려하여 pandas를 이용해 직접 csv 데이터를 불러오는 방식을 사용했습니다.  이제 이전과 동일한 방식으로 타깃 데이터를 준비하고, perch_full과 perch_weight를 훈련 세트와 테스트 세트로 나눕니다.   import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,        115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,        150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,        218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,        556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,        850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,        1000.0])  from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42)   사이킷런의 sklearn.preprocessing 패키지의 PolynomialFeatures 클래스를 사용해서 새로운 특성을 만들어 보겠습니다.  PolynomialFeatures 클래스는 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가합니다.  이때 기본적으로 1을 추가하여 각 특성에 곱함으로써 절편을 만드는데, 사이킷런 선형 모델은 자동으로 절편이 들어가있기 때문에 include_bias=False 인자를 추가함으로서 절편을 위한 항을 제거할 수 있습니다.   from sklearn.preprocessing import PolynomialFeatures  # train_input을 변환하여 train_poly 만들기 poly = PolynomialFeatures(include_bias=False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape)     (42, 9)    PolynomialFeatures 클래스는 9개의 특성이 어떻게 반영되어있는지 확인하는 방법을 제공합니다.  get_feature_names() 메서드를 사용하여 알 수 있습니다.   poly.get_feature_names()     [‘x0’, ‘x1’, ‘x2’, ‘x0^2’, ‘x0 x1’, ‘x0 x2’, ‘x1^2’, ‘x1 x2’, ‘x2^2’]    #test set 변환하기 test_poly = poly.transform(test_input)  #다중회귀 모델 훈련하기 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target))     0.9903183436982124    train set을 이용하여 점수를 내보았으니 test set도 점수를 출력해보겠습니다.   #test set 점수도 확인하기 print(lr.score(test_poly, test_target))     0.9714559911594134    테스트 세트 점수와 훈련세트 점수 모두 높게 나오면서 train 세트가 더 점수가 높게 나온 것을 확인할 수 있습니다.  이제 특성을 더 많이 추가해보겠습니다.  PolynomialFeatures 클래스의 degree 변수를 사용하면 고차항의 최대 차수를 지정할 수 있습니다.   poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape)     (42, 55)    만들어진 특성의 개수가 55개인 것을 확인할 수 있습니다.  train_poly 배열의 열의 개수가 특성의 개수라고 생각하면 됩니다.  이제 transform을 마쳤으니 이 데이터로 다시 훈련해보도록 하겠습니다.   lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target))  print(lr.score(test_poly, test_target))     0.9999999999991097  -144.40579242684848    결과를 보면 train set에 대한 점수는 매우 높게 나오고, test set에 대한 점수는 음수가 나왔습니다.  이는 특성의 개수를 크게 늘렸기 때문에 train set에 대해 과대적합된 결과로 볼 수 있습니다.  이러한 과대적합을 줄이기 위해서는 선형 회귀 모델의 계수를 규제하여 해결해보겠습니다.   from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly)   선형 회귀 모델에 규제를 추가한 모델을 ridge와 lasso라고 부르는데, 릿지는 계수를 제곱한 값을 기준으로 규제를 적용하고, 라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.  일반적으로는 릿지를 조금 더 선호하기 때문에 릿지 회귀를 먼저 훈련해보도록 하겠습니다.   from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target))  print(ridge.score(test_scaled, test_target))     0.9896101671037343  0.9790693977615397    ridge 모델을 사용하여 훈련한 결과 많은 특성을 사용했음에도 불구하고 훈련세트, 테스트 세트 모두에서 좋은 점수가 나왔습니다.  릿지와 라쏘 모델을 적용할 때 모델 객체 중 alpha 값을 사용함으로써 규제의 정도를 정할 수 있는데, alpha값이 크면 규제의 강도가 세기 때문에 과소적합될 가능성이 크고, 작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지기 때문에 과대적합될 가능성이 큽니다.  적절한 alpha 값을 찾기 위해서는 alpha 값에 대한 결정계수 값을 그려 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점을 최적의 alpha 값으로 생각하면 됩니다.  그럼 alpha값을 바꿀 때 마다 score 값을 저장할 리스트를 먼저 만들어주겠습니다.   import matplotlib.pyplot as plt train_score = [] test_score = []  #alpha 값 리스트 생성 alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]  for alpha in alpha_list :   #ridge model 생성   ridge = Ridge(alpha=alpha)   #ridge model 훈련   ridge.fit(train_scaled, train_target)   #훈련 점수와 테스트 점수 저장   train_score.append(ridge.score(train_scaled, train_target))   test_score.append(ridge.score(test_scaled, test_target))  이제 alpha 값에 따른 그래프를 그려 값을 확인해보도록 하겠습니다.  그냥 그리면 그래프 왼쪽이 너무 촘촘해지기 때문에 로그함수로 바꿔 표현합니다.   plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel('alpha') plt.ylabel('R^2') plt.show()      다음 그래프를 확인해보면, 적절한 alpha 값이 -1, 즉 0.1 이라는 것을 알 수 있습니다.  alpha 값을 0.1로 지정하여 모델을 훈련하겠습니다.   ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target))     0.9903815817570366  0.9827976465386926    train값이 test값보다 높게 나오면서 두 점수 모두 적절한 점수가 나온것을 확인할 수 있습니다.  이제 lasso 회귀모델을 훈련해보도록 하겠습니다.   train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list:   # lasso model을 만들기   lasso = Lasso(alpha=alpha, max_iter = 10000)   # lasso model 훈련하기   lasso.fit(train_scaled, train_target)   # train set와 test set score을 저장   train_score.append(lasso.score(train_scaled, train_target))   test_score.append(lasso.score(test_scaled, test_target))  #x축을 로그스케일로 바꿔 그래프 그리기 plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel('alpha') plt.ylabel('R^2') plt.show()      위 그래프를 확인했을 때 올바른 alpha값은 1이라는것을 알 수 있습니다.  이 값으로 lasso 모델을 훈련합니다.   lasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target))     0.9888067471131867  0.9824470598706695    lasso 모델은 계수값을 0으로 만들 수 있습니다.  라쏘모델의 계수중 0인것의 개수를 출력해보겠습니다.   print(np.sum(lasso.coef_ == 0))     40    계수가 0인 항이 40개인 것을 확인할 수 있습니다.  총 55개의 특성 중 15개만 사용한 것입니다.  이런 특성이 있기 때문에 라쏘 모델을 유용한 특성을 골라내는 데에 사용하기도 합니다.  릿지와 라쏘 회귀를 사용하여 최적의 alpha 값을 찾아보고 특성이 많은 데이터를 규제하여 모델의 성능을 확인해봤습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG3-3/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 4. 다양한 분류 알고리즘",
        "excerpt":"로지스틱 회귀   랜덤하게 담긴 생선의 확률을 알아보려고 합니다.  우선 앞서 배웠던 k-최근접 이웃 분류기를 사용하여 구한 이웃 클래스를 토대로 타깃 생선의 확률을 계산해보겠습니다.  먼저 데이터를 준비합니다.   import pandas as pd fish = pd.read_csv('https://bit.ly/fish_csv') fish.head()      그리고 target 데이터가 될 Species 열에 어떤 종류가 있는지 판다스의 unique 함수를 사용해서 확인합니다.   print(pd.unique(fish['Species']))     [‘Bream’ ‘Roach’ ‘Whitefish’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Smelt’]    Species 에 들어있는 종들을 확인했으니 fish 데이터에서 Species 열을 제외하고 나머지 5개의 열을 입력 데이터로 선택합니다.   fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy() print(fish_input[:5])     [[242.      25.4     30.      11.52     4.02  ]   [290.      26.3     31.2     12.48     4.3056]   [340.      26.5     31.1     12.3778   4.6961]   [363.      29.      33.5     12.73     4.4555]   [430.      29.      34.      12.444    5.134 ]]    입력 데이터가 잘 생성된 것을 확인했습니다.   동일한 방식으로 target 데이터도 만듭니다.   fish_target = fish['Species'].to_numpy()  이제 train set와 test set로 나눕니다.   from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target)  train 세트와 test를 준비했으니 데이터를 표준화 전처리를 해줍니다.  사이킷런의 StandardScaler을 이용합니다.   #훈련 테스트 세트 표준화 전처리 from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input)  사이킷런의 KNeighborsClassifier 클래스 객체를 만들고 훈련세트와 테스트세트의 각 점수를 확인해보겠습니다.  최근접 이웃의 개수인 k는 3으로 지정합니다.   #최근접 이웃 분류기의 확률 예측 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors=3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target))     0.8739495798319328  0.75    print(kn.classes_)     [‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]    알파벳 순서로 매겨진 것을 확인했습니다.  이제 predict 메서드를 사용하여 test set에 있는 처음 5개의 샘플의 타깃값을 예측합니다.   print(kn.predict(test_scaled[:5]))     [‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]    predict 예측 값이 어떻게 나왔는지 predict_proba() 메서드를 사용하면 클래스별 확률 값을 확인할 수 있습니다.   import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals=4))  #소숫점 넷째자리까지 표기(다섯번째에서 반올림)     [[0.     0.6667 0.3333 0.     0.     0.     0.    ]   [0.     0.     1.     0.     0.     0.     0.    ]   [0.     0.     0.3333 0.     0.6667 0.     0.    ]   [0.     0.     1.     0.     0.     0.     0.    ]   [0.     0.     0.3333 0.     0.3333 0.3333 0.    ]]    첫번째 열이 Bream, 두번째 열이 Parkki, 세번째 열이 Perch, … 에 대한 확률을 나타내고 있습니다.   계산한 비율이 올바른지 확인하기 위해 첫 번째 샘플의 최근접 이웃들을 출력해보겠습니다.   distances, indexes = kn.kneighbors(test_scaled[0:1]) print(train_target[indexes])     [[‘Parkki’ ‘Parkki’ ‘Perch’]]    첫 번째 샘플의 이웃은 Perch가 1개, Parkki가 2개임을 확인합니다.  여기까지 k-최근접 이웃 분류기를 사용한 확률 예측을 해보았습니다.    이제 본격적으로 로지스틱 회귀를 이용한 예측을 해보도록 하겠습니다.   로지스틱 회귀는 선형회귀와 동일하게 선형 방정식을 학습합니다.      여기서 각 변수 앞에 곱해진 값들은 가중치나 계수의 역할을 합니다.  z는 어떤 값도 가능하지만, 확률이 되기 위해서는 0과 1 사이의 값을 가져야 합니다.  z가 큰 음수일때 0이 되고 큰 양수일 때 1이 되도록 하려면 시그모이드 함수를 사용하면 됩니다.            시그모이드 함수는 다음과 같은 형태를 띕니다.  넘파이를 사용해서 그래프를 그려보겠습니다.   import matplotlib.pyplot as plt z = np.arange(-5, 5, 0.1) phi = 1 / (1 + np.exp(-z)) plt.plot(z, phi) plt.xlabel('z') plt.ylabel('phi') plt.show()        이제 로지스틱 회귀를 사용하여 이진분류를 해보겠습니다.  train 데이터에서 불리언 인덱싱을 사용하여 도미와 빙어만 따로 출력해줍니다.   #도미(Bream)와 빙어(Smelt)만 출력하기 bream_smelt_indexes = (train_target == 'Bream')|(train_target == 'Smelt') print(bream_smelt_indexes)     [False  True False False False False  True False False  True False  True   True False False False  True  True False  True False False  True False  False False  True False False False False False False False  True False   True  True False  True False False  True False False  True False  True  False False  True  True False False False  True  True False False False   True  True False False False False False False False  True False  True  False False  True False False False False  True False False False False   True False  True False False False False False False False  True  True  False False False False  True  True False False False False False False  False False False False  True  True  True False False  True False]    train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes]  이 데이터를 이용하여 로지스틱 회귀 모델을 훈련하고 그 모델을 사용하여 train_bream_smelt에 있는 처음 5개의 샘플을 출력해보겠습니다.   from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_bream_smelt, target_bream_smelt)  print(lr.predict(train_bream_smelt[:5]))     [‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Smelt’ ‘Bream’]    다섯 번째를 도미, 나머지를 빙어로 예측했습니다.  5개 샘플의 확률을 확인합니다.   print(lr.predict_proba(train_bream_smelt[:5]))     [[0.03391154 0.96608846]   [0.03603855 0.96396145]   [0.03582089 0.96417911]   [0.02900509 0.97099491]   [0.99410165 0.00589835]]    두 개의 열 중에 어떤 것이 도미이고 빙어인지 헷갈린다면 classes_ 를 사용하여 알 수 있습니다.   print(lr.classes_)     [‘Bream’ ‘Smelt’]    이제 로지스틱 회귀가 학습한 계수를 확인합니다.  print(lr.coef_, lr.intercept_)     [[-0.42646881 -0.60256452 -0.68252074 -0.99456193 -0.78263044]] [-2.28791769]    이를 통해 로지스틱 회귀 모델이 학습한 방정식을 다음과 같이 도출할 수 있습니다.      이제 이 방정식을 이용하여 z 값을 계산해보겠습니다.   decisions = lr.decision_function(train_bream_smelt[:5]) print(decisions)     [ 3.34950001  3.28646197  3.29274597  3.51084984 -5.12716733]    이 z값을 시그모이드 함수에 통과하면 확률을 얻을 수 있습니다.  scipy에 시그모이드 함수를 제공하는 expit()를 사용해서 확률을 구합니다.   from scipy.special import expit print(expit(decisions))     [0.96608846 0.96396145 0.96417911 0.97099491 0.00589835]    이제 다중 분류를 수행해보겠습니다.   lr = LogisticRegression(C = 20, max_iter = 1000) lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target))     0.9411764705882353  0.85    test 세트의 처음 5개 샘플에 대한 예측을 출력해봅니다.  그리고 그 예측에 대한 확률을 출력해보겠습니다.   #test set의 처음 5개 샘플에 대한 예측 print(lr.predict(test_scaled[:5]))     [‘Parkki’ ‘Perch’ ‘Roach’ ‘Perch’ ‘Perch’]    #test set의 처음 5개에 대한 예측 확률 proba = lr.predict_proba(test_scaled[:5]) print(np.round(proba, decimals=3)) #소수점 세번째자리까지 표기     [[0.01  0.73  0.006 0.    0.223 0.    0.03 ]   [0.01  0.    0.898 0.002 0.    0.    0.089]   [0.001 0.018 0.28  0.002 0.661 0.    0.038]   [0.002 0.    0.953 0.001 0.    0.    0.044]   [0.    0.012 0.816 0.    0.119 0.051 0.002]]    5개 샘플과 7개의 클래스가 있기 때문에 5개의 행과 7개의 열로 출력이 되었습니다.  각 열이 의미하는 것이 어떤 생선에 대한 확률인지 확인합니다.   #classes_속성으로 클래스 정보 확인 print(lr.classes_)     [‘Bream’ ‘Parkki’ ‘Perch’ ‘Pike’ ‘Roach’ ‘Smelt’ ‘Whitefish’]    print(lr.coef_.shape, lr.intercept_.shape)     (7, 5) (7,)    coef_ 배열이 7행에 5열의 형태를 띄고 있고 intercept_도 7개가 있는 것을 확인할 수 있습니다.  7개의 클래스가 있기 때문에 클래스마다 z값을 계산하여 총 7개의 z값이 나온다는 말과 같습니다.   이와 같이 7개의 z값들을 확률로 바꾸기 위해 소프트맥스 함수를 사용합니다.      7개의 z값을 소프트맥스 함수를 이용하여 확률로 바꾸면 0과 1사이의 값을 가진 확률이 출력됩니다.  그럼 7개의 z값을 구한 후에 소프트맥스 함수를 사용하여 확률로 바꾸어보겠습니다.   #z1부터 z7까지의 값 구하고 확률로 바꾸기 decision = lr.decision_function(test_scaled[:5]) print(np.round(decision, decimals=2)) #소수점 둘째자리까지 표기     [[ -0.17   4.13  -0.67  -3.44   2.95  -3.75   0.96]   [  4.31  -5.68   8.79   2.78   0.69 -17.37   6.48]   [ -2.69   0.28   3.04  -2.01   3.9   -3.56   1.05]   [  3.78  -6.28   9.85   2.82   0.69 -17.63   6.78]   [ -8.63   1.35   5.59  -4.1    3.66   2.82  -0.7 ]]    from scipy.special import softmax proba = softmax(decision, axis = 1) print(np.round(proba, decimals = 3)) #소수점 셋째자리까지 표시     [[0.01  0.73  0.006 0.    0.223 0.    0.03 ]   [0.01  0.    0.898 0.002 0.    0.    0.089]   [0.001 0.018 0.28  0.002 0.661 0.    0.038]   [0.002 0.    0.953 0.001 0.    0.    0.044]   [0.    0.012 0.816 0.    0.119 0.051 0.002]]    이렇게 로지스틱 회귀를 사용하여 7개의 클래스에 대한 확률을 예측하는 모델을 만들었습니다.  다음에는 확률적 경사 하강법에 대해 배워보도록 하겠습니다.   ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG4-1/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[Github] 깃허브 메인 페이지에 잔디심기 띄우기",
        "excerpt":"오늘은 Github Actions를 사용하여 잔디심기 대시보드를 다음과 같이 만들어보겠습니다.  참고한 포스트는 게시글 하단에 작성해 두었습니다.      1. 잔디심기 대시보드 만들 레포지토리 선택해서 권한 설정  잔디심기 대시보드를 표시할 레포지토리를 정합니다. 저는 깃허브 프로필 페이지에 띄울것이기 때문에 고유 아이디로 된 yhp2205 레포지토리로 결정했습니다.       레포지토리를 정했으면 그 레포지토리의 권한을 설정하기 위해 settings를 눌러 들어갑니다.       settings의 좌측 카테고리에서 Actions - General로 들어가서       페이지 하단에 위치한 Workflow permissions를 다음과 같이 설정해줍니다.   2. metrics 기능 fork 해서 가져오기  다음 사이트에 접속하여 metrics 기능을 fork 해서 가져오겠습니다.       이런 페이지가 뜨면 우측 상단의 fork 버튼을 누르고       Create fork를 눌러 본인의 깃허브 계정으로 가져옵니다.   3. personal token 생성하기  metrics를 가져왔으니 계정의 token을 생성해보겠습니다.        다음과 같이 계정의 settings로 들어갑니다. (레포지토리의 settings가 아닌 깃허브 계정의 settings로 들어갑니다)       우측 카테고리 제일 하단의 Developer settings를 누릅니다.       다음과 같이 personal access token - Generate new token을 눌러줍니다.             note에 token의 이름을 지정해주고, 유효기간을 설정합니다. 원하는 대로 설정하면 됩니다. 그러나 토큰의 유효기간을 짧게 설정할 경우, 그 주기마다 토큰을 다시 만들어 업데이트 해줘야하기 때문에 기한 없이 설정하는 것을 권장합니다. 그리고 아래 repo, read::packages, read:org, gist, read:user를 선택한 후에 토큰을 만들어 줍니다.       토큰을 성공적으로 생성했을 때 다음과 같은 페이지가 뜨고 빨간 박스로 표시한 아이콘을 눌러 토큰을 복사합니다. 잃어버리면 안되기 때문에 다른 메모장에 적어두셔도 좋습니다!   4. Actions secrets에 token 등록하기  토큰을 만들어 잘 복사해두었다면 다음으로 잔디심기를 올릴 레포지토리에 토큰을 등록해보겠습니다.          잔디심기 대시보드를 만들 레포지토리에 들어갑니다. 1에서 권한을 설정해 주었던 레포지토리의 settings - secrets - actions를 차례로 눌러 들어가줍니다.       New repository secret을 눌러 토큰을 등록하겠습니다.       다음과 같은 창이 뜨면 토큰의 이름을 작성하고(저는 YOUNGHYUNS_METRICS_TOKEN으로 등록하였습니다), Value 값에 복사해둔 token값을 넣고 Add secret 해줍니다.   5. repo 내에 yml 파일 만들기   깃허브 토큰 등록한 repo 내에 yml 파일을 만들겠습니다.       Actions - set up a workflow yourself를 누릅니다.   # This is a basic workflow to help you get started with Actions  name: CI  # Controls when the workflow will run on:   # 하루에 한 번 씩 빌드 수행   schedule:     - cron: '0 1 * * *'    # Allows you to run this workflow manually from the Actions tab   workflow_dispatch:  # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs:   # This workflow contains a single job called \"build\"   build:     # The type of runner that the job will run on     runs-on: ubuntu-latest      # Steps represent a sequence of tasks that will be executed as part of the job     steps:       - name: [빌드를 수행할 때 표시할 작업명]         uses: [metrics를 fork한 저장소명]/metrics@latest         with:           token: $           filename: [빌드한 후 생성할 파일명].svg           base: \"\"           plugin_isocalendar: yes           plugin_isocalendar_duration: full-year  다음 코드를 복사해서 붙여넣습니다.       다음과 같이 activity_metrics_build.yml로 이름을 지정해주고 앞선 코드에서 표시된 부분을 본인의 repo에 맞게 작성합니다. 하늘색 박스로 표시된 부분에는 전에 등록해준 토큰의 이름을 작성하면 됩니다. 저는 YOUNGHYUNS_METRICS_TOKEN 으로 지정했기 때문에 그대로 작성하였습니다.    작성이 완료되었다면 우측 상단에 위치한 Start commit 버튼을 눌러 Commit new file을 해줍니다.   6. Workflow 실행하여 svg 파일 생성하기      commit을 완료하여 yml 파일을 생성했다면 다시 Actions 탭으로 들어갑니다. 그럼 다음과 같이 CI라는 이름의 workflow가 생성된 것을 확인할 수 있습니다. CI를 선택한 후에, 빨간색으로 표시되어 있는 Run Workflow를 눌러 workflow를 실행합니다.    CI가 실행되는 데에는 약 5분정도의 시간이 걸립니다. 주황색 불이 들어와서 실행중임을 알리고 있다면 빌드가 완료될 때까지 기다립니다.      시간이 지난 후, 초록 표시가 들어오고 빌드가 완료된 것을 알리면, 레포지토리 내에 다음과 같이 svg 파일이 생성된 것을 볼 수 있습니다.   7. 잔디심기 대시보드 게시하기      마지막으로 동일 레포지토리의 Readme.md 파일에 다음과 같이 작성해줍니다.       정상적으로 실행되었을 때, 다음과 같이 깃허브 메인 화면에서 잔디심기 보드를 확인할 수 있습니다.      참고한 포스트 Github Actions를 이용한 잔디심기 대쉬보드 만들기   ","categories": ["Github"],
        "tags": ["Github","Github actions"],
        "url": "/github/GitAction/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 4. 다양한 분류 알고리즘(2)",
        "excerpt":"확률적 경사 하강법  확률적 경사 하강법이란 Stochastic Gradient Descent로써, 무작위로 배치 크기가 1인 데이터를 추출하여 기울기를 계산하고 경사 하강 알고리즘을 적용하는 방법을 말합니다. 랜덤하게 하나의 샘플을 택하여 하강 알고리즘을 적용하기 때문에 전체 샘플을 사용하지 않습니다. 이러한 형태로 전체 샘플을 모두 사용할 때 까지 알고리즘을 여러번 적용한다고 생각하면 됩니다.      우선 pandas를 사용하여 데이터를 불러오겠습니다.   #dataframe 생성 import pandas as pd fish = pd.read_csv('https://bit.ly/fish_csv')  print(fish)      우선 데이터를 불러온 후에 Species 열을 제외한 열들을 입력 데이터로, Species 열을 타깃 데이터롤 설정합니다.   fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy() fish_target = fish['Species'].to_numpy()  #train set와 test set으로 나누기 from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(         fish_input, fish_target, random_state = 42)  fish 데이터를 sklearn을 이용하여 train set와 test set로 나눠줍니다.  그리고 StandardScaler을 사용하여 train set와 test set를 표준화 전처리를 해주도록 하겠습니다.   from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input)  이제 확률적 경사 하강법을 사용하기 위해 sklearn의 SGDClassifier 클래스를 이용합니다. SGDClassifier 클래스에는 loss 변수에 손실 함수의 종류를 지정해주고, max_iter 변수에는 수행할 에포크의 횟수를 지정합니다.   from sklearn.linear_model import SGDClassifier  sc = SGDClassifier(loss='log', max_iter=10, random_state = 42) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))     0.773109243697479  0.775    점수가 낮게 나왔기 때문에 점수를 높이기 위해 이어서 훈련하여 점진적인 학습을 해보도록 합니다. 이어서 훈련할 때에는 partial_fit() 메서드를 사용하는데, 사용할 때 마다 1 epoch 씩 이어 훈련할 수 있습니다.   sc.partial_fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))     0.8151260504201681  0.85    확률적 경사 하강법을 사용한 모델은 에포크의 횟수에 따라 과대적합이나 과소적합 될 수 있기 때문에 에포크 횟수에 따른 훈련세트 점수와 테스트세트 점수를 그래프로 나타내 적절한 에포크의 값을 찾아줘야 합니다.   import numpy as np sc = SGDClassifier(loss='log', random_state = 42) train_score = [] test_score = [] classes = np.unique(train_target)  #300번의 epoch동안 훈련 반복 진행 for _ in range(0, 300):     sc.partial_fit(train_scaled, train_target, classes = classes)     train_score.append(sc.score(train_scaled, train_target))     test_score.append(sc.score(test_scaled, test_target))  #그래프 그리기 import matplotlib.pyplot as plt plt.plot(train_score) plt.plot(test_score) plt.xlabel('epoch') plt.ylabel('accuracy') plt.show()     결과인 그래프를 보면 초반에는 과소적합되어 훈련세트와 테스트세트 점수가 낮고, 약 100번째 에포크 이후에는 훈련세트와 테스트 세트의 점수가 벌어지고 있는 것을 볼 수 있습니다. 100번째 에포크가 적절한 값인 것을 확인할 수 있습니다. 반복횟수를 100으로 설정하고 모델을 다시 훈련합니다.   sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state = 42)  sc.fit(train_scaled, train_target)  print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target))     0.957983193277311  0.925    최종 점수가 잘 나오는 것을 확인할 수 있습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG4-2/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 5. 트리 알고리즘",
        "excerpt":"결정 트리 알고리즘을 사용하여 새로운 분류 문제를 다루어보도록 하겠습니다.   우선적으로 와인 데이터의 여러 변수를 사용하여 와인 종류를 구별할 수 있도록 로지스틱 회귀를 사용합니다.   로지스틱 회귀로 와인 분류하기   판다스를 이용해 데이터를 불러옵니다. head() 메서드를 사용해 데이터를 확인합니다.   import pandas as pd  wine = pd.read_csv('https://bit.ly/wine-date') wine.head()     wine.info()      와인 데이터 프레임의 각 데이터 타입과 누락된 값이 있는지 확인하기 위해 .info() 메서드를 사용합니다. 결과적으로 총 6497열 중에 각 column 값이 6497인 것을 보아 누락된 데이터는 없다는 것을 확인했습니다.  다음으로 describe() 메서드를 사용하여 각 column에 대한 간단한 통계값을 출력합니다.   wine.describe()    이 값에서 확인할 수 잇는 점은 각 column에 대해 스케일이 다르다는 것입니다. 따라서 앞서 진행했던 것처럼 표준화를 해주어야 합니다. 표준화에 앞서 배열을 넘파이 형식으로 바꾼 후에 훈련 데이터와 테스트 데이터로 나누는 작업을 진행합니다.   data = wine[['alcohol', 'sugar', 'pH']].to_numpy() target = wine['class'].to_numpy()   from sklearn.model_selection import train_test_split  train_input, test_input, train_target, test_target = train_test_split(     data, target, test_size=0.2, random_state=42) print(train_input.shape, test_input.shape)     (5197, 3) (1300, 3)    훈련 세트와 테스트 세트의 갯수를 확인해봤습니다. 이제 StandardScaler을 사용해서 표준화합니다. 그리고 그 데이터를 사용하여 로지스틱 회귀 모델을 훈련합니다.   from sklearn.preprocessing import StandardScaler  ss = StandardScaler() ss.fit(train_input)  train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input)   from sklearn.linear_model import LogisticRegression  lr = LogisticRegression() lr.fit(train_scaled, train_target)  print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target))     0.7808350971714451  0.7776923076923077    모델의 점수가 높지 않은 것을 확인할 수 있습니다. 로지스틱 회귀가 학습한 계수와 절편을 출력해봅니다.   print(lr.coef_, lr.intercept_)     [[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]    모델을 설명하기 위해 로지스틱 회귀가 학습한 계수들을 출력해보았습니다. 그러나 한눈에 알아보기 어렵다는 단점이 있고, 관계를 쉽게 설명할 수 없기 때문에 비교적 설명이 쉬운 결정트리 모델을 학습해보겠습니다.   결정트리   결정트리 모델을 사이킷런의 DecisionTreeClassifier 클래스에서 제공합니다. 모델을 훈련한 후 점수를 출력해보겠습니다.   from sklearn.tree import DecisionTreeClassifier  dt = DecisionTreeClassifier(random_state=42) dt.fit(train_scaled, train_target)  print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target))     0.996921300750433 0.8592307692307692    정확도가 아까보다 높게 출력되었습니다. 다만 점수 차이를 보니 과대적합이 된 것 같습니다. 모델을 그림으로 표현해봅니다.   import matplotlib.pyplot as plt from sklearn.tree import plot_tree  plt.figure(figsize=(10,7)) plot_tree(dt) plt.show()    트리 그림이 너무 복잡하기 때문에 깊이를 제한해서 다시 그려보겠습니다. 트리의 깊이는 max_depth() 매개변수를 사용하면 됩니다. filled는 노드의 색을 칠할 수 있고 feature_names는 특성의 이름을 전달할 수 있습니다. 다음과 같은 매개변수들을 사용해서 다시 그림을 그려보겠습니다.   plt.figure(figsize=(10,7)) plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH']) plt.show()     가지치기   결정트리에서는 노드 수가 너무 많아지면 훈련 데이터에 과대적합될 수 있기 때문에 노드의 수를 조절하는 가지치기가 필요합니다. 가지치기를 할 수 있는 가장 간단한 방법은 max_depth를 조절하는 것입니다. 루트 노드 아래로 3개의 노드만 만들어 모델을 훈련해보겠습니다.   dt = DecisionTreeClassifier(max_depth=3, random_state=42) dt.fit(train_scaled, train_target)  print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target))     0.8454877814123533  0.8415384615384616    plt.figure(figsize=(20,15)) plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH']) plt.show()     훈련 세트의 점수는 낮아지고 테스트 세트 점수는 거의 비슷하기 때문에 그림으로 그려보았습니다.  깊이 3에 있는 마지막 노드들이 최종 노드인 리프 노드입니다.  세 번째에 위치한 노드만 음성 클래스가 더 많기 때문에 이 노드에 도착한 것들만 레드 와인으로 예측합니다.  루트 노드부터 이 노드까지 도달하기 위해서는 당도가 -0.239보다 작고 -0.802보다도 작아야 합니다. 알코올 도수는 0.454보다도 작아야 이 노드에 도달할 수 있습니다.    결정트리는 불순도에 따라 샘플을 나누는데, 불순도는 클래스별 비율을 가지고 계산하기 때문에 표준화 작업이 필요가 없습니다.  그렇다면 원래 데이터를 가지고 결정트리 모델을 다시 훈련해봅니다.   dt = DecisionTreeClassifier(max_depth=3, random_state=42) dt.fit(train_input, train_target)  print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target))     0.8454877814123533  0.8415384615384616    plt.figure(figsize=(20,15)) plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH']) plt.show()    점수는 동일하지만, 특성값이 음수로 나오지 않기 때문에 설명하기 쉬워졌습니다.  마지막으로 결정 트리에서 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산합니다.  feature_importances_ 속성으로 알아볼 수 있습니다.   print(dt.feature_importances_)     [0.12345626 0.86862934 0.0079144 ]    두번째 값인 당도가 0.87 정도로 특성 중요도가 가장 높은 것을 볼 수 있습니다.  특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더해서 계산합니다.  특성 중요도를 활용하면 결정트리 모델을 특성 선택에 활용할 수 있습니다.  성능이 매우 좋지는 않지만 설명하기는 좋은 모델이라고 생각할 수 있습니다.   다음 절에서는 결정 트리의 다양한 매개변수들, 하이퍼파라미터를 자동으로 찾을 수 있는 방법에 대해 알아보도록 하겠습니다.   ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG5-1/",
        "teaser": "/assets/images/main.jpg"
      },{
        "title": "[혼공머신러닝] Ch 5. 트리 알고리즘(2)",
        "excerpt":"교차 검증과 그리드 서치   테스트 세트 성능을 올바르게 판단하기 위해서는 모델을 전부 만든 뒤에 마지막에 한 번만 사용하는 것이 좋습니다. 그렇다면 테스트 세트를 사용하지 않고 어떻게 하이퍼 파라미터 튜닝을 할 수 있을지 알아보도록 하겠습니다.   검증 세트  테스트 세트를 사용하지 않고 모델이 과대적합인지 과소적합인지 알아보기 위해서는 훈련 세트를 또 나누는 방법을 취할 수 있습니다. 이 데이터를 검증세트라고 합니다. 훈련 세트에서 모델을 훈련한 후 검증세트로 평가합니다. 이런 식으로 매개변수들을 바꿔가며 가장 좋은 모델을 고르면 됩니다.   그럼 데이터를 불러와서 train 세트와 test 세트로 나눈 후에 검증 세트로도 나눠보겠습니다.   import pandas as pd  wine = pd.read_csv('https://bit.ly/wine_csv_data')  와인 데이터를 data에는 alcohol, sugar, pH 를 지정하고, target에는 class 를 넘파이 배열로 지정해줍니다.   data = wine[['alcohol', 'sugar', 'pH']].to_numpy() target = wine['class'].to_numpy()  train_test_split를 사용하여 train data와 test data로 나눕니다. test_size를 0.2로 지정하여 20%만 test 데이터로 가져옵니다. test size를 지정하지 않을 경우 train_test_split는 디폴트로 0.25를 테스트 세트로 지정합니다.   from sklearn.model_selection import train_test_split  train_input, test_input, train_target, test_target = train_test_split(     data, target, test_size=0.2, random_state=42)  그리고 train data를 sub data와 val data로 한번 더 나누어 줍니다.   sub_input, val_input, sub_target, val_target = train_test_split(     train_input, train_target, test_size=0.2, random_state=42)  훈련 세트와 검증 세트의 크기를 확인해봅니다.   print(sub_input.shape, val_input.shape)     (4157, 3) (1040, 3)    데이터를 각각 훈련데이터, 검증데이터, 테스트 데이터로 나누어줬습니다.  다음으로 훈련데이터와 검증데이터를 사용하여 모델을 만들고 평가합니다.   from sklearn.tree import DecisionTreeClassifier  dt = DecisionTreeClassifier(random_state=42) dt.fit(sub_input, sub_target)  print(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target))     0.9971133028626413  0.864423076923077    모델이 훈련 세트에 과대적합 되어있으니 매개변수를 바꾸어 더 좋은 모델을 찾아보겠습니다.   교차 검증  많은 데이터를 훈련세트에 사용할 수록 모델의 정확도가 좋아집니다. 그렇다고 검증세트를 적게 사용하면 검증 점수가 불안정하게 나올 수 있습니다. 이를 해결하기 위해 교차검증을 이용합니다.  검증세트를 떼어놓는 과정을 여러 번 반복하고 이 점수들을 평균내어 검증 점수를 얻어냅니다. 이런 방법을 사용하면 데이터의 8,90% 정도를 훈련에 사용할 수 있습니다. 사이킷런에 cross_validate() 함수를 사용하여 교차검증을 해보도록 하겠습니다.   from sklearn.model_selection import cross_validate  scores = cross_validate(dt, train_input, train_target) print(scores)     {‘fit_time’: array([0.00482202, 0.00468516, 0.00489712, 0.00469923, 0.0045011 ]), ‘score_time’: array([0.0005331 , 0.00038004, 0.00038218, 0.00042176, 0.00033784]), ‘test_score’: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}    이 함수는 fit_time, score_time, test_score 키를 갖는 딕셔너리를 반환합니다.  cross_validate 함수는 기본적으로 5-폴드 교차검증을 수행하기 때문에 각 키마다 5개의 숫자가 담겨있습니다.  교차 검증의 점수들을 평균내어 최종 점수를 얻습니다.   import numpy as np  print(np.mean(scores['test_score']))     0.855300214703487    다만 cross_validate 함수는 훈련 세트를 섞어서 폴드를 나누지 않기 때문에 만약 훈련세트를 섞어서 나누고 싶다면 splitter을 지정해줘야합니다.  cross_validate 함수는 기본적으로 회귀모델일 경우 KFold 분할기를 사용하고 분류모델일 경우 StratifiedKFold를 사용합니다.  다음 코드와 앞서 수행한 교차 검증은 동일합니다.   from sklearn.model_selection import StratifiedKFold  scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold()) print(np.mean(scores['test_score']))     0.855300214703487    10-교차검증을 해보고 싶다면 다음과 같이 stratifiedKFold에 splits를 10으로 지정하고 splitter에 할당하여 사용하면 됩니다.   splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) scores = cross_validate(dt, train_input, train_target, cv=splitter) print(np.mean(scores['test_score']))     0.8574181117533719    하이퍼파라미터 튜닝   결정트리 모델에서는 여러 매개변수를 동시에 바꿔가며 최적의 값을 찾아야하기 때문에 그리드 서치를 사용합니다.  GridSearchCV 클래스는 하이퍼파라미터 탐색과 교차 검증을 한번에 수행해줍니다.  한 예시로 기본 매개변수를 사용한 결정트리 모델에서 min_impurity_decrease 매개변수의 최적값을 찾아보겠습니다.   from sklearn.model_selection import GridSearchCV  params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}  여기서는 5개의 값을 시도합니다.  결정트리 클래스의 객체를 생성하고 바로 전달합니다.  gs 객체에 fit 메서드를 호출합니다. 메서드를 호출하면 그리드 서치 객체는 min_impurity_decrease 값을 바꿔가며 5번 실행합니다.  많은 모델을 훈련하기 때문에 n_jobs 매개변수를 사용하여 미리 사용할 CPU 코어수를 지정해줄 수 있습니다.  이 매개변수의 기본 값은 1로, -1로 지정하면 시스템에 있는 모든 코어를 사용합니다.  사이킷런의 그리드 서치는 훈련이 끝나면 best_estimator_속성에 가장 좋은 모델을 저장합니다.  그리고 그리드 서치로 찾은 최적의 매개변수는 best_params_ 속성에 저장되어있습니다.  확인해보겠습니다.   gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)   gs.fit(train_input, train_target)     GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,              param_grid={‘min_impurity_decrease’: [0.0001, 0.0002, 0.0003,                                                    0.0004, 0.0005]})    dt = gs.best_estimator_ print(dt.score(train_input, train_target))     0.9615162593804117    print(gs.best_params_)     {‘min_impurity_decrease’: 0.0001}    0.0001이 가장 좋은 값으로 선택되었습니다.  각 매개변수에서 수행한 교차 검증의 평균 점수는 mean_test_score에 저장되어있습니다.  그리고 어떤 값이 큰지 확인하기 위해 넘파이의 argmax() 함수를사용합니다.  그다음 그 인덱스를 이용하여 params키에 저장된 매개변수를 출력합니다.   print(gs.cv_results_['mean_test_score'])     [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]    best_index = np.argmax(gs.cv_results_['mean_test_score']) print(gs.cv_results_['params'][best_index])     {‘min_impurity_decrease’: 0.0001}    이제 더 복잡한 매개변수 조합들에서도 사용해보겠습니다. params에 다음과 같이 여러 매개변수의 범위를 설정하고 그리드 서치를 실행해봅니다.   params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),           'max_depth': range(5, 20, 1),           'min_samples_split': range(2, 100, 10)           }   gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) gs.fit(train_input, train_target)     GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,              param_grid={‘max_depth’: range(5, 20),                          ‘min_impurity_decrease’: array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,        0.0009]),                          ‘min_samples_split’: range(2, 100, 10)})    print(gs.best_params_)     {‘max_depth’: 14, ‘min_impurity_decrease’: 0.0004, ‘min_samples_split’: 12}    print(np.max(gs.cv_results_['mean_test_score']))     0.8683865773302731    랜덤서치   매개변수의 값의 범위를 미리 정하기 어려울 때, 그리드 서치 수행시간이 너무 오래걸릴 때 랜덤서치를 사용할 수 있습니다.  랜덤 서치에는 매개변수를 샘플링할 수 있는 확률분포를 전달합니다.  확률 분포 클래스를 임포트하겠습니다.   from scipy.stats import uniform, randint  임포트한 두 클래스 모두 주어진 범위 내에서 고르게 값을 뽑습니다.  차이점은 randint 값은 정숫값을, uniform은 실수값을 뽑습니다.  0과 10 사이의 범위를 갖는 randint 객체를 만들어 샘플링을 해보겠습니다.   rgen = randint(0, 10) rgen.rvs(10)     array([3, 6, 1, 2, 3, 1, 7, 6, 5, 1])    고르게 샘플링이 되는지 확인하기 어려우니, 1000개를 샘플링해서 각 갯수를 세어봅니다.   np.unique(rgen.rvs(1000), return_counts=True)     (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),  array([ 79,  89, 110, 112, 106, 104,  95,  99, 104, 102]))    꽤 고르게 샘플링이 된 것을 확인할 수 있습니다. 마찬가지로 uniform도 사용해보겠습니다.   ugen = uniform(0, 1) ugen.rvs(10)     array([0.38360187, 0.85618662, 0.41423051, 0.18153867, 0.36998781,        0.93449236, 0.36294108, 0.2842492 , 0.26135179, 0.12996384])    난수 발생기와 유사한 형태로 출력되었습니다.   이제 탐색할 매개변수의 딕셔너리를 만들어줍니다.  탐색할 매개변수의 범위를 다음과 같이 지정합니다.  min_samples_leaf 매개변수는 리프 노드가 되기 위한 최소 샘플의 개수입니다. 이보다 자식노드의 샘플수가 적을경우 분할하지 않는다는 의미입니다.   params = {'min_impurity_decrease': uniform(0.0001, 0.001),           'max_depth': randint(20, 50),           'min_samples_split': randint(2, 25),           'min_samples_leaf': randint(1, 25),           }   from sklearn.model_selection import RandomizedSearchCV  gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,                          n_iter=100, n_jobs=-1, random_state=42) gs.fit(train_input, train_target)     RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42),                    n_iter=100, n_jobs=-1,                    param_distributions={‘max_depth’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f08156b80&gt;,                                         ‘min_impurity_decrease’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f510ecfd0&gt;,                                         ‘min_samples_leaf’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f081568b0&gt;,                                         ‘min_samples_split’: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8f202cc2b0&gt;},                    random_state=42)    params에서 정의된 범위에서 총 100번을 샘플링하여 교차검증을 수행하여 최적의 값을 찾습니다. 샘플링 횟수는 n_iter 매개변수에 지정해주었습니다. 이제 최적의 매개변수의 값을 출력하고 최고의 교차 검증 점수도 확인해보겠습니다.   print(gs.best_params_)     {‘max_depth’: 39, ‘min_impurity_decrease’: 0.00034102546602601173, ‘min_samples_leaf’: 7, ‘min_samples_split’: 13}    print(np.max(gs.cv_results_['mean_test_score']))     0.8695428296438884    best_estimator_에 저장되어있는 최적의 모델을 사용하여 테스트의 성능을 확인합니다.   dt = gs.best_estimator_  print(dt.score(test_input, test_target))     0.86    오늘은 수동으로 매개변수를 바꾸는 대신 그리드 서치나 랜덤 서치를 이용하여 최적의 매개변수를 찾는 방법을 알아보았습니다. 다음에는 결정 트리를 확장한 앙상블 모델에 대해 알아보도록 하겠습니다.  ","categories": ["ML"],
        "tags": ["Blog","Machine learning","Data mining","혼공머신러닝"],
        "url": "/ml/HG5-2/",
        "teaser": "/assets/images/main.jpg"
      }]
